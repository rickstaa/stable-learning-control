<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>stable_learning_control.algos.pytorch.lac.lac &mdash; stable-learning-control 6.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css?v=eafc0fe6" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/css/modify.css?v=519ed47b" />

  
    <link rel="shortcut icon" href="../../../../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js?v=9497d378"></script>
        <script src="../../../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html">
            
              <img src="../../../../../_static/logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                6.0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../usage/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../usage/docker.html">Use with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../usage/algorithms.html">Available Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../usage/running.html">Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../usage/hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../usage/saving_and_loading.html">Experiment Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../usage/plotting.html">Plotting Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../usage/eval_robustness.html">Evaluating Robustness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../usage/benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utilities</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../utils/loggers.html">Loggers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../utils/mpi.html">MPI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../utils/run_utils.html">Run Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../utils/plotter.html">Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../utils/testers.html">Policy testers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../dev/contributing.html">Contribute to stable-learning-control</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../dev/doc_dev.html">Build the documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../dev/license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../autoapi/index.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Etc.</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../etc/acknowledgements.html">Acknowledgements</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">stable-learning-control</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">stable_learning_control.algos.pytorch.lac.lac</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for stable_learning_control.algos.pytorch.lac.lac</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Lyapunov (soft) Actor-Critic (LAC) algorithm.</span>

<span class="sd">This module contains a pytorch implementation of the LAC algorithm of</span>
<span class="sd">`Han et al. 2020 &lt;https://arxiv.org/abs/2004.14288&gt;`_.</span>

<span class="sd">.. note::</span>
<span class="sd">    Code Conventions:</span>
<span class="sd">        - We use a `_` suffix to distinguish the next state from the current state.</span>
<span class="sd">        - We use a `targ` suffix to distinguish actions/values coming from the target</span>
<span class="sd">          network.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">os.path</span> <span class="k">as</span> <span class="nn">osp</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span><span class="p">,</span> <span class="n">PurePath</span>

<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">gymnasium.utils</span> <span class="kn">import</span> <span class="n">seeding</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="kn">from</span> <span class="nn">stable_learning_control.algos.common.helpers</span> <span class="kn">import</span> <span class="n">heuristic_target_entropy</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.algos.pytorch.common.buffers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FiniteHorizonReplayBuffer</span><span class="p">,</span>
    <span class="n">ReplayBuffer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.algos.pytorch.common.get_lr_scheduler</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_lr_scheduler</span><span class="p">,</span>
    <span class="n">estimate_step_learning_rate</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.algos.pytorch.common.helpers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">count_vars</span><span class="p">,</span>
    <span class="n">retrieve_device</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.algos.pytorch.policies.lyapunov_actor_critic</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">LyapunovActorCritic</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.algos.pytorch.policies.lyapunov_actor_twin_critic</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">LyapunovActorTwinCritic</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.common.helpers</span> <span class="kn">import</span> <span class="n">friendly_err</span><span class="p">,</span> <span class="n">get_env_id</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.utils.eval_utils</span> <span class="kn">import</span> <span class="n">test_agent</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.utils.gym_utils</span> <span class="kn">import</span> <span class="n">is_discrete_space</span><span class="p">,</span> <span class="n">is_gym_env</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.utils.log_utils.helpers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">log_to_std_out</span><span class="p">,</span>
    <span class="n">setup_logger_kwargs</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.utils.log_utils.logx</span> <span class="kn">import</span> <span class="n">EpochLogger</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.utils.safer_eval_util</span> <span class="kn">import</span> <span class="n">safer_eval</span>
<span class="kn">from</span> <span class="nn">stable_learning_control.utils.serialization_utils</span> <span class="kn">import</span> <span class="n">save_to_json</span>

<span class="c1"># Script settings.</span>
<div class="viewcode-block" id="SCALE_LAMBDA_MIN_MAX"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.SCALE_LAMBDA_MIN_MAX">[docs]</a><span class="n">SCALE_LAMBDA_MIN_MAX</span> <span class="o">=</span> <span class="p">(</span>
    <span class="mf">0.0</span><span class="p">,</span>
    <span class="mf">1.0</span><span class="p">,</span>
<span class="p">)</span>  <span class="c1"># Range of lambda Lagrance multiplier.</span></div>
<div class="viewcode-block" id="SCALE_ALPHA_MIN_MAX"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.SCALE_ALPHA_MIN_MAX">[docs]</a><span class="n">SCALE_ALPHA_MIN_MAX</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># Range of alpha Lagrance multiplier.</span></div>
<div class="viewcode-block" id="STD_OUT_LOG_VARS_DEFAULT"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.STD_OUT_LOG_VARS_DEFAULT">[docs]</a><span class="n">STD_OUT_LOG_VARS_DEFAULT</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Epoch&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TotalEnvInteracts&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AverageEpRet&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AverageTestEpRet&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AverageTestEpLen&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AverageAlpha&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AverageLambda&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AverageLossAlpha&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AverageLossLambda&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AverageErrorL&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AverageLossPi&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AverageEntropy&quot;</span><span class="p">,</span>
<span class="p">]</span></div>
<div class="viewcode-block" id="VALID_DECAY_TYPES"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.VALID_DECAY_TYPES">[docs]</a><span class="n">VALID_DECAY_TYPES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;exponential&quot;</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">]</span></div>
<div class="viewcode-block" id="VALID_DECAY_REFERENCES"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.VALID_DECAY_REFERENCES">[docs]</a><span class="n">VALID_DECAY_REFERENCES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">,</span> <span class="s2">&quot;epoch&quot;</span><span class="p">]</span></div>
<div class="viewcode-block" id="DEFAULT_DECAY_TYPE"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.DEFAULT_DECAY_TYPE">[docs]</a><span class="n">DEFAULT_DECAY_TYPE</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span></div>
<div class="viewcode-block" id="DEFAULT_DECAY_REFERENCE"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.DEFAULT_DECAY_REFERENCE">[docs]</a><span class="n">DEFAULT_DECAY_REFERENCE</span> <span class="o">=</span> <span class="s2">&quot;epoch&quot;</span></div>


<div class="viewcode-block" id="LAC"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC">[docs]</a><span class="k">class</span> <span class="nc">LAC</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The Lyapunov (soft) Actor-Critic (LAC) algorithm.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        ac (torch.nn.Module): The (lyapunov) actor critic module.</span>
<span class="sd">        ac_ (torch.nn.Module): The (lyapunov) target actor critic module.</span>
<span class="sd">        log_alpha (torch.Tensor): The temperature Lagrance multiplier.</span>
<span class="sd">        log_labda (torch.Tensor): The Lyapunov Lagrance multiplier.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">env</span><span class="p">,</span>
        <span class="n">actor_critic</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">ac_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;actor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;critic&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">},</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
            <span class="n">output_activation</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;actor&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">},</span>
        <span class="p">),</span>
        <span class="n">opt_type</span><span class="o">=</span><span class="s2">&quot;minimize&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
        <span class="n">alpha3</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">labda</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
        <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
        <span class="n">polyak</span><span class="o">=</span><span class="mf">0.995</span><span class="p">,</span>
        <span class="n">target_entropy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adaptive_temperature</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">lr_a</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">lr_c</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span>
        <span class="n">lr_alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">lr_labda</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialise the LAC algorithm.</span>

<span class="sd">        Args:</span>
<span class="sd">            env (:obj:`gym.env`): The gymnasium environment the LAC is training in. This is</span>
<span class="sd">                used to retrieve the activation and observation space dimensions. This</span>
<span class="sd">                is used while creating the network sizes. The environment must satisfy</span>
<span class="sd">                the gymnasium API.</span>
<span class="sd">            actor_critic (torch.nn.Module, optional): The constructor method for a</span>
<span class="sd">                Torch Module with an ``act`` method, a ``pi`` module and several</span>
<span class="sd">                ``Q`` or ``L`` modules. The ``act`` method and ``pi`` module should</span>
<span class="sd">                accept batches of observations as inputs, and the ``Q*`` and ``L``</span>
<span class="sd">                modules should accept a batch of observations and a batch of actions as</span>
<span class="sd">                inputs. When called, these modules should return:</span>

<span class="sd">                ===========  ================  ======================================</span>
<span class="sd">                Call         Output Shape      Description</span>
<span class="sd">                ===========  ================  ======================================</span>
<span class="sd">                ``act``      (batch, act_dim)  | Numpy array of actions for each</span>
<span class="sd">                                               | observation.</span>
<span class="sd">                ``Q*/L``     (batch,)          | Tensor containing one current estimate</span>
<span class="sd">                                               | of ``Q*/L`` for the provided</span>
<span class="sd">                                               | observations and actions. (Critical:</span>
<span class="sd">                                               | make sure to flatten this!)</span>
<span class="sd">                ===========  ================  ======================================</span>

<span class="sd">                Calling ``pi`` should return:</span>

<span class="sd">                ===========  ================  ======================================</span>
<span class="sd">                Symbol       Shape             Description</span>
<span class="sd">                ===========  ================  ======================================</span>
<span class="sd">                ``a``        (batch, act_dim)  | Tensor containing actions from policy</span>
<span class="sd">                                               | given observations.</span>
<span class="sd">                ``logp_pi``  (batch,)          | Tensor containing log probabilities of</span>
<span class="sd">                                               | actions in ``a``. Importantly:</span>
<span class="sd">                                               | gradients should be able to flow back</span>
<span class="sd">                                               | into ``a``.</span>
<span class="sd">                ===========  ================  ======================================</span>

<span class="sd">                Defaults to</span>
<span class="sd">                :class:`~stable_learning_control.algos.pytorch.policies.lyapunov_actor_critic.LyapunovActorCritic`</span>
<span class="sd">            ac_kwargs (dict, optional): Any kwargs appropriate for the ActorCritic</span>
<span class="sd">                object you provided to LAC. Defaults to:</span>

<span class="sd">                =======================  ============================================</span>
<span class="sd">                Kwarg                    Value</span>
<span class="sd">                =======================  ============================================</span>
<span class="sd">                ``hidden_sizes_actor``    ``256 x 2``</span>
<span class="sd">                ``hidden_sizes_critic``   ``256 x 2``</span>
<span class="sd">                ``activation``            :class:`torch.nn.ReLU`</span>
<span class="sd">                ``output_activation``     :class:`torch.nn.ReLU`</span>
<span class="sd">                =======================  ============================================</span>
<span class="sd">            opt_type (str, optional): The optimization type you want to use. Options</span>
<span class="sd">                ``maximize`` and ``minimize``. Defaults to ``maximize``.</span>
<span class="sd">            alpha (float, optional): Entropy regularization coefficient (Equivalent to</span>
<span class="sd">                inverse of reward scale in the original SAC paper). Defaults to</span>
<span class="sd">                ``0.99``.</span>
<span class="sd">            alpha3 (float, optional): The Lyapunov constraint error boundary. Defaults</span>
<span class="sd">                to ``0.2``.</span>
<span class="sd">            labda (float, optional): The Lyapunov Lagrance multiplier. Defaults to</span>
<span class="sd">                ``0.99``.</span>
<span class="sd">            gamma (float, optional): Discount factor. (Always between 0 and 1.).</span>
<span class="sd">                Defaults to ``0.99`` per Haarnoja et al. 2018, not ``0.995`` as in</span>
<span class="sd">                Han et al. 2020.</span>
<span class="sd">            polyak (float, optional): Interpolation factor in polyak averaging for</span>
<span class="sd">                target networks. Target networks are updated towards main networks</span>
<span class="sd">                according to:</span>

<span class="sd">                .. math:: \\theta_{\\text{targ}} \\leftarrow</span>
<span class="sd">                    \\rho \\theta_{\\text{targ}} + (1-\\rho) \\theta</span>

<span class="sd">                where :math:`\\rho` is polyak (Always between 0 and 1, usually close to</span>
<span class="sd">                1.). In some papers :math:`\\rho` is defined as (1 - :math:`\\tau`)</span>
<span class="sd">                where :math:`\\tau` is the soft replacement factor. Defaults to</span>
<span class="sd">                ``0.995``.</span>
<span class="sd">            target_entropy (float, optional): Initial target entropy used while learning</span>
<span class="sd">                the entropy temperature (alpha). Defaults to the</span>
<span class="sd">                maximum information (bits) contained in action space. This can be</span>
<span class="sd">                calculated according to :</span>

<span class="sd">                .. math::</span>
<span class="sd">                    -{\\prod }_{i=0}^{n}action\\_di{m}_{i}\\phantom{\\rule{0ex}{0ex}}</span>
<span class="sd">            adaptive_temperature (bool, optional): Enabled Automating Entropy Adjustment</span>
<span class="sd">                for maximum Entropy RL_learning.</span>
<span class="sd">            lr_a (float, optional): Learning rate used for the actor. Defaults to</span>
<span class="sd">                ``1e-4``.</span>
<span class="sd">            lr_c (float, optional): Learning rate used for the (lyapunov) critic.</span>
<span class="sd">                Defaults to ``1e-4``.</span>
<span class="sd">            lr_alpha (float, optional): Learning rate used for the entropy temperature.</span>
<span class="sd">                Defaults to ``1e-4``.</span>
<span class="sd">            lr_labda (float, optional): Learning rate used for the Lyapunov Lagrance</span>
<span class="sd">                multiplier. Defaults to ``3e-4``.</span>
<span class="sd">            device (str, optional): The device the networks are placed on (options:</span>
<span class="sd">                ``cpu``, ``gpu``, ``gpu:0``, ``gpu:1``, etc.). Defaults to ``cpu``.</span>

<span class="sd">        .. attention::</span>
<span class="sd">            This class will behave differently when the ``actor_critic`` argument</span>
<span class="sd">            is set to the :class:`~stable_learning_control.algos.pytorch.policies.lyapunov_actor_twin_critic.LyapunovActorTwinCritic`.</span>
<span class="sd">            For more information see the :ref:`LATC &lt;latc&gt;` documentation.</span>
<span class="sd">        &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501, D301</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<div class="viewcode-block" id="LAC._setup_kwargs"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._setup_kwargs">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_setup_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;self&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">,</span> <span class="s2">&quot;env&quot;</span><span class="p">]</span>
        <span class="p">}</span></div>

        <span class="c1"># Validate gymnasium env.</span>
        <span class="c1"># NOTE: The current implementation only works with continuous spaces.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_gym_env</span><span class="p">(</span><span class="n">env</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Env must be a valid gymnasium environment.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_discrete_space</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_discrete_space</span><span class="p">(</span>
            <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;The LAC algorithm does not yet support discrete observation/action &quot;</span>
                <span class="s2">&quot;spaces. Please open a feature/pull request on &quot;</span>
                <span class="s2">&quot;https://github.com/rickstaa/stable-learning-control/issues if you &quot;</span>
                <span class="s2">&quot;need this.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Print out some information about the environment and algorithm.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">unwrapped</span><span class="o">.</span><span class="n">spec</span><span class="p">,</span> <span class="s2">&quot;id&quot;</span><span class="p">):</span>
            <span class="n">log_to_std_out</span><span class="p">(</span>
                <span class="s2">&quot;You are using the &#39;</span><span class="si">{}</span><span class="s2">&#39; environment.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">get_env_id</span><span class="p">(</span><span class="n">env</span><span class="p">)),</span>
                <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">log_to_std_out</span><span class="p">(</span>
                <span class="s2">&quot;You are using the &#39;</span><span class="si">{}</span><span class="s2">&#39; environment.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">type</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">unwrapped</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="p">),</span>
                <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">log_to_std_out</span><span class="p">(</span><span class="s2">&quot;You are using the LAC algorithm.&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">)</span>
        <span class="n">log_to_std_out</span><span class="p">(</span>
            <span class="s2">&quot;This agent is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="s2">&quot;minimizing the cost&quot;</span>
                <span class="k">if</span> <span class="n">opt_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;minimize&quot;</span>
                <span class="k">else</span> <span class="s2">&quot;maximizing the return&quot;</span>
            <span class="p">),</span>
            <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Store algorithm parameters.</span>
<div class="viewcode-block" id="LAC._act_dim"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._act_dim">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_act_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span></div>
<div class="viewcode-block" id="LAC._obs_dim"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._obs_dim">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_obs_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span></div>
<div class="viewcode-block" id="LAC._device"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._device">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">retrieve_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>
<div class="viewcode-block" id="LAC._adaptive_temperature"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._adaptive_temperature">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_adaptive_temperature</span> <span class="o">=</span> <span class="n">adaptive_temperature</span></div>
<div class="viewcode-block" id="LAC._opt_type"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._opt_type">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_opt_type</span> <span class="o">=</span> <span class="n">opt_type</span></div>
<div class="viewcode-block" id="LAC._polyak"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._polyak">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_polyak</span> <span class="o">=</span> <span class="n">polyak</span></div>
<div class="viewcode-block" id="LAC._gamma"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._gamma">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span></div>
<div class="viewcode-block" id="LAC._alpha3"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._alpha3">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha3</span> <span class="o">=</span> <span class="n">alpha3</span></div>
<div class="viewcode-block" id="LAC._lr_a"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._lr_a">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_lr_a</span> <span class="o">=</span> <span class="n">lr_a</span></div>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adaptive_temperature</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_lr_alpha</span> <span class="o">=</span> <span class="n">lr_alpha</span>
<div class="viewcode-block" id="LAC._lr_lag"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._lr_lag">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_lr_lag</span> <span class="o">=</span> <span class="n">lr_labda</span></div>
<div class="viewcode-block" id="LAC._lr_c"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._lr_c">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_lr_c</span> <span class="o">=</span> <span class="n">lr_c</span></div>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target_entropy</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_target_entropy</span> <span class="o">=</span> <span class="n">heuristic_target_entropy</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_target_entropy</span> <span class="o">=</span> <span class="n">target_entropy</span>
<div class="viewcode-block" id="LAC._use_twin_critic"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._use_twin_critic">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_use_twin_critic</span> <span class="o">=</span> <span class="kc">False</span></div>

        <span class="c1"># Create variables for the Lagrance multipliers.</span>
        <span class="c1"># NOTE: Clip at 1e-37 to prevent log_alpha/log_lambda from becoming -np.inf</span>
<div class="viewcode-block" id="LAC.log_alpha"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.log_alpha">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">log_alpha</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1e-37</span> <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;</span> <span class="mf">1e-37</span> <span class="k">else</span> <span class="n">alpha</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span></div>
<div class="viewcode-block" id="LAC.log_labda"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.log_labda">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">log_labda</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1e-37</span> <span class="k">if</span> <span class="n">labda</span> <span class="o">&lt;</span> <span class="mf">1e-37</span> <span class="k">else</span> <span class="n">labda</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span></div>

        <span class="c1"># Get default actor critic if no &#39;actor_critic&#39; was supplied</span>
<div class="viewcode-block" id="LAC.actor_critic"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.actor_critic">[docs]</a>        <span class="n">actor_critic</span> <span class="o">=</span> <span class="n">LyapunovActorCritic</span> <span class="k">if</span> <span class="n">actor_critic</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">actor_critic</span></div>

        <span class="c1"># Create actor-critic module and target networks</span>
        <span class="c1"># NOTE: Pytorch currently uses kaiming initialization for the baises in the</span>
        <span class="c1"># future this will change to zero initialization</span>
        <span class="c1"># (https://github.com/pytorch/pytorch/issues/18182). This however does not</span>
        <span class="c1"># influence the results.</span>
<div class="viewcode-block" id="LAC.ac"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.ac">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">ac</span> <span class="o">=</span> <span class="n">actor_critic</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="o">**</span><span class="n">ac_kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="p">)</span></div>
<div class="viewcode-block" id="LAC.ac_targ"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.ac_targ">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">ac_targ</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span></div>

        <span class="c1"># Freeze target networks with respect to optimizers (updates via polyak avg.)</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac_targ</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Create optimizers.</span>
        <span class="c1"># NOTE: We here optimize for log_alpha and log_labda instead of alpha and labda</span>
        <span class="c1"># because it is more numerically stable (see:</span>
        <span class="c1"># https://github.com/rail-berkeley/softlearning/issues/136)</span>
        <span class="c1"># NOTE: The parameters() method returns a generator. This generator becomes</span>
        <span class="c1"># empty after you looped through all values. As a result, below we use a</span>
        <span class="c1"># lambda function to keep referencing the actual model parameters.</span>
<div class="viewcode-block" id="LAC._pi_optimizer"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._pi_optimizer">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_pi_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">pi</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_lr_a</span><span class="p">)</span></div>
<div class="viewcode-block" id="LAC._pi_params"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._pi_params">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_pi_params</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">pi</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span></div>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adaptive_temperature</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_alpha_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">log_alpha</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_lr_alpha</span><span class="p">)</span>
<div class="viewcode-block" id="LAC._log_labda_optimizer"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._log_labda_optimizer">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_log_labda_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">log_labda</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_lr_lag</span><span class="p">)</span></div>
<div class="viewcode-block" id="LAC._c_optimizer"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._c_optimizer">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_c_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">L</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_lr_c</span><span class="p">)</span></div>
<div class="viewcode-block" id="LAC._c_params"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._c_params">[docs]</a>        <span class="bp">self</span><span class="o">.</span><span class="n">_c_params</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">L</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span></div>

        <span class="c1"># ==== LATC code ===============================</span>
        <span class="c1"># NOTE: Added here to reduce code duplication.</span>
        <span class="c1"># Ensure parameters of both critics are used by the critic optimizer.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="p">,</span> <span class="n">LyapunovActorTwinCritic</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_use_twin_critic</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_c_params</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
                <span class="o">*</span><span class="p">[</span><span class="n">gen</span><span class="p">()</span> <span class="k">for</span> <span class="n">gen</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">L</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">L2</span><span class="o">.</span><span class="n">parameters</span><span class="p">]]</span>
            <span class="p">)</span>  <span class="c1"># Chain parameters of the two Q-critics</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_c_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_c_params</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_lr_c</span><span class="p">)</span>
        <span class="c1"># ==============================================</span>

<div class="viewcode-block" id="LAC.forward"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Wrapper around the :meth:`get_action` method that enables users to also</span>
<span class="sd">        receive actions directly by invoking ``LAC(observations)``.</span>

<span class="sd">        Args:</span>
<span class="sd">            s (numpy.ndarray): The current state.</span>
<span class="sd">            deterministic (bool, optional): Whether to return a deterministic action.</span>
<span class="sd">                Defaults to ``False``.</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.ndarray: The current action.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">)</span></div>

<div class="viewcode-block" id="LAC.get_action"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.get_action">[docs]</a>    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the current action of the policy.</span>

<span class="sd">        Args:</span>
<span class="sd">            s (numpy.ndarray): The current state.</span>
<span class="sd">            deterministic (bool, optional): Whether to return a deterministic action.</span>
<span class="sd">                Defaults to ``False``.</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.ndarray: The current action.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">act</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">deterministic</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="LAC.update"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update the actor critic network using stochastic gradient descent.</span>

<span class="sd">        Args:</span>
<span class="sd">            data (dict): Dictionary containing a batch of experiences.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">diagnostics</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">o</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">o_</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">data</span><span class="p">[</span><span class="s2">&quot;obs&quot;</span><span class="p">],</span>
            <span class="n">data</span><span class="p">[</span><span class="s2">&quot;act&quot;</span><span class="p">],</span>
            <span class="n">data</span><span class="p">[</span><span class="s2">&quot;rew&quot;</span><span class="p">],</span>
            <span class="n">data</span><span class="p">[</span><span class="s2">&quot;obs_next&quot;</span><span class="p">],</span>
            <span class="n">data</span><span class="p">[</span><span class="s2">&quot;done&quot;</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1"># Check if expected cumulative finite-horizon reward is supplied.</span>
        <span class="k">if</span> <span class="s2">&quot;horizon_rew&quot;</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;horizon_rew&quot;</span><span class="p">]</span>
        <span class="c1">################################################</span>
        <span class="c1"># Optimize (Lyapunov/Q) critic #################</span>
        <span class="c1">################################################</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_c_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opt_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;maximize&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;The LAC algorithm does not yet support maximization &quot;</span>
                <span class="s2">&quot;environments. Please open a feature/pull request on &quot;</span>
                <span class="s2">&quot;https://github.com/rickstaa/stable-learning-control/issues &quot;</span>
                <span class="s2">&quot;if you need this.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Get target Lyapunov value (Bellman-backup)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">pi_targ_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac_targ</span><span class="o">.</span><span class="n">pi</span><span class="p">(</span>
                <span class="n">o_</span>
            <span class="p">)</span>  <span class="c1"># NOTE: Target actions come from *current* *target* policy</span>
            <span class="n">l_pi_targ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac_targ</span><span class="o">.</span><span class="n">L</span><span class="p">(</span><span class="n">o_</span><span class="p">,</span> <span class="n">pi_targ_</span><span class="p">)</span>

            <span class="c1"># ==== LATC code ===============================</span>
            <span class="c1"># Retrieve second target Lyapunov value.</span>
            <span class="c1"># NOTE: Added here to reduce code duplication.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_twin_critic</span><span class="p">:</span>
                <span class="n">l_pi_targ2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac_targ</span><span class="o">.</span><span class="n">L2</span><span class="p">(</span><span class="n">o_</span><span class="p">,</span> <span class="n">pi_targ_</span><span class="p">)</span>
                <span class="n">l_pi_targ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
                    <span class="n">l_pi_targ</span><span class="p">,</span> <span class="n">l_pi_targ2</span>
                <span class="p">)</span>  <span class="c1"># Use max clipping to prevent underestimation bias.</span>
            <span class="c1"># ==============================================</span>

            <span class="c1"># Calculate Lyapunov target.</span>
            <span class="c1"># NOTE: Calculation depends on the chosen Lyapunov candidate.</span>
            <span class="k">if</span> <span class="s2">&quot;horizon_rew&quot;</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>  <span class="c1"># Finite-horizon candidate (Han eq. 9).</span>
                <span class="n">l_backup</span> <span class="o">=</span> <span class="n">v</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># Infinite-horizon bellman backup (Han eq. 8).</span>
                <span class="n">l_backup</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">d</span><span class="p">)</span> <span class="o">*</span> <span class="n">l_pi_targ</span>

        <span class="c1"># Get current Lyapunov value.</span>
        <span class="n">l1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">L</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

        <span class="c1"># ==== LATC code ===============================</span>
        <span class="c1"># NOTE: Added here to reduce code duplication.</span>
        <span class="c1"># Retrieve second Lyapunov value.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_twin_critic</span><span class="p">:</span>
            <span class="n">l2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">L2</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="c1"># ==============================================</span>

        <span class="c1"># Calculate L-critic MSE loss against Bellman backup.</span>
        <span class="c1"># NOTE: The 0.5 multiplication factor was added to make the derivation</span>
        <span class="c1"># cleaner and can be safely removed without influencing the minimization. We</span>
        <span class="c1"># kept it here for consistency.</span>
        <span class="c1"># NOTE: We currently use a manual implementation instead of using F.mse_loss</span>
        <span class="c1"># as this is 2 times faster. This can be changed back to F.mse_loss if</span>
        <span class="c1"># Torchscript is used.</span>
        <span class="n">l_error</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">l1</span> <span class="o">-</span> <span class="n">l_backup</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># See Han eq. 7</span>

        <span class="c1"># ==== LATC code ===============================</span>
        <span class="c1"># NOTE: Added here to reduce code duplication.</span>
        <span class="c1"># Add second Lyapunov *CRITIC error*.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_twin_critic</span><span class="p">:</span>
            <span class="n">l_error</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">l2</span> <span class="o">-</span> <span class="n">l_backup</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="c1"># ==============================================</span>

        <span class="n">l_error</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_c_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">l_info</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">LVals</span><span class="o">=</span><span class="n">l1</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

        <span class="c1"># ==== LATC code ===============================</span>
        <span class="c1"># NOTE: Added here to reduce code duplication.</span>
        <span class="c1"># Add second Lyapunov value to info dict.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_twin_critic</span><span class="p">:</span>
            <span class="n">l_info</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;LVals2&quot;</span><span class="p">:</span> <span class="n">l2</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()})</span>
        <span class="c1"># ==============================================</span>

        <span class="n">diagnostics</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="o">**</span><span class="n">l_info</span><span class="p">,</span> <span class="s2">&quot;ErrorL&quot;</span><span class="p">:</span> <span class="n">l_error</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()})</span>
        <span class="c1">################################################</span>
        <span class="c1"># Optimize Gaussian actor ######################</span>
        <span class="c1">################################################</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pi_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Freeze Q-networks so you don&#39;t waste computational effort</span>
        <span class="c1"># computing gradients for them during the policy learning step.</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_params</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Retrieve log probabilities of batch observations based on *current* policy</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">logp_pi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">pi</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>

        <span class="c1"># Get target lyapunov value.</span>
        <span class="n">pi_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">pi</span><span class="p">(</span><span class="n">o_</span><span class="p">)</span>  <span class="c1"># NOTE: Target actions come from *current* policy</span>
        <span class="n">lya_l_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">L</span><span class="p">(</span><span class="n">o_</span><span class="p">,</span> <span class="n">pi_</span><span class="p">)</span>

        <span class="c1"># ==== LATC code ===============================</span>
        <span class="c1"># NOTE: Added here to reduce code duplication.</span>
        <span class="c1"># Retrieve second target Lyapunov value.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_twin_critic</span><span class="p">:</span>
            <span class="n">lya_l2_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">L2</span><span class="p">(</span><span class="n">o_</span><span class="p">,</span> <span class="n">pi_</span><span class="p">)</span>
            <span class="n">lya_l_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
                <span class="n">lya_l_</span><span class="p">,</span> <span class="n">lya_l2_</span>
            <span class="p">)</span>  <span class="c1"># Use max clipping to prevent underestimation bias.</span>
        <span class="c1"># ==============================================</span>

        <span class="c1"># Compute Lyapunov Actor error.</span>
        <span class="n">l_delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">lya_l_</span> <span class="o">-</span> <span class="n">l1</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha3</span> <span class="o">*</span> <span class="n">r</span><span class="p">)</span>  <span class="c1"># See Han eq. 11</span>

        <span class="c1"># Calculate entropy-regularized policy loss</span>
        <span class="n">a_loss</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labda</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">*</span> <span class="n">l_delta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">*</span> <span class="n">logp_pi</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="p">)</span>  <span class="c1"># See Han eq. 12</span>

        <span class="n">a_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pi_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">pi_info</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">LogPi</span><span class="o">=</span><span class="n">logp_pi</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
            <span class="n">Entropy</span><span class="o">=-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">logp_pi</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="n">diagnostics</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="o">**</span><span class="n">pi_info</span><span class="p">,</span> <span class="s2">&quot;LossPi&quot;</span><span class="p">:</span> <span class="n">a_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()})</span>

        <span class="c1"># Q networks so you can optimize it at next SGD step.</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_params</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1">################################################</span>
        <span class="c1"># Optimize alpha (Entropy temperature) #########</span>
        <span class="c1">################################################</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adaptive_temperature</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_alpha_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># Calculate alpha loss.</span>
            <span class="n">alpha_loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">logp_pi</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_entropy</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># See Haarnoja eq. 17</span>

            <span class="n">alpha_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_alpha_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">alpha_info</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">Alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">diagnostics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span><span class="o">**</span><span class="n">alpha_info</span><span class="p">,</span> <span class="s2">&quot;LossAlpha&quot;</span><span class="p">:</span> <span class="n">alpha_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()}</span>
            <span class="p">)</span>
        <span class="c1">################################################</span>
        <span class="c1"># Optimize labda (Lyapunov temperature) ########</span>
        <span class="c1">################################################</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_log_labda_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Calculate labda loss.</span>
        <span class="c1"># NOTE: Log_labda was used in the lambda_loss function because using lambda</span>
        <span class="c1"># caused the gradients to vanish. This is caused since we restrict lambda</span>
        <span class="c1"># within a 0-1.0 range using the clamp function (see #34). Using log_lambda</span>
        <span class="c1"># also is more numerically stable.</span>
        <span class="n">labda_loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_labda</span> <span class="o">*</span> <span class="n">l_delta</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># See formulas under Han eq. 14</span>

        <span class="n">labda_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_log_labda_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">labda_info</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">Lambda</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">labda</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">diagnostics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span><span class="o">**</span><span class="n">labda_info</span><span class="p">,</span> <span class="s2">&quot;LossLambda&quot;</span><span class="p">:</span> <span class="n">labda_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()}</span>
        <span class="p">)</span>
        <span class="c1">################################################</span>
        <span class="c1"># Update target networks and return ############</span>
        <span class="c1"># diagnostics. #################################</span>
        <span class="c1">################################################</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_targets</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">diagnostics</span></div>

<div class="viewcode-block" id="LAC.save"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.save">[docs]</a>    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Can be used to save the current model state.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (str): The path where you want to save the policy.</span>

<span class="sd">        Raises:</span>
<span class="sd">            Exception: Raises an exception if something goes wrong during saving.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

        <span class="n">save_path</span> <span class="o">=</span> <span class="n">path</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s2">&quot;policy/model.pt&quot;</span><span class="p">)</span>
        <span class="n">save_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_state_dict</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;LAC model could not be saved.&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

        <span class="c1"># Save additional information.</span>
        <span class="n">save_info</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;alg_name&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
            <span class="s2">&quot;setup_kwargs&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_setup_kwargs</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">save_to_json</span><span class="p">(</span>
            <span class="n">input_object</span><span class="o">=</span><span class="n">save_info</span><span class="p">,</span>
            <span class="n">output_filename</span><span class="o">=</span><span class="s2">&quot;save_info.json&quot;</span><span class="p">,</span>
            <span class="n">output_path</span><span class="o">=</span><span class="n">save_path</span><span class="o">.</span><span class="n">parent</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="LAC.restore"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.restore">[docs]</a>    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">restore_lagrance_multipliers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Restores a already trained policy. Used for transfer learning.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (str): The path where the model :attr:`state_dict` of the policy is</span>
<span class="sd">                found.</span>
<span class="sd">            restore_lagrance_multipliers (bool, optional): Whether you want to restore</span>
<span class="sd">                the Lagrance multipliers. By fault ``False``.</span>

<span class="sd">        Raises:</span>
<span class="sd">            Exception: Raises an exception if something goes wrong during loading.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">osp</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="o">!=</span> <span class="p">[</span><span class="s2">&quot;model.pt&quot;</span><span class="p">,</span> <span class="s2">&quot;model.pth&quot;</span><span class="p">]:</span>
            <span class="n">load_path</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span>
                <span class="n">osp</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;**&quot;</span><span class="p">,</span> <span class="s2">&quot;model_state.pt*&quot;</span><span class="p">),</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">load_path</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">model_path</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span>
                    <span class="n">osp</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;**&quot;</span><span class="p">,</span> <span class="s2">&quot;model.pt*&quot;</span><span class="p">),</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Only whole pickled models found in &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;. Using whole &quot;</span>
                        <span class="s2">&quot;pickled models as a starting point is currently not supported &quot;</span>
                        <span class="s2">&quot;as this method of loading is discouraged by the pytorch &quot;</span>
                        <span class="s2">&quot;documentation. Please supply a path that contains a &quot;</span>
                        <span class="s2">&quot;&#39;model_state&#39; dictionary and try again.&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;No models found in &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;. Please check your policy restore&quot;</span>
                        <span class="s2">&quot;path and try again.&quot;</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">load_path</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Multiple models found in path &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;. Please check your policy &quot;</span>
                    <span class="s2">&quot;restore path and try again.&quot;</span>
                <span class="p">)</span>
            <span class="n">load_path</span> <span class="o">=</span> <span class="n">load_path</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">restored_model_state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">load_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span>
            <span class="n">restored_model_state_dict</span><span class="p">,</span>
            <span class="n">restore_lagrance_multipliers</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ac_targ</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span></div>

<div class="viewcode-block" id="LAC.export"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.export">[docs]</a>    <span class="k">def</span> <span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Can be used to export the model as a ``TorchScript`` such that it can be</span>
<span class="sd">        deployed to hardware.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (str): The path where you want to export the policy too.</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: Raised until the feature is fixed on the upstream.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># IMPROVE: Replace with TorchScript and Onyx versions when</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/29843 and</span>
        <span class="c1"># https://github.com/onnx/onnx/issues/3033 are solved (see</span>
        <span class="c1"># https://pytorch.org/tutorials/advanced/cpp_export.html and</span>
        <span class="c1"># https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html)</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;The LAC Pytorch module could not be exported as a &#39;TorchScript&#39; since the &quot;</span>
            <span class="s2">&quot;&#39;torch.distributions.normal&#39; method is not yet &#39;TorchScript&#39; compatible. &quot;</span>
            <span class="s2">&quot;The feature will be implemented when this is added to the upstream &quot;</span>
            <span class="s2">&quot;see https://github.com/pytorch/pytorch/issues/29843 to follow progress on &quot;</span>
            <span class="s2">&quot;this.&quot;</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="LAC.load_state_dict"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.load_state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">restore_lagrance_multipliers</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Copies parameters and buffers from :attr:`state_dict` into</span>
<span class="sd">        this module and its descendants.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): a dict containing parameters and</span>
<span class="sd">                persistent buffers.</span>
<span class="sd">            restore_lagrance_multipliers (bool, optional): Whether you want to restore</span>
<span class="sd">                the Lagrance multipliers. By fault ``True``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="s2">&quot;alg_name&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
            <span class="ow">and</span> <span class="s2">&quot;alg_name&quot;</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="s2">&quot;alg_name&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;alg_name&quot;</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="n">friendly_err</span><span class="p">(</span>
                    <span class="s2">&quot;The supplied &#39;state_dict&#39; could not be loaded onto the </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="s2">&quot;alg_name&quot;</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot;agent as it belongs to a </span><span class="si">{}</span><span class="s2"> agent. Please supply a </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;alg_name&quot;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="s2">&quot;alg_name&quot;</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot;compatible &#39;state_dict&#39; and try again.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="o">!=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="n">friendly_err</span><span class="p">(</span>
                    <span class="s2">&quot;The &#39;state_dict&#39; you tried to load does not seem to be right. It &quot;</span>
                    <span class="s2">&quot;contains keys: </span><span class="se">\n\n</span><span class="si">{}</span><span class="se">\n\n</span><span class="s2"> while keys: </span><span class="se">\n\n</span><span class="si">{}</span><span class="se">\n\n</span><span class="s2"> keys &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
                    <span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot;are expected for the &#39;</span><span class="si">{}</span><span class="s2">&#39; model.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="s2">&quot;alg_name&quot;</span><span class="p">]</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">restore_lagrance_multipliers</span><span class="p">:</span>
            <span class="n">log_to_std_out</span><span class="p">(</span>
                <span class="s2">&quot;Keeping Lagrance multipliers at their initial value.&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span>
            <span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;log_alpha&quot;</span><span class="p">],</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;log_labda&quot;</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">log_to_std_out</span><span class="p">(</span><span class="s2">&quot;Restoring Lagrance multipliers.&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">AttributeError</span><span class="p">,</span> <span class="ne">RuntimeError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)(</span>
                <span class="s2">&quot;The &#39;state_dict&#39; could not be loaded successfully.&quot;</span><span class="p">,</span>
            <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span></div>

<div class="viewcode-block" id="LAC.state_dict"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simple wrapper around the :meth:`torch.nn.Module.state_dict` method that</span>
<span class="sd">        saves the current class name. This is used to enable easy loading of the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;alg_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="p">)</span>  <span class="c1"># Save algorithm name state dict.</span>
        <span class="k">return</span> <span class="n">state_dict</span></div>

<div class="viewcode-block" id="LAC.bound_lr"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.bound_lr">[docs]</a>    <span class="k">def</span> <span class="nf">bound_lr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">lr_a_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lr_c_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lr_alpha_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lr_labda_final</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Function that can be used to make sure the learning rate doesn&#39;t go beyond</span>
<span class="sd">        a lower bound.</span>

<span class="sd">        Args:</span>
<span class="sd">            lr_a_final (float, optional): The lower bound for the actor learning rate.</span>
<span class="sd">                Defaults to ``None``.</span>
<span class="sd">            lr_c_final (float, optional): The lower bound for the critic learning rate.</span>
<span class="sd">                Defaults to ``None``.</span>
<span class="sd">            lr_alpha_final (float, optional): The lower bound for the alpha Lagrance</span>
<span class="sd">                multiplier learning rate. Defaults to ``None``.</span>
<span class="sd">            lr_labda_final (float, optional): The lower bound for the labda Lagrance</span>
<span class="sd">                multiplier learning rate. Defaults to ``None``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">lr_a_final</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pi_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lr_a_final</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_pi_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_a_final</span>
        <span class="k">if</span> <span class="n">lr_c_final</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lr_c_final</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_c_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_c_final</span>
        <span class="k">if</span> <span class="n">lr_alpha_final</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_alpha_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lr_alpha_final</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log_alpha_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_alpha_final</span>
        <span class="k">if</span> <span class="n">lr_labda_final</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_labda_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lr_labda_final</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log_labda_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_labda_final</span></div>

<div class="viewcode-block" id="LAC._update_targets"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._update_targets">[docs]</a>    <span class="k">def</span> <span class="nf">_update_targets</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Updates the target networks based on a Exponential moving average</span>
<span class="sd">        (Polyak averaging).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">p_targ</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">ac_targ</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
                <span class="c1"># NOTE: We use an in-place operations &quot;mul_&quot;, &quot;add_&quot; to update target</span>
                <span class="c1"># params, as opposed to &quot;mul&quot; and &quot;add&quot;, which would make</span>
                <span class="c1"># new tensors.</span>
                <span class="n">p_targ</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_polyak</span><span class="p">)</span>
                <span class="n">p_targ</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_polyak</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span></div>

<div class="viewcode-block" id="LAC._set_learning_rates"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC._set_learning_rates">[docs]</a>    <span class="k">def</span> <span class="nf">_set_learning_rates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr_a</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lr_c</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lr_alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lr_labda</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Can be used to manually adjusts the learning rates of the optimizers.</span>

<span class="sd">        Args:</span>
<span class="sd">            lr_a (float, optional): The learning rate of the actor optimizer. Defaults</span>
<span class="sd">                to ``None``.</span>
<span class="sd">            lr_c (float, optional): The learning rate of the (Lyapunov) Critic. Defaults</span>
<span class="sd">                to ``None``.</span>
<span class="sd">            lr_alpha (float, optional): The learning rate of the temperature optimizer.</span>
<span class="sd">                Defaults to ``None``.</span>
<span class="sd">            lr_labda (float, optional): The learning rate of the Lyapunov Lagrance</span>
<span class="sd">                multiplier optimizer. Defaults to ``None``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">lr_a</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pi_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_a</span>
        <span class="k">if</span> <span class="n">lr_c</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_c_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_c</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adaptive_temperature</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">lr_alpha</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log_alpha_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_alpha</span>
        <span class="k">if</span> <span class="n">lr_labda</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_labda_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_labda</span></div>

    <span class="nd">@property</span>
<div class="viewcode-block" id="LAC.alpha"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.alpha">[docs]</a>    <span class="k">def</span> <span class="nf">alpha</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Property used to clip :attr:`alpha` to be equal or bigger than ``0.0`` to</span>
<span class="sd">        prevent it from becoming nan when :attr:`log_alpha` becomes ``-inf``. For</span>
<span class="sd">        :attr:`alpha` no upper bound is used.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># NOTE: Clamping isn&#39;t needed when alpha max is np.inf due to the exponential.</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_alpha</span><span class="o">.</span><span class="n">exp</span><span class="p">(),</span> <span class="o">*</span><span class="n">SCALE_ALPHA_MIN_MAX</span><span class="p">)</span></div>

    <span class="nd">@alpha</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">alpha</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_val</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Property used to ensure :attr:`alpha` and :attr:`log_alpha` are related.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_alpha</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1e-37</span> <span class="k">if</span> <span class="n">set_val</span> <span class="o">&lt;</span> <span class="mf">1e-37</span> <span class="k">else</span> <span class="n">set_val</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_alpha</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
<div class="viewcode-block" id="LAC.labda"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.labda">[docs]</a>    <span class="k">def</span> <span class="nf">labda</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Property used to clip :attr:`lambda` to be equal or bigger than ``0.0`` in</span>
<span class="sd">        order to prevent it from becoming ``nan`` when log_labda becomes -inf. Further</span>
<span class="sd">        we clip it to be lower or equal than ``1.0`` in order to prevent lambda from</span>
<span class="sd">        exploding when the the hyperparameters are chosen badly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_labda</span><span class="o">.</span><span class="n">exp</span><span class="p">(),</span> <span class="o">*</span><span class="n">SCALE_LAMBDA_MIN_MAX</span><span class="p">)</span></div>

    <span class="nd">@labda</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">labda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_val</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Property used to make sure labda and log_labda are related.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_labda</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1e-37</span> <span class="k">if</span> <span class="n">set_val</span> <span class="o">&lt;</span> <span class="mf">1e-37</span> <span class="k">else</span> <span class="n">set_val</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_labda</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
<div class="viewcode-block" id="LAC.target_entropy"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.target_entropy">[docs]</a>    <span class="k">def</span> <span class="nf">target_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The target entropy used while learning the entropy temperature</span>
<span class="sd">        :attr:`alpha`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_entropy</span></div>

    <span class="nd">@target_entropy</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">target_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_val</span><span class="p">):</span>
        <span class="n">error_msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;Changing the &#39;target_entropy&#39; during training is not allowed.&quot;</span>
            <span class="s2">&quot;Please open a feature/pull request on &quot;</span>
            <span class="s2">&quot;https://github.com/rickstaa/stable-learning-control/issues if you need &quot;</span>
            <span class="s2">&quot;this.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">error_msg</span><span class="p">)</span>

    <span class="nd">@property</span>
<div class="viewcode-block" id="LAC.device"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.LAC.device">[docs]</a>    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The device the networks are placed on (options: ``cpu``, ``gpu``, ``gpu:0``,</span>
<span class="sd">        ``gpu:1``, etc.).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span></div>

    <span class="nd">@device</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_val</span><span class="p">):</span>
        <span class="n">error_msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;Changing the computational &#39;device&#39; during training is not allowed.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">error_msg</span><span class="p">)</span></div>


<div class="viewcode-block" id="validate_args"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.validate_args">[docs]</a><span class="k">def</span> <span class="nf">validate_args</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks if the input arguments have valid values.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If a value is invalid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;update_after&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;steps_per_epoch&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You can not set &#39;update_after&#39; bigger than the &#39;steps_per_epoch&#39;. Please &quot;</span>
            <span class="s2">&quot;change this and try again.&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="lac"><a class="viewcode-back" href="../../../../../usage/algorithms/lac.html#stable_learning_control.algos.pytorch.lac">[docs]</a><span class="k">def</span> <span class="nf">lac</span><span class="p">(</span>
    <span class="n">env_fn</span><span class="p">,</span>
    <span class="n">actor_critic</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ac_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;actor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;critic&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">},</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
        <span class="n">output_activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">opt_type</span><span class="o">=</span><span class="s2">&quot;minimize&quot;</span><span class="p">,</span>
    <span class="n">max_ep_len</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="n">start_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">update_every</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">update_after</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">steps_per_update</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">num_test_episodes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">alpha3</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">labda</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
    <span class="n">polyak</span><span class="o">=</span><span class="mf">0.995</span><span class="p">,</span>
    <span class="n">target_entropy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">adaptive_temperature</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">lr_a</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">lr_c</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span>
    <span class="n">lr_alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">lr_labda</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span>
    <span class="n">lr_a_final</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
    <span class="n">lr_c_final</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
    <span class="n">lr_alpha_final</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
    <span class="n">lr_labda_final</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
    <span class="n">lr_decay_type</span><span class="o">=</span><span class="n">DEFAULT_DECAY_TYPE</span><span class="p">,</span>
    <span class="n">lr_a_decay_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lr_c_decay_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lr_alpha_decay_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lr_labda_decay_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lr_decay_ref</span><span class="o">=</span><span class="n">DEFAULT_DECAY_REFERENCE</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">replay_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span>
    <span class="n">horizon_length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">logger_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span>
    <span class="n">save_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">start_policy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">export</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Trains the LAC algorithm in a given environment.</span>

<span class="sd">    Args:</span>
<span class="sd">        env_fn: A function which creates a copy of the environment. The environment</span>
<span class="sd">            must satisfy the gymnasium API.</span>
<span class="sd">        actor_critic (torch.nn.Module, optional): The constructor method for a</span>
<span class="sd">            Torch Module with an ``act`` method, a ``pi`` module and several</span>
<span class="sd">            ``Q`` or ``L`` modules. The ``act`` method and ``pi`` module should</span>
<span class="sd">            accept batches of observations as inputs, and the ``Q*`` and ``L``</span>
<span class="sd">            modules should accept a batch of observations and a batch of actions as</span>
<span class="sd">            inputs. When called, these modules should return:</span>

<span class="sd">            ===========  ================  ======================================</span>
<span class="sd">            Call         Output Shape      Description</span>
<span class="sd">            ===========  ================  ======================================</span>
<span class="sd">            ``act``      (batch, act_dim)   | Numpy array of actions for each</span>
<span class="sd">                                            | observation.</span>
<span class="sd">            ``Q*/L``     (batch,)           | Tensor containing one current estimate</span>
<span class="sd">                                            | of ``Q*/L`` for the provided</span>
<span class="sd">                                            | observations and actions. (Critical:</span>
<span class="sd">                                            | make sure to flatten this!)</span>
<span class="sd">            ===========  ================  ======================================</span>

<span class="sd">            Calling ``pi`` should return:</span>

<span class="sd">            ===========  ================  ======================================</span>
<span class="sd">            Symbol       Shape             Description</span>
<span class="sd">            ===========  ================  ======================================</span>
<span class="sd">            ``a``        (batch, act_dim)   | Tensor containing actions from policy</span>
<span class="sd">                                            | given observations.</span>
<span class="sd">            ``logp_pi``  (batch,)           | Tensor containing log probabilities of</span>
<span class="sd">                                            | actions in ``a``. Importantly:</span>
<span class="sd">                                            | gradients should be able to flow back</span>
<span class="sd">                                            | into ``a``.</span>
<span class="sd">            ===========  ================  ======================================</span>

<span class="sd">            Defaults to</span>
<span class="sd">            :class:`~stable_learning_control.algos.pytorch.policies.lyapunov_actor_critic.LyapunovActorCritic`</span>
<span class="sd">        ac_kwargs (dict, optional): Any kwargs appropriate for the ActorCritic</span>
<span class="sd">            object you provided to LAC. Defaults to:</span>

<span class="sd">            =======================  ============================================</span>
<span class="sd">            Kwarg                    Value</span>
<span class="sd">            =======================  ============================================</span>
<span class="sd">            ``hidden_sizes_actor``    ``256 x 2``</span>
<span class="sd">            ``hidden_sizes_critic``   ``256 x 2``</span>
<span class="sd">            ``activation``            :class:`torch.nn.ReLU`</span>
<span class="sd">            ``output_activation``     :class:`torch.nn.ReLU`</span>
<span class="sd">            =======================  ============================================</span>
<span class="sd">        opt_type (str, optional): The optimization type you want to use. Options</span>
<span class="sd">            ``maximize`` and ``minimize``. Defaults to ``maximize``.</span>
<span class="sd">        max_ep_len (int, optional): Maximum length of trajectory / episode /</span>
<span class="sd">            rollout. Defaults to the environment maximum.</span>
<span class="sd">        epochs (int, optional): Number of epochs to run and train agent. Defaults</span>
<span class="sd">            to ``100``.</span>
<span class="sd">        steps_per_epoch (int, optional): Number of steps of interaction</span>
<span class="sd">            (state-action pairs) for the agent and the environment in each epoch.</span>
<span class="sd">            Defaults to ``2048``.</span>
<span class="sd">        start_steps (int, optional): Number of steps for uniform-random action</span>
<span class="sd">            selection, before running real policy. Helps exploration. Defaults to</span>
<span class="sd">            ``0``.</span>
<span class="sd">        update_every (int, optional): Number of env interactions that should elapse</span>
<span class="sd">            between gradient descent updates. Defaults to ``100``.</span>
<span class="sd">        update_after (int, optional): Number of env interactions to collect before</span>
<span class="sd">            starting to do gradient descent updates. Ensures replay buffer</span>
<span class="sd">            is full enough for useful updates. Defaults to ``1000``.</span>
<span class="sd">        steps_per_update (int, optional): Number of gradient descent steps that are</span>
<span class="sd">            performed for each gradient descent update. This determines the ratio of</span>
<span class="sd">            env steps to gradient steps (i.e. :obj:`update_every`/</span>
<span class="sd">            :obj:`steps_per_update`). Defaults to ``100``.</span>
<span class="sd">        num_test_episodes (int, optional): Number of episodes used to test the</span>
<span class="sd">            deterministic policy at the end of each epoch. This is used for logging</span>
<span class="sd">            the performance. Defaults to ``10``.</span>
<span class="sd">        alpha (float, optional): Entropy regularization coefficient (Equivalent to</span>
<span class="sd">            inverse of reward scale in the original SAC paper). Defaults to</span>
<span class="sd">            ``0.99``.</span>
<span class="sd">        alpha3 (float, optional): The Lyapunov constraint error boundary. Defaults</span>
<span class="sd">            to ``0.2``.</span>
<span class="sd">        labda (float, optional): The Lyapunov Lagrance multiplier. Defaults to</span>
<span class="sd">            ``0.99``.</span>
<span class="sd">        gamma (float, optional): Discount factor. (Always between 0 and 1.).</span>
<span class="sd">            Defaults to ``0.99``.</span>
<span class="sd">        polyak (float, optional): Interpolation factor in polyak averaging for</span>
<span class="sd">            target networks. Target networks are updated towards main networks</span>
<span class="sd">            according to:</span>

<span class="sd">            .. math:: \\theta_{\\text{targ}} \\leftarrow</span>
<span class="sd">                \\rho \\theta_{\\text{targ}} + (1-\\rho) \\theta</span>

<span class="sd">            where :math:`\\rho` is polyak (Always between 0 and 1, usually close to 1.).</span>
<span class="sd">            In some papers :math:`\\rho` is defined as (1 - :math:`\\tau`) where</span>
<span class="sd">            :math:`\\tau` is the soft replacement factor. Defaults to ``0.995``.</span>
<span class="sd">        target_entropy (float, optional): Initial target entropy used while learning</span>
<span class="sd">            the entropy temperature (alpha). Defaults to the</span>
<span class="sd">            maximum information (bits) contained in action space. This can be</span>
<span class="sd">            calculated according to :</span>

<span class="sd">            .. math::</span>
<span class="sd">                -{\\prod }_{i=0}^{n}action\\_di{m}_{i}\\phantom{\\rule{0ex}{0ex}}</span>
<span class="sd">        adaptive_temperature (bool, optional): Enabled Automating Entropy Adjustment</span>
<span class="sd">            for maximum Entropy RL_learning.</span>
<span class="sd">        lr_a (float, optional): Learning rate used for the actor. Defaults to</span>
<span class="sd">            ``1e-4``.</span>
<span class="sd">        lr_c (float, optional): Learning rate used for the (lyapunov) critic.</span>
<span class="sd">            Defaults to ``1e-4``.</span>
<span class="sd">        lr_alpha (float, optional): Learning rate used for the entropy temperature.</span>
<span class="sd">            Defaults to ``1e-4``.</span>
<span class="sd">        lr_labda (float, optional): Learning rate used for the Lyapunov Lagrance</span>
<span class="sd">            multiplier. Defaults to ``3e-4``.</span>
<span class="sd">        lr_a_final(float, optional): The final actor learning rate that is achieved</span>
<span class="sd">            at the end of the training. Defaults to ``1e-10``.</span>
<span class="sd">        lr_c_final(float, optional): The final critic learning rate that is achieved</span>
<span class="sd">            at the end of the training. Defaults to ``1e-10``.</span>
<span class="sd">        lr_alpha_final(float, optional): The final alpha learning rate that is</span>
<span class="sd">            achieved at the end of the training. Defaults to ``1e-10``.</span>
<span class="sd">        lr_labda_final(float, optional): The final labda learning rate that is</span>
<span class="sd">            achieved at the end of the training. Defaults to ``1e-10``.</span>
<span class="sd">        lr_decay_type (str, optional): The learning rate decay type that is used (options</span>
<span class="sd">            are: ``linear`` and ``exponential`` and ``constant``). Defaults to</span>
<span class="sd">            ``linear``.Can be overridden by the specific learning rate decay types.</span>
<span class="sd">        lr_a_decay_type (str, optional): The learning rate decay type that is used for</span>
<span class="sd">            the actor learning rate (options are: ``linear`` and ``exponential`` and</span>
<span class="sd">            ``constant``). If not specified, the general learning rate decay type is used.</span>
<span class="sd">        lr_c_decay_type (str, optional): The learning rate decay type that is used for</span>
<span class="sd">            the critic learning rate (options are: ``linear`` and ``exponential`` and</span>
<span class="sd">            ``constant``). If not specified, the general learning rate decay type is used.</span>
<span class="sd">        lr_alpha_decay_type (str, optional): The learning rate decay type that is used</span>
<span class="sd">            for the alpha learning rate (options are: ``linear`` and ``exponential``</span>
<span class="sd">            and ``constant``). If not specified, the general learning rate decay type is used.</span>
<span class="sd">        lr_labda_decay_type (str, optional): The learning rate decay type that is used</span>
<span class="sd">            for the labda learning rate (options are: ``linear`` and ``exponential``</span>
<span class="sd">            and ``constant``). If not specified, the general learning rate decay type is used.</span>
<span class="sd">        lr_decay_ref (str, optional): The reference variable that is used for decaying</span>
<span class="sd">            the learning rate (options: ``epoch`` and ``step``). Defaults to ``epoch``.</span>
<span class="sd">        batch_size (int, optional): Minibatch size for SGD. Defaults to ``256``.</span>
<span class="sd">        replay_size (int, optional): Maximum length of replay buffer. Defaults to</span>
<span class="sd">            ``1e6``.</span>
<span class="sd">        horizon_length (int, optional): The length of the finite-horizon used for the</span>
<span class="sd">            Lyapunov Critic target. Defaults to ``0`` meaning the infinite-horizon</span>
<span class="sd">            bellman backup is used.</span>
<span class="sd">        seed (int): Seed for random number generators. Defaults to ``None``.</span>
<span class="sd">        device (str, optional): The device the networks are placed on (options: ``cpu``,</span>
<span class="sd">            ``gpu``, ``gpu:0``, ``gpu:1``, etc.). Defaults to ``cpu``.</span>
<span class="sd">        logger_kwargs (dict, optional): Keyword args for EpochLogger.</span>
<span class="sd">        save_freq (int, optional): How often (in terms of gap between epochs) to save</span>
<span class="sd">            the current policy and value function.</span>
<span class="sd">        start_policy (str): Path of a already trained policy to use as the starting</span>
<span class="sd">            point for the training. By default a new policy is created.</span>
<span class="sd">        export (bool): Whether you want to export the model as a ``TorchScript`` such</span>
<span class="sd">            that it can be deployed on hardware. By default ``False``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (tuple): tuple containing:</span>

<span class="sd">            -   policy (:class:`LAC`): The trained actor-critic policy.</span>
<span class="sd">            -   replay_buffer (union[:class:`~stable_learning_control.algos.pytorch.common.buffers.ReplayBuffer`, :class:`~stable_learning_control.algos.pytorch.common.buffers.FiniteHorizonReplayBuffer`]):</span>
<span class="sd">                The replay buffer used during training.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501, D301</span>
    <span class="n">update_after</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">update_after</span><span class="p">)</span>  <span class="c1"># You can not update before the first step.</span>
    <span class="n">validate_args</span><span class="p">(</span><span class="o">**</span><span class="nb">locals</span><span class="p">())</span>

    <span class="c1"># Retrieve hyperparameters while filtering out the logger_kwargs.</span>
    <span class="n">hyper_param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;logger_kwargs&quot;</span><span class="p">]}</span>

    <span class="c1"># Setup algorithm parameters.</span>
    <span class="n">total_steps</span> <span class="o">=</span> <span class="n">steps_per_epoch</span> <span class="o">*</span> <span class="n">epochs</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">env_fn</span><span class="p">()</span>
    <span class="n">hyper_param_dict</span><span class="p">[</span><span class="s2">&quot;env&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">env</span>  <span class="c1"># Add env to hyperparameters.</span>

    <span class="c1"># Validate gymnasium env.</span>
    <span class="c1"># NOTE: The current implementation only works with continuous spaces.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_gym_env</span><span class="p">(</span><span class="n">env</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Env must be a valid gymnasium environment.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_discrete_space</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_discrete_space</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;The LAC algorithm does not yet support discrete observation/action &quot;</span>
            <span class="s2">&quot;spaces. Please open a feature/pull request on &quot;</span>
            <span class="s2">&quot;https://github.com/rickstaa/stable-learning-control/issues if you &quot;</span>
            <span class="s2">&quot;need this.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Create test environment.</span>
    <span class="k">if</span> <span class="n">num_test_episodes</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">test_env</span> <span class="o">=</span> <span class="n">env_fn</span><span class="p">()</span>

    <span class="c1"># Flatten observation space and get observation, action and reward space dimensions.</span>
    <span class="c1"># NOTE: Done to ensure the algorithm works with GoalEnv environments. See</span>
    <span class="c1"># https://robotics.farama.org/content/multi-goal_api/#goalenv.</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">FlattenObservation</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
    <span class="n">test_env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">FlattenObservation</span><span class="p">(</span><span class="n">test_env</span><span class="p">)</span>
    <span class="n">obs_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">act_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Setup logger.</span>
    <span class="n">logger_kwargs</span><span class="p">[</span><span class="s2">&quot;quiet&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">logger_kwargs</span><span class="p">[</span><span class="s2">&quot;quiet&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;quiet&quot;</span> <span class="ow">in</span> <span class="n">logger_kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">else</span> <span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">logger_kwargs</span><span class="p">[</span><span class="s2">&quot;verbose_vars&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">logger_kwargs</span><span class="p">[</span><span class="s2">&quot;verbose_vars&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="s2">&quot;verbose_vars&quot;</span> <span class="ow">in</span> <span class="n">logger_kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="ow">and</span> <span class="n">logger_kwargs</span><span class="p">[</span><span class="s2">&quot;verbose_vars&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="k">else</span> <span class="n">STD_OUT_LOG_VARS_DEFAULT</span>
    <span class="p">)</span>  <span class="c1"># NOTE: Done to ensure the stdout doesn&#39;t get cluttered.</span>
    <span class="n">tb_low_log_freq</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">logger_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tb_log_freq&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;low&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;tb_log_freq&quot;</span> <span class="ow">in</span> <span class="n">logger_kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="k">else</span> <span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">use_tensorboard</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">logger_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_tensorboard&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;use_tensorboard&quot;</span> <span class="ow">in</span> <span class="n">logger_kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="k">else</span> <span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">use_wandb</span> <span class="o">=</span> <span class="n">logger_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;use_wandb&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_wandb</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">logger_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;wandb_run_name&quot;</span><span class="p">):</span>
        <span class="c1"># Create wandb_run_name if wandb is used and no name is provided.</span>
        <span class="n">logger_kwargs</span><span class="p">[</span><span class="s2">&quot;wandb_run_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">PurePath</span><span class="p">(</span><span class="n">logger_kwargs</span><span class="p">[</span><span class="s2">&quot;output_dir&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">parts</span><span class="p">[</span>
            <span class="o">-</span><span class="mi">1</span>
        <span class="p">]</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">EpochLogger</span><span class="p">(</span><span class="o">**</span><span class="n">logger_kwargs</span><span class="p">)</span>

    <span class="c1"># Retrieve max episode length.</span>
    <span class="k">if</span> <span class="n">max_ep_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_ep_len</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">_max_episode_steps</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">max_ep_len</span> <span class="o">&gt;</span> <span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">_max_episode_steps</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;You defined your &#39;max_ep_len&#39; to be </span><span class="si">{</span><span class="n">max_ep_len</span><span class="si">}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;while the environment &#39;max_episode_steps&#39; is &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">_max_episode_steps</span><span class="si">}</span><span class="s2">. As a result the environment &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;&#39;max_episode_steps&#39; has been increased to </span><span class="si">{</span><span class="n">max_ep_len</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">),</span>
                <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;warning&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">_max_episode_steps</span> <span class="o">=</span> <span class="n">max_ep_len</span>
            <span class="k">if</span> <span class="n">num_test_episodes</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">test_env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">_max_episode_steps</span> <span class="o">=</span> <span class="n">max_ep_len</span>
    <span class="k">if</span> <span class="n">hyper_param_dict</span><span class="p">[</span><span class="s2">&quot;max_ep_len&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># Store in hyperparameter dict.</span>
        <span class="n">hyper_param_dict</span><span class="p">[</span><span class="s2">&quot;max_ep_len&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_ep_len</span>

    <span class="c1"># Save experiment config to logger.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">save_config</span><span class="p">(</span><span class="n">hyper_param_dict</span><span class="p">)</span>

    <span class="c1"># Get default actor critic if no &#39;actor_critic&#39; was supplied</span>
    <span class="n">actor_critic</span> <span class="o">=</span> <span class="n">LyapunovActorCritic</span> <span class="k">if</span> <span class="n">actor_critic</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">actor_critic</span>

    <span class="c1"># Ensure the environment is correctly seeded.</span>
    <span class="c1"># NOTE: Done here since we don&#39;t want to seed on every env.reset() call.</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">env</span><span class="o">.</span><span class="n">np_random</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">seeding</span><span class="o">.</span><span class="n">np_random</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">test_env</span><span class="o">.</span><span class="n">np_random</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">seeding</span><span class="o">.</span><span class="n">np_random</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">test_env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">test_env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Set other random seed for reproducible policy results.</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># Ensure numpy is deterministic.</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># Ensure pytorch is deterministic.</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># Ensure python is deterministic.</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTHONHASHSEED&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span>
            <span class="n">seed</span>
        <span class="p">)</span>  <span class="c1"># Ensure python hashing is deterministic.</span>
        <span class="c1"># torch.use_deterministic_algorithms(True)  # Disable for reproducibility.</span>
        <span class="c1"># torch.backends.cudnn.benchmark = False  # Disable for reproducibility.</span>

    <span class="n">policy</span> <span class="o">=</span> <span class="n">LAC</span><span class="p">(</span>
        <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span>
        <span class="n">actor_critic</span><span class="o">=</span><span class="n">actor_critic</span><span class="p">,</span>
        <span class="n">ac_kwargs</span><span class="o">=</span><span class="n">ac_kwargs</span><span class="p">,</span>
        <span class="n">opt_type</span><span class="o">=</span><span class="n">opt_type</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
        <span class="n">alpha3</span><span class="o">=</span><span class="n">alpha3</span><span class="p">,</span>
        <span class="n">labda</span><span class="o">=</span><span class="n">labda</span><span class="p">,</span>
        <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
        <span class="n">polyak</span><span class="o">=</span><span class="n">polyak</span><span class="p">,</span>
        <span class="n">target_entropy</span><span class="o">=</span><span class="n">target_entropy</span><span class="p">,</span>
        <span class="n">adaptive_temperature</span><span class="o">=</span><span class="n">adaptive_temperature</span><span class="p">,</span>
        <span class="n">lr_a</span><span class="o">=</span><span class="n">lr_a</span><span class="p">,</span>
        <span class="n">lr_c</span><span class="o">=</span><span class="n">lr_c</span><span class="p">,</span>
        <span class="n">lr_alpha</span><span class="o">=</span><span class="n">lr_alpha</span><span class="p">,</span>
        <span class="n">lr_labda</span><span class="o">=</span><span class="n">lr_labda</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Restore policy if supplied.</span>
    <span class="k">if</span> <span class="n">start_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Restoring model from &#39;</span><span class="si">{</span><span class="n">start_policy</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">start_policy</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Model successfully restored.&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">err_str</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Training process has been terminated. Unable to restore the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;start_policy&#39; from &#39;</span><span class="si">{</span><span class="n">start_policy</span><span class="si">}</span><span class="s2">&#39;. Please ensure the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;start_policy&#39; is correct and try again. Error details: </span><span class="si">{</span><span class="n">err_str</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;error&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Initialize replay buffer.</span>
    <span class="k">if</span> <span class="n">horizon_length</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Finite-horizon.</span>
        <span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">FiniteHorizonReplayBuffer</span><span class="p">(</span>
            <span class="n">obs_dim</span><span class="o">=</span><span class="n">obs_dim</span><span class="p">,</span>
            <span class="n">act_dim</span><span class="o">=</span><span class="n">act_dim</span><span class="p">,</span>
            <span class="n">size</span><span class="o">=</span><span class="n">replay_size</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">policy</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">horizon_length</span><span class="o">=</span><span class="n">horizon_length</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Infinite-horizon.</span>
        <span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span>
            <span class="n">obs_dim</span><span class="o">=</span><span class="n">obs_dim</span><span class="p">,</span>
            <span class="n">act_dim</span><span class="o">=</span><span class="n">act_dim</span><span class="p">,</span>
            <span class="n">size</span><span class="o">=</span><span class="n">replay_size</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">policy</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Count policy variables and initialize param count log string.</span>
    <span class="n">var_counts</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">count_vars</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="p">[</span><span class="n">policy</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">L</span><span class="p">])</span>
    <span class="n">param_count_log_str</span> <span class="o">=</span> <span class="s2">&quot;Number of parameters: </span><span class="se">\t</span><span class="s2"> pi: </span><span class="si">%d</span><span class="s2">, </span><span class="se">\t</span><span class="s2"> L: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">var_counts</span>

    <span class="c1"># ==== LATC code ===============================</span>
    <span class="c1"># NOTE: Added here to reduce code duplication.</span>
    <span class="c1"># Extend param count log string.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">ac</span><span class="p">,</span> <span class="n">LyapunovActorTwinCritic</span><span class="p">):</span>
        <span class="n">param_count_log_str</span> <span class="o">+=</span> <span class="s2">&quot;, </span><span class="se">\t</span><span class="s2"> L2: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">count_vars</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">L2</span><span class="p">)</span>
    <span class="c1"># ===============================================</span>

    <span class="c1"># Print network structure.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">param_count_log_str</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Network structure:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">ac</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Parse learning rate decay reference.</span>
    <span class="n">lr_decay_ref</span> <span class="o">=</span> <span class="n">lr_decay_ref</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">lr_decay_ref</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">VALID_DECAY_REFERENCES</span><span class="p">:</span>
        <span class="n">options</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">option</span><span class="si">}</span><span class="s2">&#39;&quot;</span> <span class="k">for</span> <span class="n">option</span> <span class="ow">in</span> <span class="n">VALID_DECAY_REFERENCES</span><span class="p">]</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The learning rate decay reference variable was set to &#39;</span><span class="si">{</span><span class="n">lr_decay_ref</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
            <span class="s2">&quot;which is not a valid option. Valid options are &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">options</span><span class="p">)</span><span class="si">}</span><span class="s2">. The learning rate decay reference &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;variable has been set to &#39;</span><span class="si">{</span><span class="n">DEFAULT_DECAY_REFERENCE</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">lr_decay_ref</span> <span class="o">=</span> <span class="n">DEFAULT_DECAY_REFERENCE</span>

    <span class="c1"># Parse learning rate decay types.</span>
    <span class="n">lr_decay_type</span> <span class="o">=</span> <span class="n">lr_decay_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">lr_decay_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">VALID_DECAY_TYPES</span><span class="p">:</span>
        <span class="n">options</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">option</span><span class="si">}</span><span class="s2">&#39;&quot;</span> <span class="k">for</span> <span class="n">option</span> <span class="ow">in</span> <span class="n">VALID_DECAY_TYPES</span><span class="p">]</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The learning rate decay type was set to &#39;</span><span class="si">{</span><span class="n">lr_decay_type</span><span class="si">}</span><span class="s2">&#39;, which is not &quot;</span>
            <span class="s2">&quot;a valid option. Valid options are &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">options</span><span class="p">)</span><span class="si">}</span><span class="s2">. The learning rate decay type has been set to &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">DEFAULT_DECAY_TYPE</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;warning&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">lr_decay_type</span> <span class="o">=</span> <span class="n">DEFAULT_DECAY_TYPE</span>
    <span class="n">decay_types</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;actor&quot;</span><span class="p">:</span> <span class="n">lr_a_decay_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">if</span> <span class="n">lr_a_decay_type</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;critic&quot;</span><span class="p">:</span> <span class="n">lr_c_decay_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">if</span> <span class="n">lr_c_decay_type</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">lr_alpha_decay_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">if</span> <span class="n">lr_alpha_decay_type</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;labda&quot;</span><span class="p">:</span> <span class="n">lr_labda_decay_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">if</span> <span class="n">lr_labda_decay_type</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">decay_type</span> <span class="ow">in</span> <span class="n">decay_types</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">decay_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">decay_types</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_decay_type</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">decay_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">VALID_DECAY_TYPES</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Invalid </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> learning rate decay type: &#39;</span><span class="si">{</span><span class="n">decay_type</span><span class="si">}</span><span class="s2">&#39;. Using &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;global learning rate decay type: &#39;</span><span class="si">{</span><span class="n">lr_decay_type</span><span class="si">}</span><span class="s2">&#39; instead.&quot;</span><span class="p">,</span>
                    <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;warning&quot;</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">decay_types</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_decay_type</span>
    <span class="n">lr_a_decay_type</span><span class="p">,</span> <span class="n">lr_c_decay_type</span><span class="p">,</span> <span class="n">lr_alpha_decay_type</span><span class="p">,</span> <span class="n">lr_labda_decay_type</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">decay_types</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="c1"># Calculate the number of learning rate scheduler steps.</span>
    <span class="k">if</span> <span class="n">lr_decay_ref</span> <span class="o">==</span> <span class="s2">&quot;step&quot;</span><span class="p">:</span>
        <span class="c1"># NOTE: Decay applied at policy update to improve performance.</span>
        <span class="n">lr_decay_steps</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">total_steps</span> <span class="o">-</span> <span class="n">update_after</span>
        <span class="p">)</span> <span class="o">/</span> <span class="n">update_every</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># NOTE: +1 since we start at the initial learning rate.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lr_decay_steps</span> <span class="o">=</span> <span class="n">epochs</span>

    <span class="c1"># Setup learning rate schedulers.</span>
    <span class="n">lr_a_init</span><span class="p">,</span> <span class="n">lr_c_init</span><span class="p">,</span> <span class="n">lr_alpha_init</span><span class="p">,</span> <span class="n">lr_labda_init</span> <span class="o">=</span> <span class="n">lr_a</span><span class="p">,</span> <span class="n">lr_c</span><span class="p">,</span> <span class="n">lr_alpha</span><span class="p">,</span> <span class="n">lr_labda</span>
    <span class="n">opt_schedulers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;pi&quot;</span><span class="p">:</span> <span class="n">get_lr_scheduler</span><span class="p">(</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">_pi_optimizer</span><span class="p">,</span>
            <span class="n">lr_a_decay_type</span><span class="p">,</span>
            <span class="n">lr_a_init</span><span class="p">,</span>
            <span class="n">lr_a_final</span><span class="p">,</span>
            <span class="n">lr_decay_steps</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="s2">&quot;c&quot;</span><span class="p">:</span> <span class="n">get_lr_scheduler</span><span class="p">(</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">_c_optimizer</span><span class="p">,</span>
            <span class="n">lr_c_decay_type</span><span class="p">,</span>
            <span class="n">lr_c_init</span><span class="p">,</span>
            <span class="n">lr_c_final</span><span class="p">,</span>
            <span class="n">lr_decay_steps</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">get_lr_scheduler</span><span class="p">(</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">_log_alpha_optimizer</span><span class="p">,</span>
            <span class="n">lr_alpha_decay_type</span><span class="p">,</span>
            <span class="n">lr_alpha_init</span><span class="p">,</span>
            <span class="n">lr_alpha_final</span><span class="p">,</span>
            <span class="n">lr_decay_steps</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="s2">&quot;labda&quot;</span><span class="p">:</span> <span class="n">get_lr_scheduler</span><span class="p">(</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">_log_labda_optimizer</span><span class="p">,</span>
            <span class="n">lr_labda_decay_type</span><span class="p">,</span>
            <span class="n">lr_labda_init</span><span class="p">,</span>
            <span class="n">lr_labda_final</span><span class="p">,</span>
            <span class="n">lr_decay_steps</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">}</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">setup_pytorch_saver</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>

    <span class="c1"># Log model to TensorBoard.</span>
    <span class="k">if</span> <span class="n">use_tensorboard</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
            <span class="c1"># NOTE: Suppress TracerWarning because our policy is non-deterministic.</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">TracerWarning</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_model_to_tb</span><span class="p">(</span>
                <span class="n">policy</span><span class="o">.</span><span class="n">ac</span><span class="p">,</span>
                <span class="n">input_to_model</span><span class="o">=</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span>
                        <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
                    <span class="p">),</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                <span class="p">),</span>
            <span class="p">)</span>

    <span class="c1"># Log model to Weight &amp; Biases.</span>
    <span class="k">if</span> <span class="n">use_wandb</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">watch_model_in_wandb</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">ac</span><span class="p">)</span>

    <span class="c1"># Setup diagnostics tb_write dict and store initial learning rates.</span>
    <span class="n">diag_tb_log_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;ErrorL&quot;</span><span class="p">,</span>
        <span class="s2">&quot;LossPi&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Alpha&quot;</span><span class="p">,</span>
        <span class="s2">&quot;LossAlpha&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Lambda&quot;</span><span class="p">,</span>
        <span class="s2">&quot;LossLambda&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Entropy&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">use_tensorboard</span><span class="p">:</span>
        <span class="c1"># NOTE: TensorBoard counts from 0.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">log_to_tb</span><span class="p">(</span>
            <span class="s2">&quot;Lr_a&quot;</span><span class="p">,</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">_pi_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
            <span class="n">tb_prefix</span><span class="o">=</span><span class="s2">&quot;LearningRates&quot;</span><span class="p">,</span>
            <span class="n">global_step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">log_to_tb</span><span class="p">(</span>
            <span class="s2">&quot;Lr_c&quot;</span><span class="p">,</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">_c_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
            <span class="n">tb_prefix</span><span class="o">=</span><span class="s2">&quot;LearningRates&quot;</span><span class="p">,</span>
            <span class="n">global_step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">log_to_tb</span><span class="p">(</span>
            <span class="s2">&quot;Lr_alpha&quot;</span><span class="p">,</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">_log_alpha_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
            <span class="n">tb_prefix</span><span class="o">=</span><span class="s2">&quot;LearningRates&quot;</span><span class="p">,</span>
            <span class="n">global_step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">log_to_tb</span><span class="p">(</span>
            <span class="s2">&quot;Lr_labda&quot;</span><span class="p">,</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">_log_labda_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
            <span class="n">tb_prefix</span><span class="o">=</span><span class="s2">&quot;LearningRates&quot;</span><span class="p">,</span>
            <span class="n">global_step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Main loop: collect experience in env and update/log each epoch</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">o</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">ep_ret</span><span class="p">,</span> <span class="n">ep_len</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_steps</span><span class="p">):</span>
        <span class="c1"># Until start_steps have elapsed, randomly sample actions</span>
        <span class="c1"># from a uniform distribution for better exploration. Afterwards,</span>
        <span class="c1"># use the learned policy.</span>
        <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="n">start_steps</span> <span class="ow">or</span> <span class="n">start_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Take step in the env.</span>
        <span class="n">o_</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">ep_ret</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="n">ep_len</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Store experience in replay buffer.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">,</span> <span class="n">FiniteHorizonReplayBuffer</span><span class="p">):</span>
            <span class="n">replay_buffer</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">o_</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">truncated</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">replay_buffer</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">o_</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

        <span class="c1"># Make sure to update most recent observation!</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">o_</span>

        <span class="c1"># End of trajectory handling.</span>
        <span class="k">if</span> <span class="n">d</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">EpRet</span><span class="o">=</span><span class="n">ep_ret</span><span class="p">,</span> <span class="n">EpLen</span><span class="o">=</span><span class="n">ep_len</span><span class="p">)</span>
            <span class="n">o</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">ep_ret</span><span class="p">,</span> <span class="n">ep_len</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

        <span class="c1"># Update handling.</span>
        <span class="c1"># NOTE: Improved compared to Han et al. 2020. Previously, updates were based on</span>
        <span class="c1"># memory size, which only changed at terminal states.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">update_after</span> <span class="ow">and</span> <span class="p">((</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">update_after</span><span class="p">)</span> <span class="o">%</span> <span class="n">update_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps_per_update</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
                <span class="n">update_diagnostics</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="o">**</span><span class="n">update_diagnostics</span><span class="p">)</span>  <span class="c1"># Log diagnostics.</span>

            <span class="c1"># Step based learning rate decay.</span>
            <span class="k">if</span> <span class="n">lr_decay_ref</span> <span class="o">==</span> <span class="s2">&quot;step&quot;</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">scheduler</span> <span class="ow">in</span> <span class="n">opt_schedulers</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">policy</span><span class="o">.</span><span class="n">bound_lr</span><span class="p">(</span>
                    <span class="n">lr_a_final</span><span class="p">,</span> <span class="n">lr_c_final</span><span class="p">,</span> <span class="n">lr_alpha_final</span><span class="p">,</span> <span class="n">lr_labda_final</span>
                <span class="p">)</span>  <span class="c1"># Make sure lr is bounded above the final lr.</span>

            <span class="c1"># SGD batch tb logging.</span>
            <span class="k">if</span> <span class="n">use_tensorboard</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">tb_low_log_freq</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">log_to_tb</span><span class="p">(</span>
                    <span class="n">keys</span><span class="o">=</span><span class="n">diag_tb_log_list</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">t</span>
                <span class="p">)</span>  <span class="c1"># NOTE: TensorBoard counts from 0.</span>

        <span class="c1"># End of epoch handling (Save model, test performance and log data)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">steps_per_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">epoch</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">steps_per_epoch</span>

            <span class="c1"># Save model.</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">%</span> <span class="n">save_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">==</span> <span class="n">epochs</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">save_state</span><span class="p">({</span><span class="s2">&quot;env&quot;</span><span class="p">:</span> <span class="n">env</span><span class="p">},</span> <span class="n">itr</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

            <span class="c1"># Test the performance of the deterministic version of the agent.</span>
            <span class="k">if</span> <span class="n">num_test_episodes</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">eps_ret</span><span class="p">,</span> <span class="n">eps_len</span> <span class="o">=</span> <span class="n">test_agent</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">test_env</span><span class="p">,</span> <span class="n">num_test_episodes</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">store</span><span class="p">(</span>
                    <span class="n">TestEpRet</span><span class="o">=</span><span class="n">eps_ret</span><span class="p">,</span>
                    <span class="n">TestEpLen</span><span class="o">=</span><span class="n">eps_len</span><span class="p">,</span>
                    <span class="n">extend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># Retrieve current learning rates.</span>
            <span class="k">if</span> <span class="n">lr_decay_ref</span> <span class="o">==</span> <span class="s2">&quot;step&quot;</span><span class="p">:</span>
                <span class="c1"># NOTE: Estimate since &#39;step&#39; decay is applied at policy update.</span>
                <span class="n">lr_actor</span> <span class="o">=</span> <span class="n">estimate_step_learning_rate</span><span class="p">(</span>
                    <span class="n">opt_schedulers</span><span class="p">[</span><span class="s2">&quot;pi&quot;</span><span class="p">],</span>
                    <span class="n">lr_a_init</span><span class="p">,</span>
                    <span class="n">lr_a_final</span><span class="p">,</span>
                    <span class="n">update_after</span><span class="p">,</span>
                    <span class="n">total_steps</span><span class="p">,</span>
                    <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">lr_critic</span> <span class="o">=</span> <span class="n">estimate_step_learning_rate</span><span class="p">(</span>
                    <span class="n">opt_schedulers</span><span class="p">[</span><span class="s2">&quot;c&quot;</span><span class="p">],</span>
                    <span class="n">lr_c_init</span><span class="p">,</span>
                    <span class="n">lr_c_final</span><span class="p">,</span>
                    <span class="n">update_after</span><span class="p">,</span>
                    <span class="n">total_steps</span><span class="p">,</span>
                    <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">lr_alpha</span> <span class="o">=</span> <span class="n">estimate_step_learning_rate</span><span class="p">(</span>
                    <span class="n">opt_schedulers</span><span class="p">[</span><span class="s2">&quot;alpha&quot;</span><span class="p">],</span>
                    <span class="n">lr_alpha_init</span><span class="p">,</span>
                    <span class="n">lr_alpha_final</span><span class="p">,</span>
                    <span class="n">update_after</span><span class="p">,</span>
                    <span class="n">total_steps</span><span class="p">,</span>
                    <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">lr_labda</span> <span class="o">=</span> <span class="n">estimate_step_learning_rate</span><span class="p">(</span>
                    <span class="n">opt_schedulers</span><span class="p">[</span><span class="s2">&quot;labda&quot;</span><span class="p">],</span>
                    <span class="n">lr_labda_init</span><span class="p">,</span>
                    <span class="n">lr_labda_final</span><span class="p">,</span>
                    <span class="n">update_after</span><span class="p">,</span>
                    <span class="n">total_steps</span><span class="p">,</span>
                    <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lr_actor</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">_pi_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
                <span class="n">lr_critic</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">_c_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
                <span class="n">lr_alpha</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">_log_alpha_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
                <span class="n">lr_labda</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">_log_labda_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>

            <span class="c1"># Log info about epoch.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s2">&quot;TotalEnvInteracts&quot;</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;EpRet&quot;</span><span class="p">,</span>
                <span class="n">with_min_and_max</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="n">use_tensorboard</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">num_test_episodes</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                    <span class="s2">&quot;TestEpRet&quot;</span><span class="p">,</span>
                    <span class="n">with_min_and_max</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">tb_write</span><span class="o">=</span><span class="n">use_tensorboard</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s2">&quot;EpLen&quot;</span><span class="p">,</span> <span class="n">average_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tb_write</span><span class="o">=</span><span class="n">use_tensorboard</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                    <span class="s2">&quot;TestEpLen&quot;</span><span class="p">,</span>
                    <span class="n">average_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">tb_write</span><span class="o">=</span><span class="n">use_tensorboard</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;Lr_a&quot;</span><span class="p">,</span>
                <span class="n">lr_actor</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="n">use_tensorboard</span><span class="p">,</span>
                <span class="n">tb_prefix</span><span class="o">=</span><span class="s2">&quot;LearningRates&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;Lr_c&quot;</span><span class="p">,</span>
                <span class="n">lr_critic</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="n">use_tensorboard</span><span class="p">,</span>
                <span class="n">tb_prefix</span><span class="o">=</span><span class="s2">&quot;LearningRates&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;Lr_alpha&quot;</span><span class="p">,</span>
                <span class="n">lr_alpha</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="n">use_tensorboard</span><span class="p">,</span>
                <span class="n">tb_prefix</span><span class="o">=</span><span class="s2">&quot;LearningRates&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;Lr_labda&quot;</span><span class="p">,</span>
                <span class="n">lr_labda</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="n">use_tensorboard</span><span class="p">,</span>
                <span class="n">tb_prefix</span><span class="o">=</span><span class="s2">&quot;LearningRates&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;Alpha&quot;</span><span class="p">,</span>
                <span class="n">average_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="p">(</span><span class="n">use_tensorboard</span> <span class="ow">and</span> <span class="n">tb_low_log_freq</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;Lambda&quot;</span><span class="p">,</span>
                <span class="n">average_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="p">(</span><span class="n">use_tensorboard</span> <span class="ow">and</span> <span class="n">tb_low_log_freq</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;LossPi&quot;</span><span class="p">,</span>
                <span class="n">average_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="p">(</span><span class="n">use_tensorboard</span> <span class="ow">and</span> <span class="n">tb_low_log_freq</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;ErrorL&quot;</span><span class="p">,</span>
                <span class="n">average_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="p">(</span><span class="n">use_tensorboard</span> <span class="ow">and</span> <span class="n">tb_low_log_freq</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">adaptive_temperature</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                    <span class="s2">&quot;LossAlpha&quot;</span><span class="p">,</span>
                    <span class="n">average_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">tb_write</span><span class="o">=</span><span class="p">(</span><span class="n">use_tensorboard</span> <span class="ow">and</span> <span class="n">tb_low_log_freq</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;LossLambda&quot;</span><span class="p">,</span>
                <span class="n">average_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="p">(</span><span class="n">use_tensorboard</span> <span class="ow">and</span> <span class="n">tb_low_log_freq</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s2">&quot;LVals&quot;</span><span class="p">,</span> <span class="n">with_min_and_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s2">&quot;LogPi&quot;</span><span class="p">,</span> <span class="n">with_min_and_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span>
                <span class="s2">&quot;Entropy&quot;</span><span class="p">,</span>
                <span class="n">average_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">tb_write</span><span class="o">=</span><span class="p">(</span><span class="n">use_tensorboard</span> <span class="ow">and</span> <span class="n">tb_low_log_freq</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">log_tabular</span><span class="p">(</span><span class="s2">&quot;Time&quot;</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">dump_tabular</span><span class="p">(</span><span class="n">global_step</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>  <span class="c1"># NOTE: TensorBoard counts from 0.</span>

            <span class="c1"># Epoch based learning rate decay.</span>
            <span class="k">if</span> <span class="n">lr_decay_ref</span> <span class="o">!=</span> <span class="s2">&quot;step&quot;</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">scheduler</span> <span class="ow">in</span> <span class="n">opt_schedulers</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">policy</span><span class="o">.</span><span class="n">bound_lr</span><span class="p">(</span>
                    <span class="n">lr_a_final</span><span class="p">,</span> <span class="n">lr_c_final</span><span class="p">,</span> <span class="n">lr_alpha_final</span><span class="p">,</span> <span class="n">lr_labda_final</span>
                <span class="p">)</span>  <span class="c1"># Make sure lr is bounded above the final lr.</span>

    <span class="c1"># Export model to &#39;TorchScript&#39;</span>
    <span class="k">if</span> <span class="n">export</span><span class="p">:</span>
        <span class="n">policy</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">logger</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">logger_kwargs</span><span class="p">[</span><span class="s2">&quot;quiet&quot;</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
        <span class="s2">&quot;Training finished after </span><span class="si">{}</span><span class="s2">s&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">),</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Close environment and return policy and replay buffer.</span>
    <span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">policy</span><span class="p">,</span> <span class="n">replay_buffer</span></div>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<div class="viewcode-block" id="parser"><a class="viewcode-back" href="../../../../../autoapi/stable_learning_control/algos/pytorch/lac/lac/index.html#stable_learning_control.algos.pytorch.parser">[docs]</a>    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Trains a LAC agent in a given environment.&quot;</span>
    <span class="p">)</span></div>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--env&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;stable_gym:Oscillator-v1&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the gymnasium env (default: stable_gym:Oscillator-v1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>  <span class="c1"># NOTE: Environment found in https://rickstaa.dev/stable-gym.</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--hid_a&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;hidden layer size of the actor (default: 256)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--hid_c&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;hidden layer size of the lyapunov critic (default: 256)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--l_a&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of hidden layer in the actor (default: 2)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--l_c&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of hidden layer in the critic (default: 2)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--act_a&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;nn.ReLU&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the hidden layer activation function of the actor (default: nn.ReLU)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--act_c&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;nn.ReLU&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the hidden layer activation function of the critic (default: nn.ReLU)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--act_out_a&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;nn.ReLU&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the output activation function of the actor (default: nn.ReLU)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--opt_type&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;minimize&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;algorithm optimization type (default: minimize)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--max_ep_len&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;maximum episode length (default: None)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--epochs&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the number of epochs (default: 50)&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--steps_per_epoch&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the number of steps per epoch (default: 2048)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--start_steps&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the number of random exploration steps (default: 0)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--update_every&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the number of env interactions that should elapse between SGD updates &quot;</span>
            <span class="s2">&quot;(default: 100)&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--update_after&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the number of steps before starting the SGD (default: 1000)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--steps_per_update&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the number of gradient descent steps that are&quot;</span>
            <span class="s2">&quot;performed for each SGD update (default: 100)&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--num_test_episodes&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the number of episodes for the performance analysis (default: 10). When &quot;</span>
            <span class="s2">&quot;set to zero no test episodes will be performed&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--alpha&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the entropy regularization coefficient (default: 0.99)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--alpha3&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the Lyapunov constraint error boundary (default: 0.2)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--labda&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the Lyapunov Lagrance multiplier (default: 0.99)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--gamma&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;discount factor (default: 0.99)&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--polyak&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="c1"># default=0.995,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the interpolation factor in polyak averaging (default: 0.995)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--target_entropy&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the initial target entropy (default: -action_space)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--adaptive_temperature&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the boolean for enabling automating Entropy Adjustment (default: True)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_a&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;actor learning rate (default: 1e-4)&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_c&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;critic learning rate (default: 1e-4)&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_alpha&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;entropy temperature learning rate (default: 1e-4)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_labda&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;lyapunov Lagrance multiplier learning rate (default: 3e-4)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_a_final&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the finalactor learning rate (default: 1e-10)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_c_final&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the finalcritic learning rate (default: 1e-10)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_alpha_final&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the final entropy temperature learning rate (default: 1e-10)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_labda_final&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the final lyapunov Lagrance multiplier learning rate (default: 1e-10)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_decay_type&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the learning rate decay type (default: linear)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_a_decay_type&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the learning rate decay type that is used for the actor learning rate. &quot;</span>
            <span class="s2">&quot;If not specified, the general learning rate decay type is used.&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_c_decay_type&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the learning rate decay type that is used for the critic learning rate. &quot;</span>
            <span class="s2">&quot;If not specified, the general learning rate decay type is used.&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_alpha_decay_type&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the learning rate decay type that is used for the entropy temperature &quot;</span>
            <span class="s2">&quot;learning rate. If not specified, the general learning rate decay type is &quot;</span>
            <span class="s2">&quot;used.&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_labda_decay_type&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the learning rate decay type that is used for the lyapunov Lagrance &quot;</span>
            <span class="s2">&quot;multiplier learning rate. If not specified, the general learning rate &quot;</span>
            <span class="s2">&quot;decay type is used.&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr_decay_ref&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the reference variable that is used for decaying the learning rate &quot;</span>
            <span class="s2">&quot;&#39;epoch&#39; or &#39;step&#39; (default: epoch)&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--batch-size&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;mini batch size of the SGD (default: 256)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--replay-size&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;replay buffer size (default: 1e6)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--horizon_length&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;length of the finite-horizon used for the Lyapunov Critic target ( &quot;</span>
            <span class="s2">&quot;Default: 0, meaning the infinite-horizon bellman backup is used).&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--seed&quot;</span><span class="p">,</span> <span class="s2">&quot;-s&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the random seed (default: 0)&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--device&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;The device the networks are placed on. Options: &#39;cpu&#39;, &#39;gpu&#39;, &#39;gpu:0&#39;, &quot;</span>
            <span class="s2">&quot;&#39;gpu:1&#39;, etc. Defaults to &#39;cpu&#39;.&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--start_policy&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;The policy which you want to use as the starting point for the training&quot;</span>
            <span class="s2">&quot; (default: None)&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--export&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;Whether you want to export the model as a &#39;TorchScript&#39; such that &quot;</span>
            <span class="s2">&quot;it can be deployed on hardware (default: False)&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>

    <span class="c1"># Parse logger related arguments.</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--exp_name&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;lac&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the name of the experiment (default: lac)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--quiet&quot;</span><span class="p">,</span>
        <span class="s2">&quot;-q&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;suppress logging of diagnostics to stdout (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--verbose_fmt&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;line&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;log diagnostics stdout format (options: &#39;table&#39; or &#39;line&#39;, default: &quot;</span>
            <span class="s2">&quot;line)&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--verbose_vars&quot;</span><span class="p">,</span>
        <span class="n">nargs</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="n">STD_OUT_LOG_VARS_DEFAULT</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;a space separated list of the values you want to show on the stdout.&quot;</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--save_freq&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;how often (in epochs) the policy should be saved (default: 1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--save_checkpoints&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;use model checkpoints (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--use_tensorboard&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;use TensorBoard (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--tb_log_freq&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;low&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the TensorBoard log frequency. Options are &#39;low&#39; (Recommended: logs at &quot;</span>
            <span class="s2">&quot;every epoch) and &#39;high&#39; (logs at every SGD update batch). Default is &#39;low&#39;&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--use_wandb&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;use Weights &amp; Biases (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--wandb_job_type&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the Weights &amp; Biases job type (default: train)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--wandb_project&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;stable-learning-control&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the name of the wandb project (default: stable-learning-control)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--wandb_group&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the name of the Weights &amp; Biases group you want to assign the run to &quot;</span>
            <span class="s2">&quot;(defaults: None)&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--wandb_run_name&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;the name of the Weights &amp; Biases run (defaults: None, which will be &quot;</span>
            <span class="s2">&quot;set to the experiment name)&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Setup actor critic arguments.</span>
    <span class="n">output_activation</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">output_activation</span><span class="p">[</span><span class="s2">&quot;actor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">safer_eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">act_out_a</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;torch&quot;</span><span class="p">)</span>
    <span class="n">ac_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">hidden_sizes</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;actor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">hid_a</span><span class="p">]</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">l_a</span><span class="p">,</span>
            <span class="s2">&quot;critic&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">hid_c</span><span class="p">]</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">l_c</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">activation</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;actor&quot;</span><span class="p">:</span> <span class="n">safer_eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">act_a</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;torch&quot;</span><span class="p">),</span>
            <span class="s2">&quot;critic&quot;</span><span class="p">:</span> <span class="n">safer_eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">act_c</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;torch&quot;</span><span class="p">),</span>
        <span class="p">},</span>
        <span class="n">output_activation</span><span class="o">=</span><span class="n">output_activation</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Setup output dir for logger and return output kwargs.</span>
    <span class="n">logger_kwargs</span> <span class="o">=</span> <span class="n">setup_logger_kwargs</span><span class="p">(</span>
        <span class="n">args</span><span class="o">.</span><span class="n">exp_name</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">save_checkpoints</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">save_checkpoints</span><span class="p">,</span>
        <span class="n">use_tensorboard</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">use_tensorboard</span><span class="p">,</span>
        <span class="n">tb_log_freq</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">tb_log_freq</span><span class="p">,</span>
        <span class="n">use_wandb</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">use_wandb</span><span class="p">,</span>
        <span class="n">wandb_job_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">wandb_job_type</span><span class="p">,</span>
        <span class="n">wandb_project</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">wandb_project</span><span class="p">,</span>
        <span class="n">wandb_group</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">wandb_group</span><span class="p">,</span>
        <span class="n">wandb_run_name</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">wandb_run_name</span><span class="p">,</span>
        <span class="n">quiet</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">quiet</span><span class="p">,</span>
        <span class="n">verbose_fmt</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">verbose_fmt</span><span class="p">,</span>
        <span class="n">verbose_vars</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">verbose_vars</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">logger_kwargs</span><span class="p">[</span><span class="s2">&quot;output_dir&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">osp</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span>
        <span class="n">osp</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">osp</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">osp</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)),</span>
            <span class="sa">f</span><span class="s2">&quot;../../../../../data/lac/</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">/runs/run_</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">get_num_threads</span><span class="p">())</span>

    <span class="n">lac</span><span class="p">(</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="p">),</span>
        <span class="n">actor_critic</span><span class="o">=</span><span class="n">LyapunovActorCritic</span><span class="p">,</span>
        <span class="n">ac_kwargs</span><span class="o">=</span><span class="n">ac_kwargs</span><span class="p">,</span>
        <span class="n">opt_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">opt_type</span><span class="p">,</span>
        <span class="n">max_ep_len</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_ep_len</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
        <span class="n">steps_per_epoch</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">steps_per_epoch</span><span class="p">,</span>
        <span class="n">start_steps</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">start_steps</span><span class="p">,</span>
        <span class="n">update_every</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">update_every</span><span class="p">,</span>
        <span class="n">update_after</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">update_after</span><span class="p">,</span>
        <span class="n">steps_per_update</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">steps_per_update</span><span class="p">,</span>
        <span class="n">num_test_episodes</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_test_episodes</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>
        <span class="n">alpha3</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">alpha3</span><span class="p">,</span>
        <span class="n">labda</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">labda</span><span class="p">,</span>
        <span class="n">gamma</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span>
        <span class="n">polyak</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">polyak</span><span class="p">,</span>
        <span class="n">target_entropy</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">target_entropy</span><span class="p">,</span>
        <span class="n">adaptive_temperature</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">adaptive_temperature</span><span class="p">,</span>
        <span class="n">lr_a</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_a</span><span class="p">,</span>
        <span class="n">lr_c</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_c</span><span class="p">,</span>
        <span class="n">lr_alpha</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_alpha</span><span class="p">,</span>
        <span class="n">lr_labda</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_labda</span><span class="p">,</span>
        <span class="n">lr_a_final</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_a_final</span><span class="p">,</span>
        <span class="n">lr_c_final</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_c_final</span><span class="p">,</span>
        <span class="n">lr_alpha_final</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_a_final</span><span class="p">,</span>
        <span class="n">lr_labda_final</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_a_final</span><span class="p">,</span>
        <span class="n">lr_decay_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_decay_type</span><span class="p">,</span>
        <span class="n">lr_a_decay_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_a_decay_type</span><span class="p">,</span>
        <span class="n">lr_c_decay_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_c_decay_type</span><span class="p">,</span>
        <span class="n">lr_alpha_decay_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_alpha_decay_type</span><span class="p">,</span>
        <span class="n">lr_labda_decay_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_labda_decay_type</span><span class="p">,</span>
        <span class="n">lr_decay_ref</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr_decay_ref</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">replay_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">replay_size</span><span class="p">,</span>
        <span class="n">horizon_length</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">horizon_length</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">save_freq</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">save_freq</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="n">start_policy</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">start_policy</span><span class="p">,</span>
        <span class="n">export</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">export</span><span class="p">,</span>
        <span class="n">logger_kwargs</span><span class="o">=</span><span class="n">logger_kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Rick Staa.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>