

<!DOCTYPE html>
<html class="writer-html4" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Extra Material: Tensorflow Policy Gradient Implementation Examples &mdash; Machine Learning Control 0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/logo.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Control</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../control/control.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control/running.html">Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control/saving_and_loading.html">Experiment Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control/plotting.html">Plotting Results</a></li>
</ul>
<p class="caption"><span class="caption-text">Hardware</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../hardware/hardware.html">Hardware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hardware/supported.html">Supported hardware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hardware/deploy.html">Deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Utilities Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">Run Utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Zone</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dev/release_dev.html">Release package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev/doc_dev.html">Release documentation</a></li>
</ul>
<p class="caption"><span class="caption-text">Etc.</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">About the Author</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Machine Learning Control</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Extra Material: Tensorflow Policy Gradient Implementation Examples</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/mlc/extra_tf_pg_implementation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="extra-material-tensorflow-policy-gradient-implementation-examples">
<h1>Extra Material: Tensorflow Policy Gradient Implementation Examples<a class="headerlink" href="#extra-material-tensorflow-policy-gradient-implementation-examples" title="Permalink to this headline">¶</a></h1>
<div class="section" id="implementing-the-simplest-policy-gradient">
<h2>Implementing the Simplest Policy Gradient<a class="headerlink" href="#implementing-the-simplest-policy-gradient" title="Permalink to this headline">¶</a></h2>
<p>We give a short Tensorflow implementation of this simple version of the policy gradient algorithm in <code class="docutils literal notranslate"><span class="pre">spinup/examples/tf1/pg_math/1_simple_pg.py</span></code>. (It can also be viewed <a class="reference external" href="https://github.com/openai/spinningup/blob/master/spinup/examples/tf1/pg_math/1_simple_pg.py">on github</a>.) It is only 122 lines long, so we highly recommend reading through it in depth. While we won’t go through the entirety of the code here, we’ll highlight and explain a few important pieces.</p>
<p><strong>1. Making the Policy Network.</strong></p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>25
26
27
28
29
30</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1"># make core of policy network</span>
<span class="n">obs_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">obs_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">obs_ph</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="n">hidden_sizes</span><span class="o">+</span><span class="p">[</span><span class="n">n_acts</span><span class="p">])</span>

<span class="c1"># make action selection op (outputs int actions, sampled from policy)</span>
<span class="n">actions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>This block builds a feedforward neural network categorical policy. (See the <a class="reference external" href="../spinningup/rl_intro.html#stochastic-policies">Stochastic Policies</a> section in Part 1 for a refresher.) The <code class="docutils literal notranslate"><span class="pre">logits</span></code> tensor can be used to construct log-probabilities and probabilities for actions, and the <code class="docutils literal notranslate"><span class="pre">actions</span></code> tensor samples actions based on the probabilities implied by <code class="docutils literal notranslate"><span class="pre">logits</span></code>.</p>
<p><strong>2. Making the Loss Function.</strong></p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>32
33
34
35
36
37</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1"># make loss function whose gradient, for the right data, is policy gradient</span>
<span class="n">weights_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">act_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">action_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">act_ph</span><span class="p">,</span> <span class="n">n_acts</span><span class="p">)</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">action_masks</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">weights_ph</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>In this block, we build a “loss” function for the policy gradient algorithm. When the right data is plugged in, the gradient of this loss is equal to the policy gradient. The right data means a set of (state, action, weight) tuples collected while acting according to the current policy, where the weight for a state-action pair is the return from the episode to which it belongs. (Although as we will show in later subsections, there are other values you can plug in for the weight which also work correctly.)</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p>Even though we describe this as a loss function, it is <strong>not</strong> a loss function in the typical sense from supervised learning. There are two main differences from standard loss functions.</p>
<p><strong>1. The data distribution depends on the parameters.</strong> A loss function is usually defined on a fixed data distribution which is independent of the parameters we aim to optimise. Not so here, where the data must be sampled on the most recent policy.</p>
<p><strong>2. It doesn’t measure performance.</strong> A loss function usually evaluates the performance metric that we care about. Here, we care about expected return, <img class="math" src="../_images/math/96b876944de9cf0f980fe261562e8e07029245bf.svg" alt="J(\pi_{\theta})"/>, but our “loss” function does not approximate this at all, even in expectation. This “loss” function is only useful to us because, when evaluated at the current parameters, with data generated by the current parameters, it has the negative gradient of performance.</p>
<p>But after that first step of gradient descent, there is no more connection to performance. This means that minimizing this “loss” function, for a given batch of data, has <em>no</em> guarantee whatsoever of improving expected return. You can send this loss to <img class="math" src="../_images/math/5d7bd7abcf6c1c07becaa8b5fe1a2a000e559a50.svg" alt="-\infty"/> and policy performance could crater; in fact, it usually will. Sometimes a deep RL researcher might describe this outcome as the policy “overfitting” to a batch of data. This is descriptive, but should not be taken literally because it does not refer to generalization error.</p>
<p class="last">We raise this point because it is common for ML practitioners to interpret a loss function as a useful signal during training—“if the loss goes down, all is well.” In policy gradients, this intuition is wrong, and you should only care about average return. The loss function means nothing.</p>
</div>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">The approach used here to make the <code class="docutils literal notranslate"><span class="pre">log_probs</span></code> tensor—creating an action mask, and using it to select out particular log probabilities—<em>only</em> works for categorical policies. It does not work in general.</p>
</div>
<p><strong>3. Running One Epoch of Training.</strong></p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106</pre></div></td><td class="code"><div class="highlight"><pre><span></span>    <span class="c1"># for training policy</span>
    <span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">():</span>
        <span class="c1"># make some empty lists for logging.</span>
        <span class="n">batch_obs</span> <span class="o">=</span> <span class="p">[]</span>          <span class="c1"># for observations</span>
        <span class="n">batch_acts</span> <span class="o">=</span> <span class="p">[]</span>         <span class="c1"># for actions</span>
        <span class="n">batch_weights</span> <span class="o">=</span> <span class="p">[]</span>      <span class="c1"># for R(tau) weighting in policy gradient</span>
        <span class="n">batch_rets</span> <span class="o">=</span> <span class="p">[]</span>         <span class="c1"># for measuring episode returns</span>
        <span class="n">batch_lens</span> <span class="o">=</span> <span class="p">[]</span>         <span class="c1"># for measuring episode lengths</span>

        <span class="c1"># reset episode-specific variables</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>       <span class="c1"># first obs comes from starting distribution</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>            <span class="c1"># signal from environment that episode is over</span>
        <span class="n">ep_rews</span> <span class="o">=</span> <span class="p">[]</span>            <span class="c1"># list for rewards accrued throughout ep</span>

        <span class="c1"># render first episode of each epoch</span>
        <span class="n">finished_rendering_this_epoch</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># collect experience by acting in the environment with current policy</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>

            <span class="c1"># rendering</span>
            <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">finished_rendering_this_epoch</span><span class="p">):</span>
                <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

            <span class="c1"># save obs</span>
            <span class="n">batch_obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

            <span class="c1"># act in the environment</span>
            <span class="n">act</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="p">{</span><span class="n">obs_ph</span><span class="p">:</span> <span class="n">obs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)})[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>

            <span class="c1"># save action, reward</span>
            <span class="n">batch_acts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
            <span class="n">ep_rews</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rew</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="c1"># if episode is over, record info about episode</span>
                <span class="n">ep_ret</span><span class="p">,</span> <span class="n">ep_len</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ep_rews</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">ep_rews</span><span class="p">)</span>
                <span class="n">batch_rets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ep_ret</span><span class="p">)</span>
                <span class="n">batch_lens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ep_len</span><span class="p">)</span>

                <span class="c1"># the weight for each logprob(a|s) is R(tau)</span>
                <span class="n">batch_weights</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ep_ret</span><span class="p">]</span> <span class="o">*</span> <span class="n">ep_len</span>

                <span class="c1"># reset episode-specific variables</span>
                <span class="n">obs</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">ep_rews</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(),</span> <span class="kc">False</span><span class="p">,</span> <span class="p">[]</span>

                <span class="c1"># won&#39;t render again this epoch</span>
                <span class="n">finished_rendering_this_epoch</span> <span class="o">=</span> <span class="kc">True</span>

                <span class="c1"># end experience loop if we have enough of it</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_obs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">batch_size</span><span class="p">:</span>
                    <span class="k">break</span>

        <span class="c1"># take a single policy gradient update step</span>
        <span class="n">batch_loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss</span><span class="p">,</span> <span class="n">train_op</span><span class="p">],</span>
                                 <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
                                    <span class="n">obs_ph</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_obs</span><span class="p">),</span>
                                    <span class="n">act_ph</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_acts</span><span class="p">),</span>
                                    <span class="n">weights_ph</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_weights</span><span class="p">)</span>
                                 <span class="p">})</span>
        <span class="k">return</span> <span class="n">batch_loss</span><span class="p">,</span> <span class="n">batch_rets</span><span class="p">,</span> <span class="n">batch_lens</span>
</pre></div>
</td></tr></table></div>
<p>The <code class="docutils literal notranslate"><span class="pre">train_one_epoch()</span></code> function runs one “epoch” of policy gradient, which we define to be</p>
<ol class="arabic simple">
<li>the experience collection step (L62-97), where the agent acts for some number of episodes in the environment using the most recent policy, followed by</li>
<li>a single policy gradient update step (L99-105).</li>
</ol>
<p>The main loop of the algorithm just repeatedly calls <code class="docutils literal notranslate"><span class="pre">train_one_epoch()</span></code>.</p>
</div>
<div class="section" id="implementing-reward-to-go-policy-gradient">
<h2>Implementing Reward-to-Go Policy Gradient<a class="headerlink" href="#implementing-reward-to-go-policy-gradient" title="Permalink to this headline">¶</a></h2>
<p>We give a short Tensorflow implementation of the reward-to-go policy gradient in <code class="docutils literal notranslate"><span class="pre">spinup/examples/tf1/pg_math/2_rtg_pg.py</span></code>. (It can also be viewed <a class="reference external" href="https://github.com/openai/spinningup/blob/master/spinup/examples/tf1/pg_math/2_rtg_pg.py">on github</a>.)</p>
<p>The only thing that has changed from <code class="docutils literal notranslate"><span class="pre">1_simple_pg.py</span></code> is that we now use different weights in the loss function. The code modification is very slight: we add a new function, and change two other lines. The new function is:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>12
13
14
15
16
17</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reward_to_go</span><span class="p">(</span><span class="n">rews</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rews</span><span class="p">)</span>
    <span class="n">rtgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">rews</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)):</span>
        <span class="n">rtgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rews</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">rtgs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rtgs</span>
</pre></div>
</td></tr></table></div>
<p>And then we tweak the old L86-87 from:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>86
87</pre></div></td><td class="code"><div class="highlight"><pre><span></span>                <span class="c1"># the weight for each logprob(a|s) is R(tau)</span>
                <span class="n">batch_weights</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ep_ret</span><span class="p">]</span> <span class="o">*</span> <span class="n">ep_len</span>
</pre></div>
</td></tr></table></div>
<p>to:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>93
94</pre></div></td><td class="code"><div class="highlight"><pre><span></span>                <span class="c1"># the weight for each logprob(a_t|s_t) is reward-to-go from t</span>
                <span class="n">batch_weights</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">reward_to_go</span><span class="p">(</span><span class="n">ep_rews</span><span class="p">))</span>
</pre></div>
</td></tr></table></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Rick Staa

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>