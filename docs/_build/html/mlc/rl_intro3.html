

<!DOCTYPE html>
<html class="writer-html4" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Part 3: Intro to Policy Optimization &mdash; Machine Learning Control 0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/logo.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Control</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../control/control.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control/running.html">Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control/saving_and_loading.html">Experiment Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control/plotting.html">Plotting Results</a></li>
</ul>
<p class="caption"><span class="caption-text">Hardware</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../hardware/hardware.html">Hardware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hardware/supported.html">Supported hardware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hardware/deploy.html">Deploy</a></li>
</ul>
<p class="caption"><span class="caption-text">Utilities Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">Run Utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Zone</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dev/release_dev.html">Release package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev/doc_dev.html">Release documentation</a></li>
</ul>
<p class="caption"><span class="caption-text">Etc.</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">About the Author</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Machine Learning Control</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Part 3: Intro to Policy Optimization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/mlc/rl_intro3.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="part-3-intro-to-policy-optimization">
<h1><a class="toc-backref" href="#id7">Part 3: Intro to Policy Optimization</a><a class="headerlink" href="#part-3-intro-to-policy-optimization" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#part-3-intro-to-policy-optimization" id="id7">Part 3: Intro to Policy Optimization</a><ul>
<li><a class="reference internal" href="#deriving-the-simplest-policy-gradient" id="id8">Deriving the Simplest Policy Gradient</a></li>
<li><a class="reference internal" href="#implementing-the-simplest-policy-gradient" id="id9">Implementing the Simplest Policy Gradient</a></li>
<li><a class="reference internal" href="#expected-grad-log-prob-lemma" id="id10">Expected Grad-Log-Prob Lemma</a></li>
<li><a class="reference internal" href="#don-t-let-the-past-distract-you" id="id11">Don’t Let the Past Distract You</a></li>
<li><a class="reference internal" href="#implementing-reward-to-go-policy-gradient" id="id12">Implementing Reward-to-Go Policy Gradient</a></li>
<li><a class="reference internal" href="#baselines-in-policy-gradients" id="id13">Baselines in Policy Gradients</a></li>
<li><a class="reference internal" href="#other-forms-of-the-policy-gradient" id="id14">Other Forms of the Policy Gradient</a></li>
<li><a class="reference internal" href="#recap" id="id15">Recap</a></li>
</ul>
</li>
</ul>
</div>
<p>In this section, we’ll discuss the mathematical foundations of policy optimization algorithms, and connect the material to sample code. We will cover three key results in the theory of <strong>policy gradients</strong>:</p>
<ul class="simple">
<li><a class="reference external" href="../spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient">the simplest equation</a> describing the gradient of policy performance with respect to policy parameters,</li>
<li>a rule which allows us to <a class="reference external" href="../spinningup/rl_intro3.html#don-t-let-the-past-distract-you">drop useless terms</a> from that expression,</li>
<li>and a rule which allows us to <a class="reference external" href="../spinningup/rl_intro3.html#baselines-in-policy-gradients">add useful terms</a> to that expression.</li>
</ul>
<p>In the end, we’ll tie those results together and describe the advantage-based expression for the policy gradient—the version we use in our <a class="reference external" href="../algorithms/vpg.html">Vanilla Policy Gradient</a> implementation.</p>
<div class="section" id="deriving-the-simplest-policy-gradient">
<h2><a class="toc-backref" href="#id8">Deriving the Simplest Policy Gradient</a><a class="headerlink" href="#deriving-the-simplest-policy-gradient" title="Permalink to this headline">¶</a></h2>
<p>Here, we consider the case of a stochastic, parameterized policy, <img class="math" src="../_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg" alt="\pi_{\theta}"/>. We aim to maximise the expected return <img class="math" src="../_images/math/42bfa236d63e32e501baf89345748061540e102d.svg" alt="J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{R(\tau)}"/>. For the purposes of this derivation, we’ll take <img class="math" src="../_images/math/c2d6738c058406ade40dcf870311db157ed80e0f.svg" alt="R(\tau)"/> to give the <a class="reference external" href="../spinningup/rl_intro.html#reward-and-return">finite-horizon undiscounted return</a>, but the derivation for the infinite-horizon discounted return setting is almost identical.</p>
<p>We would like to optimise the policy by gradient ascent, eg</p>
<div class="math">
<p><img src="../_images/math/595de3acc68fed2ec24c11420d9abbee013497ac.svg" alt="\theta_{k+1} = \theta_k + \alpha \left. \nabla_{\theta} J(\pi_{\theta}) \right|_{\theta_k}."/></p>
</div><p>The gradient of policy performance, <img class="math" src="../_images/math/fdc185c68404ece5c4deef076c9713af689421a2.svg" alt="\nabla_{\theta} J(\pi_{\theta})"/>, is called the <strong>policy gradient</strong>, and algorithms that optimise the policy this way are called <strong>policy gradient algorithms.</strong> (Examples include Vanilla Policy Gradient and TRPO. PPO is often referred to as a policy gradient algorithm, though this is slightly inaccurate.)</p>
<p>To actually use this algorithm, we need an expression for the policy gradient which we can numerically compute. This involves two steps: 1) deriving the analytical gradient of policy performance, which turns out to have the form of an expected value, and then 2) forming a sample estimate of that expected value, which can be computed with data from a finite number of agent-environment interaction steps.</p>
<p>In this subsection, we’ll find the simplest form of that expression. In later subsections, we’ll show how to improve on the simplest form to get the version we actually use in standard policy gradient implementations.</p>
<p>We’ll begin by laying out a few facts which are useful for deriving the analytical gradient.</p>
<p><strong>1. Probability of a Trajectory.</strong> The probability of a trajectory <img class="math" src="../_images/math/59f955ca56f900cb9be25620d7c974afc0e77f32.svg" alt="\tau = (s_0, a_0, ..., s_{T+1})"/> given that actions come from <img class="math" src="../_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg" alt="\pi_{\theta}"/> is</p>
<div class="math">
<p><img src="../_images/math/cbc185e90569437c9216ea06df6e91244df1972b.svg" alt="P(\tau|\theta) = \rho_0 (s_0) \prod_{t=0}^{T} P(s_{t+1}|s_t, a_t) \pi_{\theta}(a_t |s_t)."/></p>
</div><p><strong>2. The Log-Derivative Trick.</strong> The log-derivative trick is based on a simple rule from calculus: the derivative of <img class="math" src="../_images/math/fe3fe64e081ed066a8e6a1719b6f3fc8ed9b98dc.svg" alt="\log x"/> with respect to <img class="math" src="../_images/math/ea07a4204f1f53321f76d9c7e348199f0d707db1.svg" alt="x"/> is <img class="math" src="../_images/math/37ea41f23266fbdbb332134f34f65bc547e3601d.svg" alt="1/x"/>. When rearranged and combined with chain rule, we get:</p>
<div class="math">
<p><img src="../_images/math/8025b1c01b73e7f373fa438c15743c5391e2f28f.svg" alt="\nabla_{\theta} P(\tau | \theta) = P(\tau | \theta) \nabla_{\theta} \log P(\tau | \theta)."/></p>
</div><p><strong>3. Log-Probability of a Trajectory.</strong> The log-prob of a trajectory is just</p>
<div class="math">
<p><img src="../_images/math/2c8b420444accb2f54391017f28ab21b97bab0ee.svg" alt="\log P(\tau|\theta) = \log \rho_0 (s_0) + \sum_{t=0}^{T} \bigg( \log P(s_{t+1}|s_t, a_t)  + \log \pi_{\theta}(a_t |s_t)\bigg)."/></p>
</div><p><strong>4. Gradients of Environment Functions.</strong> The environment has no dependence on <img class="math" src="../_images/math/ce5edddd490112350f4bd555d9390e0e845f754a.svg" alt="\theta"/>, so gradients of <img class="math" src="../_images/math/cd870494ffa26602c4ede5257356d572045ebb9e.svg" alt="\rho_0(s_0)"/>, <img class="math" src="../_images/math/d5f6af878d6a1071f29a9dda286b52ef53ece0e3.svg" alt="P(s_{t+1}|s_t, a_t)"/>, and <img class="math" src="../_images/math/c2d6738c058406ade40dcf870311db157ed80e0f.svg" alt="R(\tau)"/> are zero.</p>
<p><strong>5. Grad-Log-Prob of a Trajectory.</strong> The gradient of the log-prob of a trajectory is thus</p>
<div class="math">
<p><img src="../_images/math/3ef66d94ee26cfa69015915dbd112ea78fb5e7ba.svg" alt="\nabla_{\theta} \log P(\tau | \theta) &amp;= \cancel{\nabla_{\theta} \log \rho_0 (s_0)} + \sum_{t=0}^{T} \bigg( \cancel{\nabla_{\theta} \log P(s_{t+1}|s_t, a_t)}  + \nabla_{\theta} \log \pi_{\theta}(a_t |s_t)\bigg) \\
&amp;= \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t)."/></p>
</div><p>Putting it all together, we derive the following:</p>
<div class="admonition-derivation-for-basic-policy-gradient admonition">
<p class="first admonition-title">Derivation for Basic Policy Gradient</p>
<div class="last math">
<p><img src="../_images/math/b5e135d2ae389147267372abc1c5b20e644ec881.svg" alt="\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &amp;= \nabla_{\theta} \underE{\tau \sim \pi_{\theta}}{R(\tau)} &amp; \\
&amp;= \nabla_{\theta} \int_{\tau} P(\tau|\theta) R(\tau) &amp; \text{Expand expectation} \\
&amp;= \int_{\tau} \nabla_{\theta} P(\tau|\theta) R(\tau) &amp; \text{Bring gradient under integral} \\
&amp;= \int_{\tau} P(\tau|\theta) \nabla_{\theta} \log P(\tau|\theta) R(\tau) &amp; \text{Log-derivative trick} \\
&amp;= \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log P(\tau|\theta) R(\tau)} &amp; \text{Return to expectation form} \\
\therefore \nabla_{\theta} J(\pi_{\theta}) &amp;= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau)} &amp; \text{Expression for grad-log-prob}
\end{align*}"/></p>
</div></div>
<p>This is an expectation, which means that we can estimate it with a sample mean. If we collect a set of trajectories <img class="math" src="../_images/math/fcfe0c7093afb2538a298abf5bf8dc0bc435fbb0.svg" alt="\mathcal{D} = \{\tau_i\}_{i=1,...,N}"/> where each trajectory is obtained by letting the agent act in the environment using the policy <img class="math" src="../_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg" alt="\pi_{\theta}"/>, the policy gradient can be estimated with</p>
<div class="math">
<p><img src="../_images/math/3d29a18c0f98b1cdb656ecdf261ee37ffe8bb74b.svg" alt="\hat{g} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau),"/></p>
</div><p>where <img class="math" src="../_images/math/9818d8f4b97ecbea56ebfb3c0a8890faca55c4de.svg" alt="|\mathcal{D}|"/> is the number of trajectories in <img class="math" src="../_images/math/8c5790e22c3e4fca88012b71d8b024c66699cba5.svg" alt="\mathcal{D}"/> (here, <img class="math" src="../_images/math/9d2283e49e17a5e18105bf77e0cc9382192d2510.svg" alt="N"/>).</p>
<p>This last expression is the simplest version of the computable expression we desired. Assuming that we have represented our policy in a way which allows us to calculate <img class="math" src="../_images/math/0a5342e24606940a10098d0cdf85d5e5e346ed72.svg" alt="\nabla_{\theta} \log \pi_{\theta}(a|s)"/>, and if we are able to run the policy in the environment to collect the trajectory dataset, we can compute the policy gradient and take an update step.</p>
</div>
<div class="section" id="implementing-the-simplest-policy-gradient">
<h2><a class="toc-backref" href="#id9">Implementing the Simplest Policy Gradient</a><a class="headerlink" href="#implementing-the-simplest-policy-gradient" title="Permalink to this headline">¶</a></h2>
<p>We give a short PyTorch implementation of this simple version of the policy gradient algorithm in <code class="docutils literal notranslate"><span class="pre">spinup/examples/pytorch/pg_math/1_simple_pg.py</span></code>. (It can also be viewed <a class="reference external" href="https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py">on github</a>.) It is only 128 lines long, so we highly recommend reading through it in depth. While we won’t go through the entirety of the code here, we’ll highlight and explain a few important pieces.</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">This section was previously written with a Tensorflow example. The old Tensorflow section can be found <a class="reference external" href="../spinningup/extra_tf_pg_implementation.html#implementing-the-simplest-policy-gradient">here</a>.</p>
</div>
<p><strong>1. Making the Policy Network.</strong></p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>30
31
32
33
34
35
36
37
38
39
40</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1"># make core of policy network</span>
<span class="n">logits_net</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">sizes</span><span class="o">=</span><span class="p">[</span><span class="n">obs_dim</span><span class="p">]</span><span class="o">+</span><span class="n">hidden_sizes</span><span class="o">+</span><span class="p">[</span><span class="n">n_acts</span><span class="p">])</span>

<span class="c1"># make function to compute action distribution</span>
<span class="k">def</span> <span class="nf">get_policy</span><span class="p">(</span><span class="n">obs</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">logits_net</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>

<span class="c1"># make action selection function (outputs int actions, sampled from policy)</span>
<span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">obs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">get_policy</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</td></tr></table></div>
<p>This block builds modules and functions for using a feedforward neural network categorical policy. (See the <a class="reference external" href="../spinningup/rl_intro.html#stochastic-policies">Stochastic Policies</a> section in Part 1 for a refresher.) The output from the <code class="docutils literal notranslate"><span class="pre">logits_net</span></code> module can be used to construct log-probabilities and probabilities for actions, and the <code class="docutils literal notranslate"><span class="pre">get_action</span></code> function samples actions based on probabilities computed from the logits. (Note: this particular <code class="docutils literal notranslate"><span class="pre">get_action</span></code> function assumes that there will only be one <code class="docutils literal notranslate"><span class="pre">obs</span></code> provided, and therefore only one integer action output. That’s why it uses <code class="docutils literal notranslate"><span class="pre">.item()</span></code>, which is used to <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item">get the contents of a Tensor with only one element</a>.)</p>
<p>A lot of work in this example is getting done by the <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> object on L36. This is a PyTorch <code class="docutils literal notranslate"><span class="pre">Distribution</span></code> object that wraps up some mathematical functions associated with probability distributions. In particular, it has a method for sampling from the distribution (which we use on L40) and a method for computing log probabilities of given samples (which we use later). Since PyTorch distributions are really useful for RL, check out <a class="reference external" href="https://pytorch.org/docs/stable/distributions.html">their documentation</a> to get a feel for how they work.</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p>Friendly reminder! When we talk about a categorical distribution having “logits,” what we mean is that the probabilities for each outcome are given by the Softmax function of the logits. That is, the probability for action <img class="math" src="../_images/math/b42a5fa0aad66603180aff0fc5e346e98a2364ca.svg" alt="j"/> under a categorical distribution with logits <img class="math" src="../_images/math/3292288e05ddeecc31b25cde7cc7aeafff61bf43.svg" alt="x_j"/> is:</p>
<div class="last math">
<p><img src="../_images/math/54e96f0cfda9010b97808eae635a2200e1482829.svg" alt="p_j = \frac{\exp(x_j)}{\sum_{i} \exp(x_i)}"/></p>
</div></div>
<p><strong>2. Making the Loss Function.</strong></p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>42
43
44
45</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1"># make loss function whose gradient, for the right data, is policy gradient</span>
<span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">act</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">logp</span> <span class="o">=</span> <span class="n">get_policy</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">logp</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</td></tr></table></div>
<p>In this block, we build a “loss” function for the policy gradient algorithm. When the right data is plugged in, the gradient of this loss is equal to the policy gradient. The right data means a set of (state, action, weight) tuples collected while acting according to the current policy, where the weight for a state-action pair is the return from the episode to which it belongs. (Although as we will show in later subsections, there are other values you can plug in for the weight which also work correctly.)</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p>Even though we describe this as a loss function, it is <strong>not</strong> a loss function in the typical sense from supervised learning. There are two main differences from standard loss functions.</p>
<p><strong>1. The data distribution depends on the parameters.</strong> A loss function is usually defined on a fixed data distribution which is independent of the parameters we aim to optimise. Not so here, where the data must be sampled on the most recent policy.</p>
<p><strong>2. It doesn’t measure performance.</strong> A loss function usually evaluates the performance metric that we care about. Here, we care about expected return, <img class="math" src="../_images/math/96b876944de9cf0f980fe261562e8e07029245bf.svg" alt="J(\pi_{\theta})"/>, but our “loss” function does not approximate this at all, even in expectation. This “loss” function is only useful to us because, when evaluated at the current parameters, with data generated by the current parameters, it has the negative gradient of performance.</p>
<p>But after that first step of gradient descent, there is no more connection to performance. This means that minimizing this “loss” function, for a given batch of data, has <em>no</em> guarantee whatsoever of improving expected return. You can send this loss to <img class="math" src="../_images/math/5d7bd7abcf6c1c07becaa8b5fe1a2a000e559a50.svg" alt="-\infty"/> and policy performance could crater; in fact, it usually will. Sometimes a deep RL researcher might describe this outcome as the policy “overfitting” to a batch of data. This is descriptive, but should not be taken literally because it does not refer to generalization error.</p>
<p class="last">We raise this point because it is common for ML practitioners to interpret a loss function as a useful signal during training—“if the loss goes down, all is well.” In policy gradients, this intuition is wrong, and you should only care about average return. The loss function means nothing.</p>
</div>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p>The approach used here to make the <code class="docutils literal notranslate"><span class="pre">logp</span></code> tensor–calling the <code class="docutils literal notranslate"><span class="pre">log_prob</span></code> method of a PyTorch <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> object–may require some modification to work with other kinds of distribution objects.</p>
<p>For example, if you are using a <a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#normal">Normal distribution</a> (for a diagonal Gaussian policy), the output from calling <code class="docutils literal notranslate"><span class="pre">policy.log_prob(act)</span></code> will give you a Tensor containing separate log probabilities for each component of each vector-valued action. That is to say, you put in a Tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">act_dim)</span></code>, and get out a Tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">act_dim)</span></code>, when what you need for making an RL loss is a Tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch,)</span></code>. In that case, you would sum up the log probabilities of the action components to get the log probabilities of the actions. That is, you would compute:</p>
<div class="last highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logp</span> <span class="o">=</span> <span class="n">get_policy</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">act</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><strong>3. Running One Epoch of Training.</strong></p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1"># for training policy</span>
<span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">():</span>
    <span class="c1"># make some empty lists for logging.</span>
    <span class="n">batch_obs</span> <span class="o">=</span> <span class="p">[]</span>          <span class="c1"># for observations</span>
    <span class="n">batch_acts</span> <span class="o">=</span> <span class="p">[]</span>         <span class="c1"># for actions</span>
    <span class="n">batch_weights</span> <span class="o">=</span> <span class="p">[]</span>      <span class="c1"># for R(tau) weighting in policy gradient</span>
    <span class="n">batch_rets</span> <span class="o">=</span> <span class="p">[]</span>         <span class="c1"># for measuring episode returns</span>
    <span class="n">batch_lens</span> <span class="o">=</span> <span class="p">[]</span>         <span class="c1"># for measuring episode lengths</span>

    <span class="c1"># reset episode-specific variables</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>       <span class="c1"># first obs comes from starting distribution</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>            <span class="c1"># signal from environment that episode is over</span>
    <span class="n">ep_rews</span> <span class="o">=</span> <span class="p">[]</span>            <span class="c1"># list for rewards accrued throughout ep</span>

    <span class="c1"># render first episode of each epoch</span>
    <span class="n">finished_rendering_this_epoch</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># collect experience by acting in the environment with current policy</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>

        <span class="c1"># rendering</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">finished_rendering_this_epoch</span><span class="p">)</span> <span class="ow">and</span> <span class="n">render</span><span class="p">:</span>
            <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

        <span class="c1"># save obs</span>
        <span class="n">batch_obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

        <span class="c1"># act in the environment</span>
        <span class="n">act</span> <span class="o">=</span> <span class="n">get_action</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>

        <span class="c1"># save action, reward</span>
        <span class="n">batch_acts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
        <span class="n">ep_rews</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rew</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># if episode is over, record info about episode</span>
            <span class="n">ep_ret</span><span class="p">,</span> <span class="n">ep_len</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ep_rews</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">ep_rews</span><span class="p">)</span>
            <span class="n">batch_rets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ep_ret</span><span class="p">)</span>
            <span class="n">batch_lens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ep_len</span><span class="p">)</span>

            <span class="c1"># the weight for each logprob(a|s) is R(tau)</span>
            <span class="n">batch_weights</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ep_ret</span><span class="p">]</span> <span class="o">*</span> <span class="n">ep_len</span>

            <span class="c1"># reset episode-specific variables</span>
            <span class="n">obs</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">ep_rews</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(),</span> <span class="kc">False</span><span class="p">,</span> <span class="p">[]</span>

            <span class="c1"># won&#39;t render again this epoch</span>
            <span class="n">finished_rendering_this_epoch</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="c1"># end experience loop if we have enough of it</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_obs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="k">break</span>

    <span class="c1"># take a single policy gradient update step</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">obs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">batch_obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                              <span class="n">act</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">batch_acts</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                              <span class="n">weights</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">batch_weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                              <span class="p">)</span>
    <span class="n">batch_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">batch_loss</span><span class="p">,</span> <span class="n">batch_rets</span><span class="p">,</span> <span class="n">batch_lens</span>
</pre></div>
</td></tr></table></div>
<p>The <code class="docutils literal notranslate"><span class="pre">train_one_epoch()</span></code> function runs one “epoch” of policy gradient, which we define to be</p>
<ol class="arabic simple">
<li>the experience collection step (L67-102), where the agent acts for some number of episodes in the environment using the most recent policy, followed by</li>
<li>a single policy gradient update step (L104-111).</li>
</ol>
<p>The main loop of the algorithm just repeatedly calls <code class="docutils literal notranslate"><span class="pre">train_one_epoch()</span></code>.</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">If you aren’t already familiar with optimization in PyTorch, observe the pattern for taking one gradient descent step as shown in lines 104-111. First, clear the gradient buffers. Then, compute the loss function. Then, compute a backward pass on the loss function; this accumulates fresh gradients into the gradient buffers. Finally, take a step with the optimizer.</p>
</div>
</div>
<div class="section" id="expected-grad-log-prob-lemma">
<h2><a class="toc-backref" href="#id10">Expected Grad-Log-Prob Lemma</a><a class="headerlink" href="#expected-grad-log-prob-lemma" title="Permalink to this headline">¶</a></h2>
<p>In this subsection, we will derive an intermediate result which is extensively used throughout the theory of policy gradients. We will call it the Expected Grad-Log-Prob (EGLP) lemma. <a class="footnote-reference" href="#id2" id="id1">[1]</a></p>
<p><strong>EGLP Lemma.</strong> Suppose that <img class="math" src="../_images/math/9c44674c334586fd6d417281ea223857ea3ee0d6.svg" alt="P_{\theta}"/> is a parameterized probability distribution over a random variable, <img class="math" src="../_images/math/ea07a4204f1f53321f76d9c7e348199f0d707db1.svg" alt="x"/>. Then:</p>
<div class="math">
<p><img src="../_images/math/f7a02965e7c07537dfb98391da78ab7613e887f7.svg" alt="\underE{x \sim P_{\theta}}{\nabla_{\theta} \log P_{\theta}(x)} = 0."/></p>
</div><div class="admonition-proof admonition">
<p class="first admonition-title">Proof</p>
<p>Recall that all probability distributions are <em>normalised</em>:</p>
<div class="math">
<p><img src="../_images/math/cc70eeb5c704222ca73cf6d2a7b28b8ae2f2e2aa.svg" alt="\int_x P_{\theta}(x) = 1."/></p>
</div><p>Take the gradient of both sides of the normalization condition:</p>
<div class="math">
<p><img src="../_images/math/acde4d18e221ccd91e93e6ca4e25ae84d05f29d8.svg" alt="\nabla_{\theta} \int_x P_{\theta}(x) = \nabla_{\theta} 1 = 0."/></p>
</div><p>Use the log derivative trick to get:</p>
<div class="last math">
<p><img src="../_images/math/479e34ecc40103079f87e46e47837f3c066d30bd.svg" alt="0 &amp;= \nabla_{\theta} \int_x P_{\theta}(x) \\
&amp;= \int_x \nabla_{\theta} P_{\theta}(x) \\
&amp;= \int_x P_{\theta}(x) \nabla_{\theta} \log P_{\theta}(x) \\
\therefore 0 &amp;= \underE{x \sim P_{\theta}}{\nabla_{\theta} \log P_{\theta}(x)}."/></p>
</div></div>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>The author of this article is not aware of this lemma being given a standard name anywhere in the literature. But given how often it comes up, it seems pretty worthwhile to give it some kind of name for ease of reference.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="don-t-let-the-past-distract-you">
<h2><a class="toc-backref" href="#id11">Don’t Let the Past Distract You</a><a class="headerlink" href="#don-t-let-the-past-distract-you" title="Permalink to this headline">¶</a></h2>
<p>Examine our most recent expression for the policy gradient:</p>
<div class="math">
<p><img src="../_images/math/e8b721fa0eb7fa2aa4b088106518b3ee88ff7707.svg" alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau)}."/></p>
</div><p>Taking a step with this gradient pushes up the log-probabilities of each action in proportion to <img class="math" src="../_images/math/c2d6738c058406ade40dcf870311db157ed80e0f.svg" alt="R(\tau)"/>, the sum of <em>all rewards ever obtained</em>. But this doesn’t make much sense.</p>
<p>Agents should really only reinforce actions on the basis of their <em>consequences</em>. Rewards obtained before taking an action have no bearing on how good that action was: only rewards that come <em>after</em>.</p>
<p>It turns out that this intuition shows up in the math, and we can show that the policy gradient can also be expressed by</p>
<div class="math">
<p><img src="../_images/math/62e6b4e06a1c35fac29e94103988cdc6e940660b.svg" alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1})}."/></p>
</div><p>In this form, actions are only reinforced based on rewards obtained after they are taken.</p>
<p>We’ll call this form the “reward-to-go policy gradient,” because the sum of rewards after a point in a trajectory,</p>
<div class="math">
<p><img src="../_images/math/d299609cb8b73f294e77708f9cdc6ea0024b6c6c.svg" alt="\hat{R}_t \doteq \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}),"/></p>
</div><p>is called the <strong>reward-to-go</strong> from that point, and this policy gradient expression depends on the reward-to-go from state-action pairs.</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last"><strong>But how is this better?</strong> A key problem with policy gradients is how many sample trajectories are needed to get a low-variance sample estimate for them. The formula we started with included terms for reinforcing actions proportional to past rewards, all of which had zero mean, but nonzero variance: as a result, they would just add noise to sample estimates of the policy gradient. By removing them, we reduce the number of sample trajectories needed.</p>
</div>
<p>An (optional) proof of this claim can be found <a href="#id16"><span class="problematic" id="id17">`here`_</span></a>, and it ultimately depends on the EGLP lemma.</p>
</div>
<div class="section" id="implementing-reward-to-go-policy-gradient">
<h2><a class="toc-backref" href="#id12">Implementing Reward-to-Go Policy Gradient</a><a class="headerlink" href="#implementing-reward-to-go-policy-gradient" title="Permalink to this headline">¶</a></h2>
<p>We give a short PyTorch implementation of the reward-to-go policy gradient in <code class="docutils literal notranslate"><span class="pre">spinup/examples/pytorch/pg_math/2_rtg_pg.py</span></code>. (It can also be viewed <a class="reference external" href="https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/2_rtg_pg.py">on github</a>.)</p>
<p>The only thing that has changed from <code class="docutils literal notranslate"><span class="pre">1_simple_pg.py</span></code> is that we now use different weights in the loss function. The code modification is very slight: we add a new function, and change two other lines. The new function is:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>17
18
19
20
21
22</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reward_to_go</span><span class="p">(</span><span class="n">rews</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rews</span><span class="p">)</span>
    <span class="n">rtgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">rews</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)):</span>
        <span class="n">rtgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rews</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">rtgs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rtgs</span>
</pre></div>
</td></tr></table></div>
<p>And then we tweak the old L91-92 from:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>91
92</pre></div></td><td class="code"><div class="highlight"><pre><span></span>                <span class="c1"># the weight for each logprob(a|s) is R(tau)</span>
                <span class="n">batch_weights</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ep_ret</span><span class="p">]</span> <span class="o">*</span> <span class="n">ep_len</span>
</pre></div>
</td></tr></table></div>
<p>to:</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>98
99</pre></div></td><td class="code"><div class="highlight"><pre><span></span>                <span class="c1"># the weight for each logprob(a_t|s_t) is reward-to-go from t</span>
                <span class="n">batch_weights</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">reward_to_go</span><span class="p">(</span><span class="n">ep_rews</span><span class="p">))</span>
</pre></div>
</td></tr></table></div>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">This section was previously written with a Tensorflow example. The old Tensorflow section can be found <a class="reference external" href="../spinningup/extra_tf_pg_implementation.html#implementing-reward-to-go-policy-gradient">here</a>.</p>
</div>
</div>
<div class="section" id="baselines-in-policy-gradients">
<h2><a class="toc-backref" href="#id13">Baselines in Policy Gradients</a><a class="headerlink" href="#baselines-in-policy-gradients" title="Permalink to this headline">¶</a></h2>
<p>An immediate consequence of the EGLP lemma is that for any function <img class="math" src="../_images/math/99ac829ad51642abad0797da299214e7ce1da722.svg" alt="b"/> which only depends on state,</p>
<div class="math">
<p><img src="../_images/math/3bedd2ab2262f396f232d49c8c85621ce5397955.svg" alt="\underE{a_t \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) b(s_t)} = 0."/></p>
</div><p>This allows us to add or subtract any number of terms like this from our expression for the policy gradient, without changing it in expectation:</p>
<div class="math">
<p><img src="../_images/math/3a111dcb6e04aa632bd69e9a7e769e06e2530a0a.svg" alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \left(\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t)\right)}."/></p>
</div><p>Any function <img class="math" src="../_images/math/99ac829ad51642abad0797da299214e7ce1da722.svg" alt="b"/> used in this way is called a <strong>baseline</strong>.</p>
<p>The most common choice of baseline is the <a class="reference external" href="../spinningup/rl_intro.html#value-functions">on-policy value function</a> <img class="math" src="../_images/math/263857fcb5eaaf46a4e9a5e6ce2be414d95748aa.svg" alt="V^{\pi}(s_t)"/>. Recall that this is the average return an agent gets if it starts in state <img class="math" src="../_images/math/4fcf0bf03c2a691496ce04ade269159d8b89caa5.svg" alt="s_t"/> and then acts according to policy <img class="math" src="../_images/math/1ae2bd722da01b3a89ffc139af2437c28364a966.svg" alt="\pi"/> for the rest of its life.</p>
<p>Empirically, the choice <img class="math" src="../_images/math/67567dbe29590f80b62fd2221481df61e502a450.svg" alt="b(s_t) = V^{\pi}(s_t)"/> has the desirable effect of reducing variance in the sample estimate for the policy gradient. This results in faster and more stable policy learning. It is also appealing from a conceptual angle: it encodes the intuition that if an agent gets what it expected, it should “feel” neutral about it.</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p>In practice, <img class="math" src="../_images/math/263857fcb5eaaf46a4e9a5e6ce2be414d95748aa.svg" alt="V^{\pi}(s_t)"/> cannot be computed exactly, so it has to be approximated. This is usually done with a neural network, <img class="math" src="../_images/math/d313ca98e5fb043c581841a09ea19bce2ce53b04.svg" alt="V_{\phi}(s_t)"/>, which is updated concurrently with the policy (so that the value network always approximates the value function of the most recent policy).</p>
<p>The simplest method for learning <img class="math" src="../_images/math/4be08f55c234dcd60641540fb682afc3e988ae17.svg" alt="V_{\phi}"/>, used in most implementations of policy optimization algorithms (including VPG, TRPO, PPO, and A2C), is to minimise a mean-squared-error objective:</p>
<div class="math">
<p><img src="../_images/math/a82208dd637243514710948c4ebbc3c59e9a2e57.svg" alt="\phi_k = \arg \min_{\phi} \underE{s_t, \hat{R}_t \sim \pi_k}{\left( V_{\phi}(s_t) - \hat{R}_t \right)^2},"/></p>
</div><div class="line-block">
<div class="line"><br /></div>
</div>
<p class="last">where <img class="math" src="../_images/math/35e7c0260233cac2eb0745c92a34cfaa21e558cb.svg" alt="\pi_k"/> is the policy at epoch <img class="math" src="../_images/math/a29aa94bd66ac7a6bb3195233fd9a9df8575af86.svg" alt="k"/>. This is done with one or more steps of gradient descent, starting from the previous value parameters <img class="math" src="../_images/math/ba40df6b8d4ee942c6856a8d919ce01fc92096f6.svg" alt="\phi_{k-1}"/>.</p>
</div>
</div>
<div class="section" id="other-forms-of-the-policy-gradient">
<h2><a class="toc-backref" href="#id14">Other Forms of the Policy Gradient</a><a class="headerlink" href="#other-forms-of-the-policy-gradient" title="Permalink to this headline">¶</a></h2>
<p>What we have seen so far is that the policy gradient has the general form</p>
<div class="math">
<p><img src="../_images/math/eb524fc4ce3052c9058d2221471ac8b302c9c023.svg" alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Phi_t},"/></p>
</div><p>where <img class="math" src="../_images/math/7bec6ad1be0afd7200e874effc2db6d9cafb3ea4.svg" alt="\Phi_t"/> could be any of</p>
<div class="math">
<p><img src="../_images/math/cecde3d5124076dfc773c8fa658e61f41cb3efc2.svg" alt="\Phi_t &amp;= R(\tau),"/></p>
</div><p>or</p>
<div class="math">
<p><img src="../_images/math/02a2c10508e4a4c018634a2ba03384350faa7cab.svg" alt="\Phi_t &amp;= \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}),"/></p>
</div><p>or</p>
<div class="math">
<p><img src="../_images/math/65fd02144cdac143a61396dc8fe585e8db5f7d81.svg" alt="\Phi_t &amp;= \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t)."/></p>
</div><p>All of these choices lead to the same expected value for the policy gradient, despite having different variances. It turns out that there are two more valid choices of weights <img class="math" src="../_images/math/7bec6ad1be0afd7200e874effc2db6d9cafb3ea4.svg" alt="\Phi_t"/> which are important to know.</p>
<p><strong>1. On-Policy Action-Value Function.</strong> The choice</p>
<div class="math">
<p><img src="../_images/math/90381ce64af2629806c052198d4a0851ff6be36b.svg" alt="\Phi_t = Q^{\pi_{\theta}}(s_t, a_t)"/></p>
</div><p>is also valid. See <a class="reference external" href="../spinningup/extra_pg_proof2.html">this page</a> for an (optional) proof of this claim.</p>
<p><strong>2. The Advantage Function.</strong> Recall that the <a class="reference external" href="../spinningup/rl_intro.html#advantage-functions">advantage of an action</a>, defined by <img class="math" src="../_images/math/e2d2e497244979ed8fe62fe2e994a6b49039580c.svg" alt="A^{\pi}(s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t)"/>,  describes how much better or worse it is than other actions on average (relative to the current policy). This choice,</p>
<div class="math">
<p><img src="../_images/math/e67e0f92306202526b49c7d0a7a6239ba68a7abc.svg" alt="\Phi_t = A^{\pi_{\theta}}(s_t, a_t)"/></p>
</div><p>is also valid. The proof is that it’s equivalent to using <img class="math" src="../_images/math/71febd8462276063cccd137802934788492dc5f1.svg" alt="\Phi_t = Q^{\pi_{\theta}}(s_t, a_t)"/> and then using a value function baseline, which we are always free to do.</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">The formulation of policy gradients with advantage functions is extremely common, and there are many different ways of estimating the advantage function used by different algorithms.</p>
</div>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p>For a more detailed treatment of this topic, you should read the paper on <a class="reference external" href="https://arxiv.org/abs/1506.02438">Generalized Advantage Estimation</a> (GAE), which goes into depth about different choices of <img class="math" src="../_images/math/7bec6ad1be0afd7200e874effc2db6d9cafb3ea4.svg" alt="\Phi_t"/> in the background sections.</p>
<p class="last">That paper then goes on to describe GAE, a method for approximating the advantage function in policy optimization algorithms which enjoys widespread use. For instance, Spinning Up’s implementations of VPG, TRPO, and PPO make use of it. As a result, we strongly advise you to study it.</p>
</div>
</div>
<div class="section" id="recap">
<h2><a class="toc-backref" href="#id15">Recap</a><a class="headerlink" href="#recap" title="Permalink to this headline">¶</a></h2>
<p>In this chapter, we described the basic theory of policy gradient methods and connected some of the early results to code examples. The interested student should continue from here by studying how the later results (value function baselines and the advantage formulation of policy gradients) translate into Spinning Up’s implementation of <a class="reference external" href="../algorithms/vpg.html">Vanilla Policy Gradient</a>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Rick Staa

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>