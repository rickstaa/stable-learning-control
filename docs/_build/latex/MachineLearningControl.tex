%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{User Documentation}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\makeatletter
\def\fnum@figure{\figurename\thefigure{}}
\makeatother
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\makeatletter
\def\fnum@table{\tablename\thetable{}}
\makeatother
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}


\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{cancel}


\newcommand{\E}{{\mathrm E}}

\newcommand{\underE}[2]{\underset{\begin{subarray}{c}#1 \end{subarray}}{\E}\left[ #2 \right]}

\newcommand{\Epi}[1]{\underset{\begin{subarray}{c}\tau \sim \pi \end{subarray}}{\E}\left[ #1 \right]}


\title{Machine Learning Control Documentation}
\date{Dec 20, 2020}
\release{}
\author{Joshua Achiam}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\noindent\sphinxincludegraphics{{c1760c9970339547abf78e19e4455ecf15596acc}.jpg}


\chapter{Introduction}
\label{\detokenize{user/introduction:introduction}}\label{\detokenize{user/introduction::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{user/introduction:id1}}{\hyperref[\detokenize{user/introduction:introduction}]{\sphinxcrossref{Introduction}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{user/introduction:id2}}{\hyperref[\detokenize{user/introduction:what-this-is}]{\sphinxcrossref{What This Is}}}

\item {} 
\phantomsection\label{\detokenize{user/introduction:id3}}{\hyperref[\detokenize{user/introduction:why-we-built-this}]{\sphinxcrossref{Why We Built This}}}

\item {} 
\phantomsection\label{\detokenize{user/introduction:id4}}{\hyperref[\detokenize{user/introduction:how-this-serves-our-mission}]{\sphinxcrossref{How This Serves Our Mission}}}

\item {} 
\phantomsection\label{\detokenize{user/introduction:id5}}{\hyperref[\detokenize{user/introduction:code-design-philosophy}]{\sphinxcrossref{Code Design Philosophy}}}

\item {} 
\phantomsection\label{\detokenize{user/introduction:id6}}{\hyperref[\detokenize{user/introduction:long-term-support-and-support-history}]{\sphinxcrossref{Long-Term Support and Support History}}}

\end{itemize}

\end{itemize}
\end{sphinxShadowBox}


\section{What This Is}
\label{\detokenize{user/introduction:what-this-is}}
Welcome to the Machine Learning Control package!

For a good introduction into RL see the \sphinxhref{https://spinningup.openai.com/en/latest/}{Spinning Up} package.


\section{Why We Built This}
\label{\detokenize{user/introduction:why-we-built-this}}

\section{How This Serves Our Mission}
\label{\detokenize{user/introduction:how-this-serves-our-mission}}

\section{Code Design Philosophy}
\label{\detokenize{user/introduction:code-design-philosophy}}

\section{Long-Term Support and Support History}
\label{\detokenize{user/introduction:long-term-support-and-support-history}}

\chapter{Installation}
\label{\detokenize{user/installation:installation}}\label{\detokenize{user/installation::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{user/installation:id1}}{\hyperref[\detokenize{user/installation:installation}]{\sphinxcrossref{Installation}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{user/installation:id2}}{\hyperref[\detokenize{user/installation:installing-python}]{\sphinxcrossref{Installing Python}}}

\item {} 
\phantomsection\label{\detokenize{user/installation:id3}}{\hyperref[\detokenize{user/installation:installing-openmpi}]{\sphinxcrossref{Installing OpenMPI}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{user/installation:id4}}{\hyperref[\detokenize{user/installation:ubuntu}]{\sphinxcrossref{Ubuntu}}}

\item {} 
\phantomsection\label{\detokenize{user/installation:id5}}{\hyperref[\detokenize{user/installation:mac-os-x}]{\sphinxcrossref{Mac OS X}}}

\end{itemize}

\item {} 
\phantomsection\label{\detokenize{user/installation:id6}}{\hyperref[\detokenize{user/installation:installing-spinning-up}]{\sphinxcrossref{Installing Spinning Up}}}

\item {} 
\phantomsection\label{\detokenize{user/installation:id7}}{\hyperref[\detokenize{user/installation:check-your-install}]{\sphinxcrossref{Check Your Install}}}

\item {} 
\phantomsection\label{\detokenize{user/installation:id8}}{\hyperref[\detokenize{user/installation:installing-mujoco-optional}]{\sphinxcrossref{Installing MuJoCo (Optional)}}}

\end{itemize}

\end{itemize}
\end{sphinxShadowBox}
\begin{description}
\item[{Machine Learning Control requires Python3 and OpenMPI. It comes}] \leavevmode
It is currently only supported on Linux and OSX.

\end{description}

\begin{sphinxadmonition}{note}{You Should Know}

Many examples and benchmarks in Machine Learning Control refer to RL environments that use the \sphinxhref{http://www.mujoco.org/index.html}{MuJoCo} physics engine.
MuJoCo is a proprietary software that requires a license, which is free to trial and free for students, but otherwise is not free.
As a result, installing it is optional, but because of its importance to the research community—it is the de facto standard for
benchmarking deep RL algorithms in continuous control—it is preferred.

Don’t worry if you decide not to install MuJoCo, though. You can definitely get started in RL by running RL algorithms on the
\sphinxhref{https://gym.openai.com/envs/\#classic\_control}{Classic Control} and \sphinxhref{https://gym.openai.com/envs/\#box2d}{Box2d} environments in Gym, which are totally free to use.
\end{sphinxadmonition}


\section{Installing Python}
\label{\detokenize{user/installation:installing-python}}
We recommend installing Python through Anaconda. Anaconda is a library that includes Python and many useful packages for
Python, as well as an environment manager called conda that makes package management simple.

Follow \sphinxhref{https://docs.continuum.io/anaconda/install/}{the installation instructions} for Anaconda here. Download and install Anaconda3 (at time of writing, \sphinxhref{https://repo.anaconda.com/archive/}{Anaconda3-5.3.0}).
Then create a conda Python 3.6 env for organizing packages used in Spinning Up:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{conda} \PYG{n}{create} \PYG{o}{\PYGZhy{}}\PYG{n}{n} \PYG{n}{spinningup} \PYG{n}{python}\PYG{o}{=}\PYG{l+m+mf}{3.6}
\end{sphinxVerbatim}

To use Python from the environment you just created, activate the environment with:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{conda} \PYG{n}{activate} \PYG{n}{spinningup}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{You Should Know}

If you’re new to python environments and package management, this stuff can quickly get confusing or overwhelming, and you’ll probably hit some snags along the way. (Especially, you should expect problems like, “I just installed this thing, but it says it’s not found when I try to use it!”) You may want to read through some clean explanations about what package management is, why it’s a good idea, and what commands you’ll typically have to execute to correctly use it.

\sphinxhref{https://medium.freecodecamp.org/why-you-need-python-environments-and-how-to-manage-them-with-conda-85f155f4353c}{FreeCodeCamp} has a good explanation worth reading. There’s a shorter description on \sphinxhref{https://towardsdatascience.com/environment-management-with-conda-python-2-3-b9961a8a5097}{Towards Data Science} which is also helpful and informative. Finally, if you’re an extremely patient person, you may want to read the (dry, but very informative) \sphinxhref{https://conda.io/docs/user-guide/tasks/manage-environments.html}{documentation page from Conda}.
\end{sphinxadmonition}


\section{Installing OpenMPI}
\label{\detokenize{user/installation:installing-openmpi}}

\subsection{Ubuntu}
\label{\detokenize{user/installation:ubuntu}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sudo} \PYG{n}{apt}\PYG{o}{\PYGZhy{}}\PYG{n}{get} \PYG{n}{update} \PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}} \PYG{n}{sudo} \PYG{n}{apt}\PYG{o}{\PYGZhy{}}\PYG{n}{get} \PYG{n}{install} \PYG{n}{libopenmpi}\PYG{o}{\PYGZhy{}}\PYG{n}{dev}
\end{sphinxVerbatim}


\subsection{Mac OS X}
\label{\detokenize{user/installation:mac-os-x}}
Installation of system packages on Mac requires \sphinxhref{https://brew.sh}{Homebrew}. With Homebrew installed, run the follwing:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{brew} \PYG{n}{install} \PYG{n}{openmpi}
\end{sphinxVerbatim}


\section{Installing Spinning Up}
\label{\detokenize{user/installation:installing-spinning-up}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{git} \PYG{n}{clone} \PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{github}\PYG{o}{.}\PYG{n}{com}\PYG{o}{/}\PYG{n}{openai}\PYG{o}{/}\PYG{n}{spinningup}\PYG{o}{.}\PYG{n}{git}
\PYG{n}{cd} \PYG{n}{spinningup}
\PYG{n}{pip} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{n}{e} \PYG{o}{.}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{You Should Know}

Spinning Up defaults to installing everything in Gym \sphinxstylestrong{except} the MuJoCo environments. In case you run into any trouble with the Gym installation, check out the \sphinxhref{https://github.com/openai/gym}{Gym} github page for help. If you want the MuJoCo environments, see the optional installation section below.
\end{sphinxadmonition}


\section{Check Your Install}
\label{\detokenize{user/installation:check-your-install}}
To see if you’ve successfully installed Spinning Up, try running PPO in the LunarLander-v2 environment with

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{o}{\PYGZhy{}}\PYG{n}{m} \PYG{n}{spinup}\PYG{o}{.}\PYG{n}{run} \PYG{n}{ppo} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hid} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[32,32]}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{env} \PYG{n}{LunarLander}\PYG{o}{\PYGZhy{}}\PYG{n}{v2} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{exp\PYGZus{}name} \PYG{n}{installtest} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{gamma} \PYG{l+m+mf}{0.999}
\end{sphinxVerbatim}

This might run for around 10 minutes, and you can leave it going in the background while you continue reading through documentation. This won’t train the agent to completion, but will run it for long enough that you can see \sphinxstyleemphasis{some} learning progress when the results come in.

After it finishes training, watch a video of the trained policy with

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{o}{\PYGZhy{}}\PYG{n}{m} \PYG{n}{spinup}\PYG{o}{.}\PYG{n}{run} \PYG{n}{test\PYGZus{}policy} \PYG{n}{data}\PYG{o}{/}\PYG{n}{installtest}\PYG{o}{/}\PYG{n}{installtest\PYGZus{}s0}
\end{sphinxVerbatim}

And plot the results with

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{o}{\PYGZhy{}}\PYG{n}{m} \PYG{n}{spinup}\PYG{o}{.}\PYG{n}{run} \PYG{n}{plot} \PYG{n}{data}\PYG{o}{/}\PYG{n}{installtest}\PYG{o}{/}\PYG{n}{installtest\PYGZus{}s0}
\end{sphinxVerbatim}


\section{Installing MuJoCo (Optional)}
\label{\detokenize{user/installation:installing-mujoco-optional}}
First, go to the \sphinxhref{https://github.com/openai/mujoco-py}{mujoco-py} github page. Follow the installation instructions in the README, which describe how to install the MuJoCo physics engine and the mujoco-py package (which allows the use of MuJoCo from Python).

\begin{sphinxadmonition}{note}{You Should Know}

In order to use the MuJoCo simulator, you will need to get a \sphinxhref{https://www.roboti.us/license.html}{MuJoCo license}. Free 30-day licenses are available to anyone, and free 1-year licenses are available to full-time students.
\end{sphinxadmonition}

Once you have installed MuJoCo, install the corresponding Gym environments with

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{gym}\PYG{p}{[}\PYG{n}{mujoco}\PYG{p}{,}\PYG{n}{robotics}\PYG{p}{]}
\end{sphinxVerbatim}

And then check that things are working by running PPO in the Walker2d-v2 environment with

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{o}{\PYGZhy{}}\PYG{n}{m} \PYG{n}{spinup}\PYG{o}{.}\PYG{n}{run} \PYG{n}{ppo} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hid} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[32,32]}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{env} \PYG{n}{Walker2d}\PYG{o}{\PYGZhy{}}\PYG{n}{v2} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{exp\PYGZus{}name} \PYG{n}{mujocotest}
\end{sphinxVerbatim}


\chapter{Soft Actor-Critic}
\label{\detokenize{algorithms/sac:soft-actor-critic}}\label{\detokenize{algorithms/sac::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id2}}{\hyperref[\detokenize{algorithms/sac:soft-actor-critic}]{\sphinxcrossref{Soft Actor-Critic}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id3}}{\hyperref[\detokenize{algorithms/sac:background}]{\sphinxcrossref{Background}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id4}}{\hyperref[\detokenize{algorithms/sac:quick-facts}]{\sphinxcrossref{Quick Facts}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id5}}{\hyperref[\detokenize{algorithms/sac:key-equations}]{\sphinxcrossref{Key Equations}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id6}}{\hyperref[\detokenize{algorithms/sac:entropy-regularized-reinforcement-learning}]{\sphinxcrossref{Entropy-Regularized Reinforcement Learning}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id7}}{\hyperref[\detokenize{algorithms/sac:id1}]{\sphinxcrossref{Soft Actor-Critic}}}

\end{itemize}

\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id8}}{\hyperref[\detokenize{algorithms/sac:exploration-vs-exploitation}]{\sphinxcrossref{Exploration vs. Exploitation}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id9}}{\hyperref[\detokenize{algorithms/sac:pseudocode}]{\sphinxcrossref{Pseudocode}}}

\end{itemize}

\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id10}}{\hyperref[\detokenize{algorithms/sac:documentation}]{\sphinxcrossref{Documentation}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id11}}{\hyperref[\detokenize{algorithms/sac:documentation-pytorch-version}]{\sphinxcrossref{Documentation: PyTorch Version}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id12}}{\hyperref[\detokenize{algorithms/sac:saved-model-contents-pytorch-version}]{\sphinxcrossref{Saved Model Contents: PyTorch Version}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id13}}{\hyperref[\detokenize{algorithms/sac:documentation-tensorflow-version}]{\sphinxcrossref{Documentation: Tensorflow Version}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id14}}{\hyperref[\detokenize{algorithms/sac:saved-model-contents-tensorflow-version}]{\sphinxcrossref{Saved Model Contents: Tensorflow Version}}}

\end{itemize}

\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id15}}{\hyperref[\detokenize{algorithms/sac:references}]{\sphinxcrossref{References}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id16}}{\hyperref[\detokenize{algorithms/sac:relevant-papers}]{\sphinxcrossref{Relevant Papers}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/sac:id17}}{\hyperref[\detokenize{algorithms/sac:other-public-implementations}]{\sphinxcrossref{Other Public Implementations}}}

\end{itemize}

\end{itemize}

\end{itemize}
\end{sphinxShadowBox}


\section{Background}
\label{\detokenize{algorithms/sac:background}}
(Previously: \sphinxhref{../algorithms/td3.html\#background}{Background for TD3})

Soft Actor Critic (SAC) is an algorithm that optimizes a stochastic policy in an off-policy way, forming a bridge between stochastic policy optimization and DDPG-style approaches. It isn’t a direct successor to TD3 (having been published roughly concurrently), but it incorporates the clipped double-Q trick, and due to the inherent stochasticity of the policy in SAC, it also winds up benefiting from something like target policy smoothing.

A central feature of SAC is \sphinxstylestrong{entropy regularisation.} The policy is trained to maximise a trade-off between expected return and \sphinxhref{https://en.wikipedia.org/wiki/Entropy\_(information\_theory)}{entropy}, a measure of randomness in the policy. This has a close connection to the exploration-exploitation trade-off: increasing entropy results in more exploration, which can accelerate learning later on. It can also prevent the policy from prematurely converging to a bad local optimum.


\subsection{Quick Facts}
\label{\detokenize{algorithms/sac:quick-facts}}\begin{itemize}
\item {} 
SAC is an off-policy algorithm.

\item {} 
The version of SAC implemented here can only be used for environments with continuous action spaces.

\item {} 
An alternate version of SAC, which slightly changes the policy update rule, can be implemented to handle discrete action spaces.

\item {} 
The Spinning Up implementation of SAC does not support parallelization.

\end{itemize}


\subsection{Key Equations}
\label{\detokenize{algorithms/sac:key-equations}}
To explain Soft Actor Critic, we first have to introduce the entropy-regularised reinforcement learning setting. In entropy-regularised RL, there are slightly-different equations for value functions.


\subsubsection{Entropy-Regularized Reinforcement Learning}
\label{\detokenize{algorithms/sac:entropy-regularized-reinforcement-learning}}
Entropy is a quantity which, roughly speaking, says how random a random variable is. If a coin is weighted so that it almost always comes up heads, it has low entropy; if it’s evenly weighted and has a half chance of either outcome, it has high entropy.

Let \(x\) be a random variable with probability mass or density function \(P\). The entropy \(H\) of \(x\) is computed from its distribution \(P\) according to
\begin{equation*}
\begin{split}H(P) = \underE{x \sim P}{-\log P(x)}.\end{split}
\end{equation*}
In entropy-regularised reinforcement learning, the agent gets a bonus reward at each time step proportional to the entropy of the policy at that timestep. This changes \sphinxhref{../spinningup/rl\_intro.html\#the-rl-problem}{the RL problem} to:
\begin{equation*}
\begin{split}\pi^* = \arg \max_{\pi} \underE{\tau \sim \pi}{ \sum_{t=0}^{\infty} \gamma^t \bigg( R(s_t, a_t, s_{t+1}) + \alpha H\left(\pi(\cdot|s_t)\right) \bigg)},\end{split}
\end{equation*}
where \(\alpha > 0\) is the trade-off coefficient. (Note: we’re assuming an infinite-horizon discounted setting here, and we’ll do the same for the rest of this page.) We can now define the slightly-different value functions in this setting. \(V^{\pi}\) is changed to include the entropy bonuses from every timestep:
\begin{equation*}
\begin{split}V^{\pi}(s) = \underE{\tau \sim \pi}{ \left. \sum_{t=0}^{\infty} \gamma^t \bigg( R(s_t, a_t, s_{t+1}) + \alpha H\left(\pi(\cdot|s_t)\right) \bigg) \right| s_0 = s}\end{split}
\end{equation*}
\(Q^{\pi}\) is changed to include the entropy bonuses from every timestep \sphinxstyleemphasis{except the first}:
\begin{equation*}
\begin{split}Q^{\pi}(s,a) = \underE{\tau \sim \pi}{ \left. \sum_{t=0}^{\infty} \gamma^t  R(s_t, a_t, s_{t+1}) + \alpha \sum_{t=1}^{\infty} \gamma^t H\left(\pi(\cdot|s_t)\right)\right| s_0 = s, a_0 = a}\end{split}
\end{equation*}
With these definitions, \(V^{\pi}\) and \(Q^{\pi}\) are connected by:
\begin{equation*}
\begin{split}V^{\pi}(s) = \underE{a \sim \pi}{Q^{\pi}(s,a)} + \alpha H\left(\pi(\cdot|s)\right)\end{split}
\end{equation*}
and the Bellman equation for \(Q^{\pi}\) is
\begin{equation*}
\begin{split}Q^{\pi}(s,a) &= \underE{s' \sim P \\ a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') + \alpha H\left(\pi(\cdot|s')\right) \right)} \\
&= \underE{s' \sim P}{R(s,a,s') + \gamma V^{\pi}(s')}.\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{You Should Know}

The way we’ve set up the value functions in the entropy-regularised setting is a little bit arbitrary, and actually we could have done it differently (eg make \(Q^{\pi}\) include the entropy bonus at the first timestep). The choice of definition may vary slightly across papers on the subject.
\end{sphinxadmonition}


\subsubsection{Soft Actor-Critic}
\label{\detokenize{algorithms/sac:id1}}
SAC concurrently learns a policy \(\pi_{\theta}\) and two Q-functions \(Q_{\phi_1}, Q_{\phi_2}\). There are two variants of SAC that are currently standard: one that uses a fixed entropy regularisation coefficient \(\alpha\), and another that enforces an entropy constraint by varying \(\alpha\) over the course of training. For simplicity, Spinning Up makes use of the version with a fixed entropy regularisation coefficient, but the entropy-constrained variant is generally preferred by practitioners.

\begin{sphinxadmonition}{note}{You Should Know}

The SAC algorithm has changed a little bit over time. An older version of SAC also learns a value function \(V_{\psi}\) in addition to the Q-functions; this page will focus on the modern version that omits the extra value function.
\end{sphinxadmonition}

\sphinxstylestrong{Learning Q.} The Q-functions are learned in a similar way to TD3, but with a few key differences.

First, what’s similar?
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi )}
\makeatletter\def\p@enumii{\p@enumi \theenumi )}\makeatother
\item {} 
Like in TD3, both Q-functions are learned with MSBE minimization, by regressing to a single shared target.

\item {} 
Like in TD3, the shared target is computed using target Q-networks, and the target Q-networks are obtained by polyak averaging the Q-network parameters over the course of training.

\item {} 
Like in TD3, the shared target makes use of the \sphinxstylestrong{clipped double-Q} trick.

\end{enumerate}

What’s different?
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi )}
\makeatletter\def\p@enumii{\p@enumi \theenumi )}\makeatother
\item {} 
Unlike in TD3, the target also includes a term that comes from SAC’s use of entropy regularisation.

\item {} 
Unlike in TD3, the next-state actions used in the target come from the \sphinxstylestrong{current policy} instead of a target policy.

\item {} 
Unlike in TD3, there is no explicit target policy smoothing. TD3 trains a deterministic policy, and so it accomplishes smoothing by adding random noise to the next-state actions. SAC trains a stochastic policy, and so the noise from that stochasticity is sufficient to get a similar effect.

\end{enumerate}

Before we give the final form of the Q-loss, let’s take a moment to discuss how the contribution from entropy regularisation comes in. We’ll start by taking our recursive Bellman equation for the entropy-regularised \(Q^{\pi}\) from earlier, and rewriting it a little bit by using the definition of entropy:
\begin{equation*}
\begin{split}Q^{\pi}(s,a) &= \underE{s' \sim P \\ a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') + \alpha H\left(\pi(\cdot|s')\right) \right)} \\
&= \underE{s' \sim P \\ a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') - \alpha \log \pi(a'|s') \right)}\end{split}
\end{equation*}
The RHS is an expectation over next states (which come from the replay buffer) and next actions (which come from the current policy, and \sphinxstylestrong{not} the replay buffer). Since it’s an expectation, we can approximate it with samples:
\begin{equation*}
\begin{split}Q^{\pi}(s,a) &\approx r + \gamma\left(Q^{\pi}(s',\tilde{a}') - \alpha \log \pi(\tilde{a}'|s') \right), \;\;\;\;\;  \tilde{a}' \sim \pi(\cdot|s').\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{You Should Know}

We switch next action notation to \(\tilde{a}'\), instead of \(a'\), to highlight that the next actions have to be sampled fresh from the policy (whereas by contrast, \(r\) and \(s'\) should come from the replay buffer).
\end{sphinxadmonition}

SAC sets up the MSBE loss for each Q-function using this kind of sample approximation for the target. The only thing still undetermined here is which Q-function gets used to compute the sample backup: like TD3, SAC uses the clipped double-Q trick, and takes the minimum Q-value between the two Q approximators.

Putting it all together, the loss functions for the Q-networks in SAC are:
\begin{equation*}
\begin{split}L(\phi_i, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi_i}(s,a) - y(r,s',d) \Bigg)^2
    \right],\end{split}
\end{equation*}
where the target is given by
\begin{equation*}
\begin{split}y(r, s', d) = r + \gamma (1 - d) \left( \min_{j=1,2} Q_{\phi_{\text{targ},j}}(s', \tilde{a}') - \alpha \log \pi_{\theta}(\tilde{a}'|s') \right), \;\;\;\;\; \tilde{a}' \sim \pi_{\theta}(\cdot|s').\end{split}
\end{equation*}
\sphinxstylestrong{Learning the Policy.} The policy should, in each state, act to maximise the expected future return plus expected future entropy. That is, it should maximise \(V^{\pi}(s)\), which we expand out into
\begin{equation*}
\begin{split}V^{\pi}(s) &= \underE{a \sim \pi}{Q^{\pi}(s,a)} + \alpha H\left(\pi(\cdot|s)\right) \\
&= \underE{a \sim \pi}{Q^{\pi}(s,a) - \alpha \log \pi(a|s)}.\end{split}
\end{equation*}
The way we optimise the policy makes use of the \sphinxstylestrong{reparameterization trick}, in which a sample from \(\pi_{\theta}(\cdot|s)\) is drawn by computing a deterministic function of state, policy parameters, and independent noise. To illustrate: following the authors of the SAC paper, we use a squashed Gaussian policy, which means that samples are obtained according to
\begin{equation*}
\begin{split}\tilde{a}_{\theta}(s, \xi) = \tanh\left( \mu_{\theta}(s) + \sigma_{\theta}(s) \odot \xi \right), \;\;\;\;\; \xi \sim \mathcal{N}(0, I).\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{You Should Know}

This policy has two key differences from the policies we use in the other policy optimization algorithms:

\sphinxstylestrong{1. The squashing function.} The \(\tanh\) in the SAC policy ensures that actions are bounded to a finite range. This is absent in the VPG, TRPO, and PPO policies. It also changes the distribution: before the \(\tanh\) the SAC policy is a factored Gaussian like the other algorithms’ policies, but after the \(\tanh\) it is not. (You can still compute the log-probabilities of actions in closed form, though: see the paper appendix for details.)

\sphinxstylestrong{2. The way standard deviations are parameterized.} In VPG, TRPO, and PPO, we represent the log std devs with state-independent parameter vectors. In SAC, we represent the log std devs as outputs from the neural network, meaning that they depend on state in a complex way. SAC with state-independent log std devs, in our experience, did not work. (Can you think of why? Or better yet: run an experiment to verify?)
\end{sphinxadmonition}

The reparameterization trick allows us to rewrite the expectation over actions (which contains a pain point: the distribution depends on the policy parameters) into an expectation over noise (which removes the pain point: the distribution now has no dependence on parameters):
\begin{equation*}
\begin{split}\underE{a \sim \pi_{\theta}}{Q^{\pi_{\theta}}(s,a) - \alpha \log \pi_{\theta}(a|s)} = \underE{\xi \sim \mathcal{N}}{Q^{\pi_{\theta}}(s,\tilde{a}_{\theta}(s,\xi)) - \alpha \log \pi_{\theta}(\tilde{a}_{\theta}(s,\xi)|s)}\end{split}
\end{equation*}
To get the policy loss, the final step is that we need to substitute \(Q^{\pi_{\theta}}\) with one of our function approximators. Unlike in TD3, which uses \(Q_{\phi_1}\) (just the first Q approximator), SAC uses \(\min_{j=1,2} Q_{\phi_j}\) (the minimum of the two Q approximators). The policy is thus optimized according to
\begin{equation*}
\begin{split}\max_{\theta} \underE{s \sim \mathcal{D} \\ \xi \sim \mathcal{N}}{\min_{j=1,2} Q_{\phi_j}(s,\tilde{a}_{\theta}(s,\xi)) - \alpha \log \pi_{\theta}(\tilde{a}_{\theta}(s,\xi)|s)},\end{split}
\end{equation*}
which is almost the same as the DDPG and TD3 policy optimization, except for the min-double-Q trick, the stochasticity, and the entropy term.


\subsection{Exploration vs. Exploitation}
\label{\detokenize{algorithms/sac:exploration-vs-exploitation}}
SAC trains a stochastic policy with entropy regularisation, and explores in an on-policy way. The entropy regularisation coefficient \(\alpha\) explicitly controls the explore-exploit tradeoff, with higher \(\alpha\) corresponding to more exploration, and lower \(\alpha\) corresponding to more exploitation. The right coefficient (the one which leads to the stablest / highest-reward learning) may vary from environment to environment, and could require careful tuning.

At test time, to see how well the policy exploits what it has learned, we remove stochasticity and use the mean action instead of a sample from the distribution. This tends to improve performance over the original stochastic policy.

\begin{sphinxadmonition}{note}{You Should Know}

Our SAC implementation uses a trick to improve exploration at the start of training. For a fixed number of steps at the beginning (set with the \sphinxcode{\sphinxupquote{start\_steps}} keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal SAC exploration.
\end{sphinxadmonition}


\subsection{Pseudocode}
\label{\detokenize{algorithms/sac:pseudocode}}\begin{algorithm}[H]
    \caption{Soft Actor-Critic}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta$, Q-function parameters $\phi_1$, $\phi_2$, empty replay buffer $\mathcal{D}$
    \STATE Set target parameters equal to main parameters $\phi_{\text{targ},1} \leftarrow \phi_1$, $\phi_{\text{targ},2} \leftarrow \phi_2$
    \REPEAT
        \STATE Observe state $s$ and select action $a \sim \pi_{\theta}(\cdot|s)$
        \STATE Execute $a$ in the environment
        \STATE Observe next state $s'$, reward $r$, and done signal $d$ to indicate whether $s'$ is terminal
        \STATE Store $(s,a,r,s',d)$ in replay buffer $\mathcal{D}$
        \STATE If $s'$ is terminal, reset environment state.
        \IF{it's time to update}
            \FOR{$j$ in range(however many updates)}
                \STATE Randomly sample a batch of transitions, $B = \{ (s,a,r,s',d) \}$ from $\mathcal{D}$
                \STATE Compute targets for the Q functions:
                \begin{align*}
                    y (r,s',d) &= r + \gamma (1-d) \left(\min_{i=1,2} Q_{\phi_{\text{targ}, i}} (s', \tilde{a}') - \alpha \log \pi_{\theta}(\tilde{a}'|s')\right), && \tilde{a}' \sim \pi_{\theta}(\cdot|s')
                \end{align*}
                \STATE Update Q-functions by one step of gradient descent using
                \begin{align*}
                    & \nabla_{\phi_i} \frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} \left( Q_{\phi_i}(s,a) - y(r,s',d) \right)^2 && \text{for } i=1,2
                \end{align*}
                \STATE Update policy by one step of gradient ascent using
                \begin{equation*}
                    \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B} \Big(\min_{i=1,2} Q_{\phi_i}(s, \tilde{a}_{\theta}(s)) - \alpha \log \pi_{\theta} \left(\left. \tilde{a}_{\theta}(s) \right| s\right) \Big),
                \end{equation*}
                where $\tilde{a}_{\theta}(s)$ is a sample from $\pi_{\theta}(\cdot|s)$ which is differentiable wrt $\theta$ via the reparametrization trick.
                \STATE Update target networks with
                \begin{align*}
                    \phi_{\text{targ},i} &\leftarrow \rho \phi_{\text{targ}, i} + (1-\rho) \phi_i && \text{for } i=1,2
                \end{align*}
            \ENDFOR
        \ENDIF
    \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}

\section{Documentation}
\label{\detokenize{algorithms/sac:documentation}}
\begin{sphinxadmonition}{note}{You Should Know}

In what follows, we give documentation for the PyTorch and Tensorflow implementations of SAC in Spinning Up. They have nearly identical function calls and docstrings, except for details relating to model construction. However, we include both full docstrings for completeness.
\end{sphinxadmonition}


\subsection{Documentation: PyTorch Version}
\label{\detokenize{algorithms/sac:documentation-pytorch-version}}

\subsection{Saved Model Contents: PyTorch Version}
\label{\detokenize{algorithms/sac:saved-model-contents-pytorch-version}}
The PyTorch saved model can be loaded with \sphinxcode{\sphinxupquote{ac = torch.load('path/to/model.pt')}}, yielding an actor-critic object (\sphinxcode{\sphinxupquote{ac}}) that has the properties described in the docstring for \sphinxcode{\sphinxupquote{sac\_pytorch}}.

You can get actions from this model with

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{actions} \PYG{o}{=} \PYG{n}{ac}\PYG{o}{.}\PYG{n}{act}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{as\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{obs}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Documentation: Tensorflow Version}
\label{\detokenize{algorithms/sac:documentation-tensorflow-version}}

\subsection{Saved Model Contents: Tensorflow Version}
\label{\detokenize{algorithms/sac:saved-model-contents-tensorflow-version}}
The computation graph saved by the logger includes:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
Key
&\sphinxstyletheadfamily 
Value
\\
\hline
\sphinxcode{\sphinxupquote{x}}
&
Tensorflow placeholder for state input.
\\
\hline
\sphinxcode{\sphinxupquote{a}}
&
Tensorflow placeholder for action input.
\\
\hline
\sphinxcode{\sphinxupquote{mu}}
&
Deterministically computes mean action from the agent, given states in \sphinxcode{\sphinxupquote{x}}.
\\
\hline
\sphinxcode{\sphinxupquote{pi}}
&
Samples an action from the agent, conditioned on states in \sphinxcode{\sphinxupquote{x}}.
\\
\hline
\sphinxcode{\sphinxupquote{q1}}
&
Gives one action-value estimate for states in \sphinxcode{\sphinxupquote{x}} and actions in \sphinxcode{\sphinxupquote{a}}.
\\
\hline
\sphinxcode{\sphinxupquote{q2}}
&
Gives the other action-value estimate for states in \sphinxcode{\sphinxupquote{x}} and actions in \sphinxcode{\sphinxupquote{a}}.
\\
\hline
\sphinxcode{\sphinxupquote{v}}
&
Gives the value estimate for states in \sphinxcode{\sphinxupquote{x}}.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

This saved model can be accessed either by
\begin{itemize}
\item {} 
running the trained policy with the \sphinxhref{../user/saving\_and\_loading.html\#loading-and-running-trained-policies}{test\_policy.py} tool,

\item {} 
or loading the whole saved graph into a program with \sphinxhref{../utils/logger.html\#spinup.utils.logx.restore\_tf\_graph}{restore\_tf\_graph}.

\end{itemize}

Note: for SAC, the correct evaluation policy is given by \sphinxcode{\sphinxupquote{mu}} and not by \sphinxcode{\sphinxupquote{pi}}. The policy \sphinxcode{\sphinxupquote{pi}} may be thought of as the exploration policy, while \sphinxcode{\sphinxupquote{mu}} is the exploitation policy.


\section{References}
\label{\detokenize{algorithms/sac:references}}

\subsection{Relevant Papers}
\label{\detokenize{algorithms/sac:relevant-papers}}\begin{itemize}
\item {} 
\sphinxhref{https://arxiv.org/abs/1801.01290}{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, Haarnoja et al, 2018

\item {} 
\sphinxhref{https://arxiv.org/abs/1812.05905}{Soft Actor-Critic Algorithms and Applications}, Haarnoja et al, 2018

\item {} 
\sphinxhref{https://arxiv.org/abs/1812.11103}{Learning to Walk via Deep Reinforcement Learning}, Haarnoja et al, 2018

\end{itemize}


\subsection{Other Public Implementations}
\label{\detokenize{algorithms/sac:other-public-implementations}}\begin{itemize}
\item {} 
\sphinxhref{https://github.com/haarnoja/sac}{SAC release repo} (original “official” codebase)

\item {} 
\sphinxhref{https://github.com/rail-berkeley/softlearning}{Softlearning repo} (current “official” codebase)

\item {} 
\sphinxhref{https://github.com/denisyarats/pytorch\_sac}{Yarats and Kostrikov repo}

\end{itemize}


\chapter{Lyapunov Actor-Critic}
\label{\detokenize{algorithms/lac:lyapunov-actor-critic}}\label{\detokenize{algorithms/lac::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id1}}{\hyperref[\detokenize{algorithms/lac:lyapunov-actor-critic}]{\sphinxcrossref{Lyapunov Actor-Critic}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id2}}{\hyperref[\detokenize{algorithms/lac:background}]{\sphinxcrossref{Background}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id3}}{\hyperref[\detokenize{algorithms/lac:quick-facts}]{\sphinxcrossref{Quick Facts}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id4}}{\hyperref[\detokenize{algorithms/lac:key-equations}]{\sphinxcrossref{Key Equations}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id5}}{\hyperref[\detokenize{algorithms/lac:entropy-regularized-reinforcement-learning}]{\sphinxcrossref{Entropy-Regularized Reinforcement Learning}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id6}}{\hyperref[\detokenize{algorithms/lac:soft-actor-critic}]{\sphinxcrossref{Soft Actor-Critic}}}

\end{itemize}

\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id7}}{\hyperref[\detokenize{algorithms/lac:exploration-vs-exploitation}]{\sphinxcrossref{Exploration vs. Exploitation}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id8}}{\hyperref[\detokenize{algorithms/lac:pseudocode}]{\sphinxcrossref{Pseudocode}}}

\end{itemize}

\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id9}}{\hyperref[\detokenize{algorithms/lac:documentation}]{\sphinxcrossref{Documentation}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id10}}{\hyperref[\detokenize{algorithms/lac:documentation-pytorch-version}]{\sphinxcrossref{Documentation: PyTorch Version}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id11}}{\hyperref[\detokenize{algorithms/lac:saved-model-contents-pytorch-version}]{\sphinxcrossref{Saved Model Contents: PyTorch Version}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id12}}{\hyperref[\detokenize{algorithms/lac:documentation-tensorflow-version}]{\sphinxcrossref{Documentation: Tensorflow Version}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id13}}{\hyperref[\detokenize{algorithms/lac:saved-model-contents-tensorflow-version}]{\sphinxcrossref{Saved Model Contents: Tensorflow Version}}}

\end{itemize}

\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id14}}{\hyperref[\detokenize{algorithms/lac:references}]{\sphinxcrossref{References}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id15}}{\hyperref[\detokenize{algorithms/lac:relevant-papers}]{\sphinxcrossref{Relevant Papers}}}

\item {} 
\phantomsection\label{\detokenize{algorithms/lac:id16}}{\hyperref[\detokenize{algorithms/lac:other-public-implementations}]{\sphinxcrossref{Other Public Implementations}}}

\end{itemize}

\end{itemize}

\end{itemize}
\end{sphinxShadowBox}


\section{Background}
\label{\detokenize{algorithms/lac:background}}
(Previously: \sphinxhref{../algorithms/td3.html\#background}{Background for TD3})

Soft Actor Critic (SAC) is an algorithm that optimizes a stochastic policy in an off-policy way, forming a bridge between stochastic policy optimization and DDPG-style approaches. It isn’t a direct successor to TD3 (having been published roughly concurrently), but it incorporates the clipped double-Q trick, and due to the inherent stochasticity of the policy in SAC, it also winds up benefiting from something like target policy smoothing.

A central feature of SAC is \sphinxstylestrong{entropy regularisation.} The policy is trained to maximise a trade-off between expected return and \sphinxhref{https://en.wikipedia.org/wiki/Entropy\_(information\_theory)}{entropy}, a measure of randomness in the policy. This has a close connection to the exploration-exploitation trade-off: increasing entropy results in more exploration, which can accelerate learning later on. It can also prevent the policy from prematurely converging to a bad local optimum.


\subsection{Quick Facts}
\label{\detokenize{algorithms/lac:quick-facts}}\begin{itemize}
\item {} 
SAC is an off-policy algorithm.

\item {} 
The version of SAC implemented here can only be used for environments with continuous action spaces.

\item {} 
An alternate version of SAC, which slightly changes the policy update rule, can be implemented to handle discrete action spaces.

\item {} 
The Spinning Up implementation of SAC does not support parallelization.

\end{itemize}


\subsection{Key Equations}
\label{\detokenize{algorithms/lac:key-equations}}
To explain Soft Actor Critic, we first have to introduce the entropy-regularised reinforcement learning setting. In entropy-regularised RL, there are slightly-different equations for value functions.


\subsubsection{Entropy-Regularized Reinforcement Learning}
\label{\detokenize{algorithms/lac:entropy-regularized-reinforcement-learning}}
Entropy is a quantity which, roughly speaking, says how random a random variable is. If a coin is weighted so that it almost always comes up heads, it has low entropy; if it’s evenly weighted and has a half chance of either outcome, it has high entropy.

Let \(x\) be a random variable with probability mass or density function \(P\). The entropy \(H\) of \(x\) is computed from its distribution \(P\) according to
\begin{equation*}
\begin{split}H(P) = \underE{x \sim P}{-\log P(x)}.\end{split}
\end{equation*}
In entropy-regularised reinforcement learning, the agent gets a bonus reward at each time step proportional to the entropy of the policy at that timestep. This changes \sphinxhref{../spinningup/rl\_intro.html\#the-rl-problem}{the RL problem} to:
\begin{equation*}
\begin{split}\pi^* = \arg \max_{\pi} \underE{\tau \sim \pi}{ \sum_{t=0}^{\infty} \gamma^t \bigg( R(s_t, a_t, s_{t+1}) + \alpha H\left(\pi(\cdot|s_t)\right) \bigg)},\end{split}
\end{equation*}
where \(\alpha > 0\) is the trade-off coefficient. (Note: we’re assuming an infinite-horizon discounted setting here, and we’ll do the same for the rest of this page.) We can now define the slightly-different value functions in this setting. \(V^{\pi}\) is changed to include the entropy bonuses from every timestep:
\begin{equation*}
\begin{split}V^{\pi}(s) = \underE{\tau \sim \pi}{ \left. \sum_{t=0}^{\infty} \gamma^t \bigg( R(s_t, a_t, s_{t+1}) + \alpha H\left(\pi(\cdot|s_t)\right) \bigg) \right| s_0 = s}\end{split}
\end{equation*}
\(Q^{\pi}\) is changed to include the entropy bonuses from every timestep \sphinxstyleemphasis{except the first}:
\begin{equation*}
\begin{split}Q^{\pi}(s,a) = \underE{\tau \sim \pi}{ \left. \sum_{t=0}^{\infty} \gamma^t  R(s_t, a_t, s_{t+1}) + \alpha \sum_{t=1}^{\infty} \gamma^t H\left(\pi(\cdot|s_t)\right)\right| s_0 = s, a_0 = a}\end{split}
\end{equation*}
With these definitions, \(V^{\pi}\) and \(Q^{\pi}\) are connected by:
\begin{equation*}
\begin{split}V^{\pi}(s) = \underE{a \sim \pi}{Q^{\pi}(s,a)} + \alpha H\left(\pi(\cdot|s)\right)\end{split}
\end{equation*}
and the Bellman equation for \(Q^{\pi}\) is
\begin{equation*}
\begin{split}Q^{\pi}(s,a) &= \underE{s' \sim P \\ a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') + \alpha H\left(\pi(\cdot|s')\right) \right)} \\
&= \underE{s' \sim P}{R(s,a,s') + \gamma V^{\pi}(s')}.\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{You Should Know}

The way we’ve set up the value functions in the entropy-regularised setting is a little bit arbitrary, and actually we could have done it differently (eg make \(Q^{\pi}\) include the entropy bonus at the first timestep). The choice of definition may vary slightly across papers on the subject.
\end{sphinxadmonition}


\subsubsection{Soft Actor-Critic}
\label{\detokenize{algorithms/lac:soft-actor-critic}}
SAC concurrently learns a policy \(\pi_{\theta}\) and two Q-functions \(Q_{\phi_1}, Q_{\phi_2}\). There are two variants of SAC that are currently standard: one that uses a fixed entropy regularisation coefficient \(\alpha\), and another that enforces an entropy constraint by varying \(\alpha\) over the course of training. For simplicity, Spinning Up makes use of the version with a fixed entropy regularisation coefficient, but the entropy-constrained variant is generally preferred by practitioners.

\begin{sphinxadmonition}{note}{You Should Know}

The SAC algorithm has changed a little bit over time. An older version of SAC also learns a value function \(V_{\psi}\) in addition to the Q-functions; this page will focus on the modern version that omits the extra value function.
\end{sphinxadmonition}

\sphinxstylestrong{Learning Q.} The Q-functions are learned in a similar way to TD3, but with a few key differences.

First, what’s similar?
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi )}
\makeatletter\def\p@enumii{\p@enumi \theenumi )}\makeatother
\item {} 
Like in TD3, both Q-functions are learned with MSBE minimization, by regressing to a single shared target.

\item {} 
Like in TD3, the shared target is computed using target Q-networks, and the target Q-networks are obtained by polyak averaging the Q-network parameters over the course of training.

\item {} 
Like in TD3, the shared target makes use of the \sphinxstylestrong{clipped double-Q} trick.

\end{enumerate}

What’s different?
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi )}
\makeatletter\def\p@enumii{\p@enumi \theenumi )}\makeatother
\item {} 
Unlike in TD3, the target also includes a term that comes from SAC’s use of entropy regularisation.

\item {} 
Unlike in TD3, the next-state actions used in the target come from the \sphinxstylestrong{current policy} instead of a target policy.

\item {} 
Unlike in TD3, there is no explicit target policy smoothing. TD3 trains a deterministic policy, and so it accomplishes smoothing by adding random noise to the next-state actions. SAC trains a stochastic policy, and so the noise from that stochasticity is sufficient to get a similar effect.

\end{enumerate}

Before we give the final form of the Q-loss, let’s take a moment to discuss how the contribution from entropy regularisation comes in. We’ll start by taking our recursive Bellman equation for the entropy-regularised \(Q^{\pi}\) from earlier, and rewriting it a little bit by using the definition of entropy:
\begin{equation*}
\begin{split}Q^{\pi}(s,a) &= \underE{s' \sim P \\ a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') + \alpha H\left(\pi(\cdot|s')\right) \right)} \\
&= \underE{s' \sim P \\ a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') - \alpha \log \pi(a'|s') \right)}\end{split}
\end{equation*}
The RHS is an expectation over next states (which come from the replay buffer) and next actions (which come from the current policy, and \sphinxstylestrong{not} the replay buffer). Since it’s an expectation, we can approximate it with samples:
\begin{equation*}
\begin{split}Q^{\pi}(s,a) &\approx r + \gamma\left(Q^{\pi}(s',\tilde{a}') - \alpha \log \pi(\tilde{a}'|s') \right), \;\;\;\;\;  \tilde{a}' \sim \pi(\cdot|s').\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{You Should Know}

We switch next action notation to \(\tilde{a}'\), instead of \(a'\), to highlight that the next actions have to be sampled fresh from the policy (whereas by contrast, \(r\) and \(s'\) should come from the replay buffer).
\end{sphinxadmonition}

SAC sets up the MSBE loss for each Q-function using this kind of sample approximation for the target. The only thing still undetermined here is which Q-function gets used to compute the sample backup: like TD3, SAC uses the clipped double-Q trick, and takes the minimum Q-value between the two Q approximators.

Putting it all together, the loss functions for the Q-networks in SAC are:
\begin{equation*}
\begin{split}L(\phi_i, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi_i}(s,a) - y(r,s',d) \Bigg)^2
    \right],\end{split}
\end{equation*}
where the target is given by
\begin{equation*}
\begin{split}y(r, s', d) = r + \gamma (1 - d) \left( \min_{j=1,2} Q_{\phi_{\text{targ},j}}(s', \tilde{a}') - \alpha \log \pi_{\theta}(\tilde{a}'|s') \right), \;\;\;\;\; \tilde{a}' \sim \pi_{\theta}(\cdot|s').\end{split}
\end{equation*}
\sphinxstylestrong{Learning the Policy.} The policy should, in each state, act to maximise the expected future return plus expected future entropy. That is, it should maximise \(V^{\pi}(s)\), which we expand out into
\begin{equation*}
\begin{split}V^{\pi}(s) &= \underE{a \sim \pi}{Q^{\pi}(s,a)} + \alpha H\left(\pi(\cdot|s)\right) \\
&= \underE{a \sim \pi}{Q^{\pi}(s,a) - \alpha \log \pi(a|s)}.\end{split}
\end{equation*}
The way we optimise the policy makes use of the \sphinxstylestrong{reparameterization trick}, in which a sample from \(\pi_{\theta}(\cdot|s)\) is drawn by computing a deterministic function of state, policy parameters, and independent noise. To illustrate: following the authors of the SAC paper, we use a squashed Gaussian policy, which means that samples are obtained according to
\begin{equation*}
\begin{split}\tilde{a}_{\theta}(s, \xi) = \tanh\left( \mu_{\theta}(s) + \sigma_{\theta}(s) \odot \xi \right), \;\;\;\;\; \xi \sim \mathcal{N}(0, I).\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{You Should Know}

This policy has two key differences from the policies we use in the other policy optimization algorithms:

\sphinxstylestrong{1. The squashing function.} The \(\tanh\) in the SAC policy ensures that actions are bounded to a finite range. This is absent in the VPG, TRPO, and PPO policies. It also changes the distribution: before the \(\tanh\) the SAC policy is a factored Gaussian like the other algorithms’ policies, but after the \(\tanh\) it is not. (You can still compute the log-probabilities of actions in closed form, though: see the paper appendix for details.)

\sphinxstylestrong{2. The way standard deviations are parameterized.} In VPG, TRPO, and PPO, we represent the log std devs with state-independent parameter vectors. In SAC, we represent the log std devs as outputs from the neural network, meaning that they depend on state in a complex way. SAC with state-independent log std devs, in our experience, did not work. (Can you think of why? Or better yet: run an experiment to verify?)
\end{sphinxadmonition}

The reparameterization trick allows us to rewrite the expectation over actions (which contains a pain point: the distribution depends on the policy parameters) into an expectation over noise (which removes the pain point: the distribution now has no dependence on parameters):
\begin{equation*}
\begin{split}\underE{a \sim \pi_{\theta}}{Q^{\pi_{\theta}}(s,a) - \alpha \log \pi_{\theta}(a|s)} = \underE{\xi \sim \mathcal{N}}{Q^{\pi_{\theta}}(s,\tilde{a}_{\theta}(s,\xi)) - \alpha \log \pi_{\theta}(\tilde{a}_{\theta}(s,\xi)|s)}\end{split}
\end{equation*}
To get the policy loss, the final step is that we need to substitute \(Q^{\pi_{\theta}}\) with one of our function approximators. Unlike in TD3, which uses \(Q_{\phi_1}\) (just the first Q approximator), SAC uses \(\min_{j=1,2} Q_{\phi_j}\) (the minimum of the two Q approximators). The policy is thus optimized according to
\begin{equation*}
\begin{split}\max_{\theta} \underE{s \sim \mathcal{D} \\ \xi \sim \mathcal{N}}{\min_{j=1,2} Q_{\phi_j}(s,\tilde{a}_{\theta}(s,\xi)) - \alpha \log \pi_{\theta}(\tilde{a}_{\theta}(s,\xi)|s)},\end{split}
\end{equation*}
which is almost the same as the DDPG and TD3 policy optimization, except for the min-double-Q trick, the stochasticity, and the entropy term.


\subsection{Exploration vs. Exploitation}
\label{\detokenize{algorithms/lac:exploration-vs-exploitation}}
SAC trains a stochastic policy with entropy regularisation, and explores in an on-policy way. The entropy regularisation coefficient \(\alpha\) explicitly controls the explore-exploit tradeoff, with higher \(\alpha\) corresponding to more exploration, and lower \(\alpha\) corresponding to more exploitation. The right coefficient (the one which leads to the stablest / highest-reward learning) may vary from environment to environment, and could require careful tuning.

At test time, to see how well the policy exploits what it has learned, we remove stochasticity and use the mean action instead of a sample from the distribution. This tends to improve performance over the original stochastic policy.

\begin{sphinxadmonition}{note}{You Should Know}

Our SAC implementation uses a trick to improve exploration at the start of training. For a fixed number of steps at the beginning (set with the \sphinxcode{\sphinxupquote{start\_steps}} keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal SAC exploration.
\end{sphinxadmonition}


\subsection{Pseudocode}
\label{\detokenize{algorithms/lac:pseudocode}}\begin{algorithm}[H]
    \caption{Soft Actor-Critic}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta$, Q-function parameters $\phi_1$, $\phi_2$, empty replay buffer $\mathcal{D}$
    \STATE Set target parameters equal to main parameters $\phi_{\text{targ},1} \leftarrow \phi_1$, $\phi_{\text{targ},2} \leftarrow \phi_2$
    \REPEAT
        \STATE Observe state $s$ and select action $a \sim \pi_{\theta}(\cdot|s)$
        \STATE Execute $a$ in the environment
        \STATE Observe next state $s'$, reward $r$, and done signal $d$ to indicate whether $s'$ is terminal
        \STATE Store $(s,a,r,s',d)$ in replay buffer $\mathcal{D}$
        \STATE If $s'$ is terminal, reset environment state.
        \IF{it's time to update}
            \FOR{$j$ in range(however many updates)}
                \STATE Randomly sample a batch of transitions, $B = \{ (s,a,r,s',d) \}$ from $\mathcal{D}$
                \STATE Compute targets for the Q functions:
                \begin{align*}
                    y (r,s',d) &= r + \gamma (1-d) \left(\min_{i=1,2} Q_{\phi_{\text{targ}, i}} (s', \tilde{a}') - \alpha \log \pi_{\theta}(\tilde{a}'|s')\right), && \tilde{a}' \sim \pi_{\theta}(\cdot|s')
                \end{align*}
                \STATE Update Q-functions by one step of gradient descent using
                \begin{align*}
                    & \nabla_{\phi_i} \frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} \left( Q_{\phi_i}(s,a) - y(r,s',d) \right)^2 && \text{for } i=1,2
                \end{align*}
                \STATE Update policy by one step of gradient ascent using
                \begin{equation*}
                    \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B} \Big(\min_{i=1,2} Q_{\phi_i}(s, \tilde{a}_{\theta}(s)) - \alpha \log \pi_{\theta} \left(\left. \tilde{a}_{\theta}(s) \right| s\right) \Big),
                \end{equation*}
                where $\tilde{a}_{\theta}(s)$ is a sample from $\pi_{\theta}(\cdot|s)$ which is differentiable wrt $\theta$ via the reparametrization trick.
                \STATE Update target networks with
                \begin{align*}
                    \phi_{\text{targ},i} &\leftarrow \rho \phi_{\text{targ}, i} + (1-\rho) \phi_i && \text{for } i=1,2
                \end{align*}
            \ENDFOR
        \ENDIF
    \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}

\section{Documentation}
\label{\detokenize{algorithms/lac:documentation}}
\begin{sphinxadmonition}{note}{You Should Know}

In what follows, we give documentation for the PyTorch and Tensorflow implementations of SAC in Spinning Up. They have nearly identical function calls and docstrings, except for details relating to model construction. However, we include both full docstrings for completeness.
\end{sphinxadmonition}


\subsection{Documentation: PyTorch Version}
\label{\detokenize{algorithms/lac:documentation-pytorch-version}}

\subsection{Saved Model Contents: PyTorch Version}
\label{\detokenize{algorithms/lac:saved-model-contents-pytorch-version}}
The PyTorch saved model can be loaded with \sphinxcode{\sphinxupquote{ac = torch.load('path/to/model.pt')}}, yielding an actor-critic object (\sphinxcode{\sphinxupquote{ac}}) that has the properties described in the docstring for \sphinxcode{\sphinxupquote{sac\_pytorch}}.

You can get actions from this model with

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{actions} \PYG{o}{=} \PYG{n}{ac}\PYG{o}{.}\PYG{n}{act}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{as\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{obs}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Documentation: Tensorflow Version}
\label{\detokenize{algorithms/lac:documentation-tensorflow-version}}

\subsection{Saved Model Contents: Tensorflow Version}
\label{\detokenize{algorithms/lac:saved-model-contents-tensorflow-version}}
The computation graph saved by the logger includes:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
Key
&\sphinxstyletheadfamily 
Value
\\
\hline
\sphinxcode{\sphinxupquote{x}}
&
Tensorflow placeholder for state input.
\\
\hline
\sphinxcode{\sphinxupquote{a}}
&
Tensorflow placeholder for action input.
\\
\hline
\sphinxcode{\sphinxupquote{mu}}
&
Deterministically computes mean action from the agent, given states in \sphinxcode{\sphinxupquote{x}}.
\\
\hline
\sphinxcode{\sphinxupquote{pi}}
&
Samples an action from the agent, conditioned on states in \sphinxcode{\sphinxupquote{x}}.
\\
\hline
\sphinxcode{\sphinxupquote{q1}}
&
Gives one action-value estimate for states in \sphinxcode{\sphinxupquote{x}} and actions in \sphinxcode{\sphinxupquote{a}}.
\\
\hline
\sphinxcode{\sphinxupquote{q2}}
&
Gives the other action-value estimate for states in \sphinxcode{\sphinxupquote{x}} and actions in \sphinxcode{\sphinxupquote{a}}.
\\
\hline
\sphinxcode{\sphinxupquote{v}}
&
Gives the value estimate for states in \sphinxcode{\sphinxupquote{x}}.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

This saved model can be accessed either by
\begin{itemize}
\item {} 
running the trained policy with the \sphinxhref{../user/saving\_and\_loading.html\#loading-and-running-trained-policies}{test\_policy.py} tool,

\item {} 
or loading the whole saved graph into a program with \sphinxhref{../utils/logger.html\#spinup.utils.logx.restore\_tf\_graph}{restore\_tf\_graph}.

\end{itemize}

Note: for SAC, the correct evaluation policy is given by \sphinxcode{\sphinxupquote{mu}} and not by \sphinxcode{\sphinxupquote{pi}}. The policy \sphinxcode{\sphinxupquote{pi}} may be thought of as the exploration policy, while \sphinxcode{\sphinxupquote{mu}} is the exploitation policy.


\section{References}
\label{\detokenize{algorithms/lac:references}}

\subsection{Relevant Papers}
\label{\detokenize{algorithms/lac:relevant-papers}}\begin{itemize}
\item {} 
\sphinxhref{https://arxiv.org/abs/1801.01290}{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, Haarnoja et al, 2018

\item {} 
\sphinxhref{https://arxiv.org/abs/1812.05905}{Soft Actor-Critic Algorithms and Applications}, Haarnoja et al, 2018

\item {} 
\sphinxhref{https://arxiv.org/abs/1812.11103}{Learning to Walk via Deep Reinforcement Learning}, Haarnoja et al, 2018

\end{itemize}


\subsection{Other Public Implementations}
\label{\detokenize{algorithms/lac:other-public-implementations}}\begin{itemize}
\item {} 
\sphinxhref{https://github.com/haarnoja/sac}{SAC release repo} (original “official” codebase)

\item {} 
\sphinxhref{https://github.com/rail-berkeley/softlearning}{Softlearning repo} (current “official” codebase)

\item {} 
\sphinxhref{https://github.com/denisyarats/pytorch\_sac}{Yarats and Kostrikov repo}

\end{itemize}


\chapter{Logger}
\label{\detokenize{utils/logger:logger}}\label{\detokenize{utils/logger::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{utils/logger:id2}}{\hyperref[\detokenize{utils/logger:logger}]{\sphinxcrossref{Logger}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{utils/logger:id3}}{\hyperref[\detokenize{utils/logger:using-a-logger}]{\sphinxcrossref{Using a Logger}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{utils/logger:id4}}{\hyperref[\detokenize{utils/logger:examples}]{\sphinxcrossref{Examples}}}

\item {} 
\phantomsection\label{\detokenize{utils/logger:id5}}{\hyperref[\detokenize{utils/logger:logging-and-pytorch}]{\sphinxcrossref{Logging and PyTorch}}}

\item {} 
\phantomsection\label{\detokenize{utils/logger:id6}}{\hyperref[\detokenize{utils/logger:logging-and-mpi}]{\sphinxcrossref{Logging and MPI}}}

\end{itemize}

\item {} 
\phantomsection\label{\detokenize{utils/logger:id7}}{\hyperref[\detokenize{utils/logger:logger-classes}]{\sphinxcrossref{Logger Classes}}}

\item {} 
\phantomsection\label{\detokenize{utils/logger:id8}}{\hyperref[\detokenize{utils/logger:loading-saved-models-pytorch-only}]{\sphinxcrossref{Loading Saved Models (PyTorch Only)}}}

\item {} 
\phantomsection\label{\detokenize{utils/logger:id9}}{\hyperref[\detokenize{utils/logger:loading-saved-graphs-tensorflow-only}]{\sphinxcrossref{Loading Saved Graphs (Tensorflow Only)}}}

\end{itemize}

\end{itemize}
\end{sphinxShadowBox}


\section{Using a Logger}
\label{\detokenize{utils/logger:using-a-logger}}
Spinning Up ships with basic logging tools, implemented in the classes \sphinxhref{../utils/logger.html\#spinup.utils.logx.Logger}{Logger} and \sphinxhref{../utils/logger.html\#spinup.utils.logx.EpochLogger}{EpochLogger}. The Logger class contains most of the basic functionality for saving diagnostics, hyperparameter configurations, the state of a training run, and the trained model. The EpochLogger class adds a thin layer on top of that to make it easy to track the average, standard deviation, min, and max value of a diagnostic over each epoch and across MPI workers.

\begin{sphinxadmonition}{note}{You Should Know}

All Spinning Up algorithm implementations use an EpochLogger.
\end{sphinxadmonition}


\subsection{Examples}
\label{\detokenize{utils/logger:examples}}
First, let’s look at a simple example of how an EpochLogger keeps track of a diagnostic value:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{spinup}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{logx} \PYG{k+kn}{import} \PYG{n}{EpochLogger}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{epoch\PYGZus{}logger} \PYG{o}{=} \PYG{n}{EpochLogger}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
\PYG{g+go}{        epoch\PYGZus{}logger.store(Test=i)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{epoch\PYGZus{}logger}\PYG{o}{.}\PYG{n}{log\PYGZus{}tabular}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{with\PYGZus{}min\PYGZus{}and\PYGZus{}max}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{epoch\PYGZus{}logger}\PYG{o}{.}\PYG{n}{dump\PYGZus{}tabular}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{g+go}{\textbar{}     AverageTest \textbar{}             4.5 \textbar{}}
\PYG{g+go}{\textbar{}         StdTest \textbar{}            2.87 \textbar{}}
\PYG{g+go}{\textbar{}         MaxTest \textbar{}               9 \textbar{}}
\PYG{g+go}{\textbar{}         MinTest \textbar{}               0 \textbar{}}
\PYG{g+go}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\end{sphinxVerbatim}

The \sphinxcode{\sphinxupquote{store}} method is used to save all values of \sphinxcode{\sphinxupquote{Test}} to the \sphinxcode{\sphinxupquote{epoch\_logger}}’s internal state. Then, when \sphinxcode{\sphinxupquote{log\_tabular}} is called, it computes the average, standard deviation, min, and max of \sphinxcode{\sphinxupquote{Test}} over all of the values in the internal state. The internal state is wiped clean after the call to \sphinxcode{\sphinxupquote{log\_tabular}} (to prevent leakage into the statistics at the next epoch). Finally, \sphinxcode{\sphinxupquote{dump\_tabular}} is called to write the diagnostics to file and to stdout.

Next, let’s look at a full training procedure with the logger embedded, to highlight configuration and model saving as well as diagnostic logging:

\fvset{hllines={, 18, 19, 42, 43, 54, 58, 61, 62, 63, 64, 65, 66,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
 \PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
 \PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k}{as} \PYG{n+nn}{tf}
 \PYG{k+kn}{import} \PYG{n+nn}{time}
 \PYG{k+kn}{from} \PYG{n+nn}{spinup}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{logx} \PYG{k+kn}{import} \PYG{n}{EpochLogger}


 \PYG{k}{def} \PYG{n+nf}{mlp}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}sizes}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{32}\PYG{p}{,}\PYG{p}{)}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{tanh}\PYG{p}{,} \PYG{n}{output\PYGZus{}activation}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
     \PYG{k}{for} \PYG{n}{h} \PYG{o+ow}{in} \PYG{n}{hidden\PYGZus{}sizes}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}
         \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{dense}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{units}\PYG{o}{=}\PYG{n}{h}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{n}{activation}\PYG{p}{)}
     \PYG{k}{return} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{dense}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{units}\PYG{o}{=}\PYG{n}{hidden\PYGZus{}sizes}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{n}{output\PYGZus{}activation}\PYG{p}{)}


 \PYG{c+c1}{\PYGZsh{} Simple script for training an MLP on MNIST.}
 \PYG{k}{def} \PYG{n+nf}{train\PYGZus{}mnist}\PYG{p}{(}\PYG{n}{steps\PYGZus{}per\PYGZus{}epoch}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
                 \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,} \PYG{n}{layers}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{64}\PYG{p}{,}
                 \PYG{n}{logger\PYGZus{}kwargs}\PYG{o}{=}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{save\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}

     \PYG{n}{logger} \PYG{o}{=} \PYG{n}{EpochLogger}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{logger\PYGZus{}kwargs}\PYG{p}{)}
     \PYG{n}{logger}\PYG{o}{.}\PYG{n}{save\PYGZus{}config}\PYG{p}{(}\PYG{n+nb}{locals}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

     \PYG{c+c1}{\PYGZsh{} Load and preprocess MNIST data}
     \PYG{p}{(}\PYG{n}{x\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{datasets}\PYG{o}{.}\PYG{n}{mnist}\PYG{o}{.}\PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
     \PYG{n}{x\PYGZus{}train} \PYG{o}{=} \PYG{n}{x\PYGZus{}train}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{o}{*}\PYG{l+m+mi}{28}\PYG{p}{)} \PYG{o}{/} \PYG{l+m+mf}{255.0}

     \PYG{c+c1}{\PYGZsh{} Define inputs \PYGZam{} main outputs from computation graph}
     \PYG{n}{x\PYGZus{}ph} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{(}\PYG{k+kc}{None}\PYG{p}{,} \PYG{l+m+mi}{28}\PYG{o}{*}\PYG{l+m+mi}{28}\PYG{p}{)}\PYG{p}{)}
     \PYG{n}{y\PYGZus{}ph} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{int32}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{(}\PYG{k+kc}{None}\PYG{p}{,}\PYG{p}{)}\PYG{p}{)}
     \PYG{n}{logits} \PYG{o}{=} \PYG{n}{mlp}\PYG{p}{(}\PYG{n}{x\PYGZus{}ph}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}sizes}\PYG{o}{=}\PYG{p}{[}\PYG{n}{hidden\PYGZus{}size}\PYG{p}{]}\PYG{o}{*}\PYG{n}{layers} \PYG{o}{+} \PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{relu}\PYG{p}{)}
     \PYG{n}{predict} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{logits}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{output\PYGZus{}type}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{int32}\PYG{p}{)}

     \PYG{c+c1}{\PYGZsh{} Define loss function, accuracy, and training op}
     \PYG{n}{y} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{one\PYGZus{}hot}\PYG{p}{(}\PYG{n}{y\PYGZus{}ph}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}
     \PYG{n}{loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{losses}\PYG{o}{.}\PYG{n}{softmax\PYGZus{}cross\PYGZus{}entropy}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{logits}\PYG{p}{)}
     \PYG{n}{acc} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{cast}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{n}{y\PYGZus{}ph}\PYG{p}{,} \PYG{n}{predict}\PYG{p}{)}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{)}
     \PYG{n}{train\PYGZus{}op} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{AdamOptimizer}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{minimise}\PYG{p}{(}\PYG{n}{loss}\PYG{p}{)}

     \PYG{c+c1}{\PYGZsh{} Prepare session}
     \PYG{n}{sess} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Session}\PYG{p}{(}\PYG{p}{)}
     \PYG{n}{sess}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{global\PYGZus{}variables\PYGZus{}initializer}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

     \PYG{c+c1}{\PYGZsh{} Setup model saving}
     \PYG{n}{logger}\PYG{o}{.}\PYG{n}{setup\PYGZus{}tf\PYGZus{}saver}\PYG{p}{(}\PYG{n}{sess}\PYG{p}{,} \PYG{n}{inputs}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{x\PYGZus{}ph}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                                 \PYG{n}{outputs}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{logits}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{logits}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{predict}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{predict}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

     \PYG{n}{start\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}

     \PYG{c+c1}{\PYGZsh{} Run main training loop}
     \PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{epochs}\PYG{p}{)}\PYG{p}{:}
         \PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{steps\PYGZus{}per\PYGZus{}epoch}\PYG{p}{)}\PYG{p}{:}
             \PYG{n}{idxs} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{x\PYGZus{}train}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{32}\PYG{p}{)}
             \PYG{n}{feed\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{x\PYGZus{}ph}\PYG{p}{:} \PYG{n}{x\PYGZus{}train}\PYG{p}{[}\PYG{n}{idxs}\PYG{p}{]}\PYG{p}{,}
                          \PYG{n}{y\PYGZus{}ph}\PYG{p}{:} \PYG{n}{y\PYGZus{}train}\PYG{p}{[}\PYG{n}{idxs}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
             \PYG{n}{outs} \PYG{o}{=} \PYG{n}{sess}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{p}{[}\PYG{n}{loss}\PYG{p}{,} \PYG{n}{acc}\PYG{p}{,} \PYG{n}{train\PYGZus{}op}\PYG{p}{]}\PYG{p}{,} \PYG{n}{feed\PYGZus{}dict}\PYG{o}{=}\PYG{n}{feed\PYGZus{}dict}\PYG{p}{)}
             \PYG{n}{logger}\PYG{o}{.}\PYG{n}{store}\PYG{p}{(}\PYG{n}{Loss}\PYG{o}{=}\PYG{n}{outs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{Acc}\PYG{o}{=}\PYG{n}{outs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}

         \PYG{c+c1}{\PYGZsh{} Save model}
         \PYG{k}{if} \PYG{p}{(}\PYG{n}{epoch} \PYG{o}{\PYGZpc{}} \PYG{n}{save\PYGZus{}freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o+ow}{or} \PYG{p}{(}\PYG{n}{epoch} \PYG{o}{==} \PYG{n}{epochs}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
             \PYG{n}{logger}\PYG{o}{.}\PYG{n}{save\PYGZus{}state}\PYG{p}{(}\PYG{n}{state\PYGZus{}dict}\PYG{o}{=}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{itr}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}

         \PYG{c+c1}{\PYGZsh{} Log info about epoch}
         \PYG{n}{logger}\PYG{o}{.}\PYG{n}{log\PYGZus{}tabular}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Epoch}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{epoch}\PYG{p}{)}
         \PYG{n}{logger}\PYG{o}{.}\PYG{n}{log\PYGZus{}tabular}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Acc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{with\PYGZus{}min\PYGZus{}and\PYGZus{}max}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
         \PYG{n}{logger}\PYG{o}{.}\PYG{n}{log\PYGZus{}tabular}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Loss}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{average\PYGZus{}only}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
         \PYG{n}{logger}\PYG{o}{.}\PYG{n}{log\PYGZus{}tabular}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TotalGradientSteps}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{epoch}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{*}\PYG{n}{steps\PYGZus{}per\PYGZus{}epoch}\PYG{p}{)}
         \PYG{n}{logger}\PYG{o}{.}\PYG{n}{log\PYGZus{}tabular}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{start\PYGZus{}time}\PYG{p}{)}
         \PYG{n}{logger}\PYG{o}{.}\PYG{n}{dump\PYGZus{}tabular}\PYG{p}{(}\PYG{p}{)}

 \PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
     \PYG{n}{train\PYGZus{}mnist}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
\sphinxresetverbatimhllines

In this example, observe that
\begin{itemize}
\item {} 
On line 19, \sphinxhref{../utils/logger.html\#spinup.utils.logx.Logger.save\_config}{logger.save\_config} is used to save the hyperparameter configuration to a JSON file.

\item {} 
On lines 42 and 43, \sphinxhref{../utils/logger.html\#spinup.utils.logx.Logger.setup\_tf\_saver}{logger.setup\_tf\_saver} is used to prepare the logger to save the key elements of the computation graph.

\item {} 
On line 54, diagnostics are saved to the logger’s internal state via \sphinxhref{../utils/logger.html\#spinup.utils.logx.EpochLogger.store}{logger.store}.

\item {} 
On line 58, the computation graph is saved once per epoch via \sphinxhref{../utils/logger.html\#spinup.utils.logx.Logger.save\_state}{logger.save\_state}.

\item {} 
On lines 61-66, \sphinxhref{../utils/logger.html\#spinup.utils.logx.EpochLogger.log\_tabular}{logger.log\_tabular} and \sphinxhref{../utils/logger.html\#spinup.utils.logx.Logger.dump\_tabular}{logger.dump\_tabular} are used to write the epoch diagnostics to file. Note that the keys passed into \sphinxhref{../utils/logger.html\#spinup.utils.logx.EpochLogger.log\_tabular}{logger.log\_tabular} are the same as the keys passed into \sphinxhref{../utils/logger.html\#spinup.utils.logx.EpochLogger.store}{logger.store}.

\end{itemize}


\subsection{Logging and PyTorch}
\label{\detokenize{utils/logger:logging-and-pytorch}}
The preceding example was given in Tensorflow. For PyTorch, everything is the same except for L42-43: instead of \sphinxcode{\sphinxupquote{logger.setup\_tf\_saver}}, you would use \sphinxcode{\sphinxupquote{logger.setup\_pytorch\_saver}}, and you would pass it \sphinxhref{https://pytorch.org/docs/stable/nn.html\#torch.nn.Module}{a PyTorch module} (the network you are training) as an argument.

The behaviour of \sphinxcode{\sphinxupquote{logger.save\_state}} is the same as in the Tensorflow case: each time it is called, it’ll save the latest version of the PyTorch module.


\subsection{Logging and MPI}
\label{\detokenize{utils/logger:logging-and-mpi}}
\begin{sphinxadmonition}{note}{You Should Know}

Several algorithms in RL are easily parallelized by using MPI to average gradients and/or other key quantities. The Spinning Up loggers are designed to be well-behaved when using MPI: things will only get written to stdout and to file from the process with rank 0. But information from other processes isn’t lost if you use the EpochLogger: everything which is passed into EpochLogger via \sphinxcode{\sphinxupquote{store}}, regardless of which process it’s stored in, gets used to compute average/std/min/max values for a diagnostic.
\end{sphinxadmonition}


\section{Logger Classes}
\label{\detokenize{utils/logger:logger-classes}}

\section{Loading Saved Models (PyTorch Only)}
\label{\detokenize{utils/logger:loading-saved-models-pytorch-only}}
To load an actor-critic model saved by a PyTorch Spinning Up implementation, run:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ac} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{path/to/model.pt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

When you use this method to load an actor-critic model, you can minimally expect it to have an \sphinxcode{\sphinxupquote{act}} method that allows you to sample actions from the policy, given observations:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{actions} \PYG{o}{=} \PYG{n}{ac}\PYG{o}{.}\PYG{n}{act}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{as\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{obs}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Loading Saved Graphs (Tensorflow Only)}
\label{\detokenize{utils/logger:loading-saved-graphs-tensorflow-only}}
When you use this method to restore a graph saved by a Tensorflow Spinning Up implementation, you can minimally expect it to include the following:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline
\sphinxstyletheadfamily 
Key
&\sphinxstyletheadfamily 
Value
\\
\hline
\sphinxcode{\sphinxupquote{x}}
&
Tensorflow placeholder for state input.
\\
\hline
\sphinxcode{\sphinxupquote{pi}}
&
\begin{DUlineblock}{0em}
\item[] Samples an action from the agent, conditioned
\item[] on states in \sphinxcode{\sphinxupquote{x}}.
\end{DUlineblock}
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}

The relevant value functions for an algorithm are also typically stored. For details of what else gets saved by a given algorithm, see its documentation page.


\chapter{Plotter}
\label{\detokenize{utils/plotter:plotter}}\label{\detokenize{utils/plotter::doc}}
See the page on \sphinxhref{../user/plotting.html}{plotting results} for documentation of the plotter.


\chapter{MPI Tools}
\label{\detokenize{utils/mpi:mpi-tools}}\label{\detokenize{utils/mpi::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{utils/mpi:id1}}{\hyperref[\detokenize{utils/mpi:mpi-tools}]{\sphinxcrossref{MPI Tools}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{utils/mpi:id2}}{\hyperref[\detokenize{utils/mpi:core-mpi-utilities}]{\sphinxcrossref{Core MPI Utilities}}}

\item {} 
\phantomsection\label{\detokenize{utils/mpi:id3}}{\hyperref[\detokenize{utils/mpi:mpi-pytorch-utilities}]{\sphinxcrossref{MPI + PyTorch Utilities}}}

\item {} 
\phantomsection\label{\detokenize{utils/mpi:id4}}{\hyperref[\detokenize{utils/mpi:mpi-tensorflow-utilities}]{\sphinxcrossref{MPI + Tensorflow Utilities}}}

\end{itemize}

\end{itemize}
\end{sphinxShadowBox}


\section{Core MPI Utilities}
\label{\detokenize{utils/mpi:core-mpi-utilities}}

\section{MPI + PyTorch Utilities}
\label{\detokenize{utils/mpi:mpi-pytorch-utilities}}
\sphinxcode{\sphinxupquote{spinup.utils.mpi\_pytorch}} contains a few tools to make it easy to do data-parallel PyTorch optimization across MPI processes. The two main ingredients are syncing parameters and averaging gradients before they are used by the adaptive optimizer. Also there’s a hacky fix for a problem where the PyTorch instance in each separate process tries to get too many threads, and they start to clobber each other.

The pattern for using these tools looks something like this:
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi )}
\makeatletter\def\p@enumii{\p@enumi \theenumi )}\makeatother
\item {} 
At the beginning of the training script, call \sphinxcode{\sphinxupquote{setup\_pytorch\_for\_mpi()}}. (Avoids clobbering problem.)

\item {} 
After you’ve constructed a PyTorch module, call \sphinxcode{\sphinxupquote{sync\_params(module)}}.

\item {} 
Then, during gradient descent, call \sphinxcode{\sphinxupquote{mpi\_avg\_grads}} after the backward pass, like so:

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{loss} \PYG{o}{=} \PYG{n}{compute\PYGZus{}loss}\PYG{p}{(}\PYG{n}{module}\PYG{p}{)}
\PYG{n}{loss}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{mpi\PYGZus{}avg\PYGZus{}grads}\PYG{p}{(}\PYG{n}{module}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} averages gradient buffers across MPI processes!}
\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\section{MPI + Tensorflow Utilities}
\label{\detokenize{utils/mpi:mpi-tensorflow-utilities}}
The \sphinxcode{\sphinxupquote{spinup.utils.mpi\_tf}} contains a a few tools to make it easy to use the AdamOptimizer across many MPI processes. This is a bit hacky—if you’re looking for something more sophisticated and general-purpose, consider \sphinxhref{https://github.com/uber/horovod}{horovod}.


\chapter{Run Utils}
\label{\detokenize{utils/run_utils:run-utils}}\label{\detokenize{utils/run_utils::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Table of Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{utils/run_utils:id1}}{\hyperref[\detokenize{utils/run_utils:run-utils}]{\sphinxcrossref{Run Utils}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{utils/run_utils:id2}}{\hyperref[\detokenize{utils/run_utils:experimentgrid}]{\sphinxcrossref{ExperimentGrid}}}

\item {} 
\phantomsection\label{\detokenize{utils/run_utils:id3}}{\hyperref[\detokenize{utils/run_utils:calling-experiments}]{\sphinxcrossref{Calling Experiments}}}

\end{itemize}

\end{itemize}
\end{sphinxShadowBox}


\section{ExperimentGrid}
\label{\detokenize{utils/run_utils:experimentgrid}}
Spinning Up ships with a tool called ExperimentGrid for making hyperparameter ablations easier. This is based on (but simpler than) \sphinxhref{https://github.com/rll/rllab/blob/master/rllab/misc/instrument.py\#L173}{the rllab tool} called VariantGenerator.


\section{Calling Experiments}
\label{\detokenize{utils/run_utils:calling-experiments}}

\chapter{Acknowledgements}
\label{\detokenize{etc/acknowledgements:acknowledgements}}\label{\detokenize{etc/acknowledgements::doc}}
We gratefully acknowledge the contributions of \sphinxhref{https://spinningup.openai.com/en/latest/etc/author.html}{Josh Achiam} for creating the \sphinxhref{https://spinningup.openai.com/en/latest/}{Spinning Up} project.
It provided us with a great template to build the new version of our algorithms on.


\chapter{About the Author}
\label{\detokenize{etc/author:about-the-author}}\label{\detokenize{etc/author::doc}}
The Machine Learning Control is developed and maintained by the \sphinxhref{https://www.tudelft.nl/en/3me/about/departments/cognitive-robotics-cor/}{Cognitive Robotics department}
of the \sphinxhref{https://tudelft.nl}{TU Delft}. It was created out of a Master Thesis of \sphinxhref{https://github.com/rickstaa}{Rick Staa} under the supervision of
\sphinxhref{https://www.tudelft.nl/staff/wei.pan/}{dr. Wei Pan}.


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}