<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Soft Actor-Critic &mdash; stable-learning-control 6.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=eafc0fe6" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/modify.css?v=519ed47b" />

  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=9497d378"></script>
        <script src="../../_static/doctools.js?v=888ff710"></script>
        <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Running Experiments" href="../running.html" />
    <link rel="prev" title="Lyapunov Actor-Twin Critic (LATC)" href="latc.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html">
            
              <img src="../../_static/logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                6.0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker.html">Use with Docker</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../algorithms.html">Available Agents</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../algorithms.html#stable-agents">Stable Agents</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../algorithms.html#unstable-agents">Unstable Agents</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Soft Actor-Critic</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="#implementation">Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../running.html">Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hyperparameter_tuning.html">Hyperparameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../saving_and_loading.html">Experiment Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plotting.html">Plotting Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../eval_robustness.html">Evaluating Robustness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utilities</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../utils/loggers.html">Loggers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../utils/mpi.html">MPI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../utils/run_utils.html">Run Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../utils/plotter.html">Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../utils/testers.html">Policy testers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dev/contributing.html">Contribute to stable-learning-control</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/doc_dev.html">Build the documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../autoapi/index.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Etc.</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../etc/acknowledgements.html">Acknowledgements</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">stable-learning-control</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../algorithms.html">Available Agents</a></li>
      <li class="breadcrumb-item active">Soft Actor-Critic</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/rickstaa/stable-learning-control/blob/main/docs/source/usage/algorithms/sac.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="soft-actor-critic">
<span id="sac"></span><h1><a class="toc-backref" href="#id2" role="doc-backlink">Soft Actor-Critic</a><a class="headerlink" href="#soft-actor-critic" title="Permalink to this heading"></a></h1>
<nav class="contents" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#soft-actor-critic" id="id2">Soft Actor-Critic</a></p>
<ul>
<li><p><a class="reference internal" href="#background" id="id3">Background</a></p>
<ul>
<li><p><a class="reference internal" href="#quick-facts" id="id4">Quick Facts</a></p></li>
<li><p><a class="reference internal" href="#further-reading" id="id5">Further Reading</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#implementation" id="id6">Implementation</a></p>
<ul>
<li><p><a class="reference internal" href="#algorithm-pytorch-version" id="id7">Algorithm: PyTorch Version</a></p></li>
<li><p><a class="reference internal" href="#saved-model-contents-pytorch-version" id="id8">Saved Model Contents: PyTorch Version</a></p></li>
<li><p><a class="reference internal" href="#algorithm-tensorflow-version" id="id9">Algorithm: TensorFlow Version</a></p></li>
<li><p><a class="reference internal" href="#saved-model-contents-tensorflow-version" id="id10">Saved Model Contents: TensorFlow Version</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references" id="id11">References</a></p>
<ul>
<li><p><a class="reference internal" href="#relevant-papers" id="id12">Relevant Papers</a></p></li>
<li><p><a class="reference internal" href="#other-public-implementations" id="id13">Other Public Implementations</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The SAC algorithm has no stability guarantees. Please use the <a class="reference internal" href="lac.html#lac"><span class="std std-ref">LAC</span></a> algorithm if
you require stability guarantees.</p>
</div>
<section id="background">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Background</a><a class="headerlink" href="#background" title="Permalink to this heading"></a></h2>
<p>Soft Actor-Critic (SAC) is an algorithm that optimises a stochastic policy in an off-policy way,
forming a bridge between stochastic policy optimisation and <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html">DDPG-style</a> approaches. It isn’t a
direct successor to <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/td3.html?highlight=TD3">TD3</a> (having been published roughly concurrently). Still, it incorporates
the clipped double-Q trick, and due to the inherent stochasticity of the policy in SAC, it also
winds up benefiting from something like target policy smoothing.</p>
<p>A central feature of SAC is <strong>entropy regularisation.</strong> The policy is trained to maximise a
trade-off between expected return and <a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">entropy</a>, a measure of randomness in the policy. This
is closely connected to the exploration-exploitation trade-off: increasing entropy results in
more exploration, which can accelerate learning later on. It can also prevent the policy from
prematurely converging to a bad local optimum.</p>
<section id="quick-facts">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Quick Facts</a><a class="headerlink" href="#quick-facts" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>SAC is an off-policy algorithm.</p></li>
<li><p>The version of SAC implemented here can only be used for environments with continuous action spaces.</p></li>
<li><p>An alternate version of SAC, which slightly changes the policy update rule, can be implemented to
handle discrete action spaces.</p></li>
<li><p>The SLC implementation of SAC does not support parallelisation.</p></li>
</ul>
</section>
<section id="further-reading">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Further Reading</a><a class="headerlink" href="#further-reading" title="Permalink to this heading"></a></h3>
<p>The version implemented here was based on the version implemented in the <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/sac.html">SpinningUp repository</a>. For more
information on the SAC algorithm, you are referred to the <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/sac.html">SpinningUp documentation</a> or the original
paper of <a class="reference external" href="https://arxiv.org/pdf/1812.05905.pdf">Haarnoja et al., 2019</a>. Our implementation slightly differs from the SpinningUp version in
that we also added the Automatic Entropy Tuning scheme introduced by <a class="reference external" href="https://arxiv.org/pdf/1812.05905.pdf">Haarnoja et al., 2019</a>. As a
result, during training, the entropy Lagrange Multiplier <img class="math" src="../../_images/math/ba6a568c016c30c0977a2e41bda111996c10a0b8.svg" alt="\alpha"/> is updated by</p>
<div class="math">
<p><img src="../../_images/math/4bf8d9602b11f293676695842a1fc840e75096e8.svg" alt="\alpha \leftarrow \max(0, \alpha + \delta \bigtriangledown_{\alpha}J(\alpha)))"/></p>
</div><p>where <img class="math" src="../../_images/math/a9aa35dae0bbfb077081fc99c98a871e7d7b1bc4.svg" alt="\delta"/> is the learning rate. As explained in <a class="reference external" href="https://arxiv.org/pdf/1812.05905.pdf">Haarnoja et al., 2019</a>, this constrains
the policy’s average entropy.</p>
</section>
</section>
<section id="implementation">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Implementation</a><a class="headerlink" href="#implementation" title="Permalink to this heading"></a></h2>
<div class="admonition-you-should-know admonition">
<p class="admonition-title">You Should Know</p>
<p>In what follows, we give documentation for the PyTorch and TensorFlow implementations of SAC
in SLC. They have nearly identical function calls and docstrings, except for details relating
to model construction. However, we include both full docstrings for completeness.</p>
</div>
<section id="algorithm-pytorch-version">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Algorithm: PyTorch Version</a><a class="headerlink" href="#algorithm-pytorch-version" title="Permalink to this heading"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="stable_learning_control.algos.pytorch.sac.sac">
<span class="sig-prename descclassname"><span class="pre">stable_learning_control.algos.pytorch.sac.</span></span><span class="sig-name descname"><span class="pre">sac</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">env_fn,</span> <span class="pre">actor_critic=None,</span> <span class="pre">ac_kwargs={'activation':</span> <span class="pre">{'actor':</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;,</span> <span class="pre">'critic':</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;},</span> <span class="pre">'hidden_sizes':</span> <span class="pre">{'actor':</span> <span class="pre">[256,</span> <span class="pre">256],</span> <span class="pre">'critic':</span> <span class="pre">[256,</span> <span class="pre">256]},</span> <span class="pre">'output_activation':</span> <span class="pre">{'actor':</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;,</span> <span class="pre">'critic':</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.linear.Identity'&gt;}},</span> <span class="pre">opt_type='maximize',</span> <span class="pre">max_ep_len=None,</span> <span class="pre">epochs=100,</span> <span class="pre">steps_per_epoch=2048,</span> <span class="pre">start_steps=0,</span> <span class="pre">update_every=100,</span> <span class="pre">update_after=1000,</span> <span class="pre">steps_per_update=100,</span> <span class="pre">num_test_episodes=10,</span> <span class="pre">alpha=0.99,</span> <span class="pre">gamma=0.99,</span> <span class="pre">polyak=0.995,</span> <span class="pre">target_entropy=None,</span> <span class="pre">adaptive_temperature=True,</span> <span class="pre">lr_a=0.0001,</span> <span class="pre">lr_c=0.0003,</span> <span class="pre">lr_alpha=0.0001,</span> <span class="pre">lr_a_final=1e-10,</span> <span class="pre">lr_c_final=1e-10,</span> <span class="pre">lr_alpha_final=1e-10,</span> <span class="pre">lr_decay_type='linear',</span> <span class="pre">lr_a_decay_type=None,</span> <span class="pre">lr_c_decay_type=None,</span> <span class="pre">lr_alpha_decay_type=None,</span> <span class="pre">lr_decay_ref='epoch',</span> <span class="pre">batch_size=256,</span> <span class="pre">replay_size=1000000,</span> <span class="pre">seed=None,</span> <span class="pre">device='cpu',</span> <span class="pre">logger_kwargs={},</span> <span class="pre">save_freq=1,</span> <span class="pre">start_policy=None,</span> <span class="pre">export=False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/stable_learning_control/algos/pytorch/sac/sac.html#sac"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_learning_control.algos.pytorch.sac.sac" title="Permalink to this definition"></a></dt>
<dd><p>Trains the SAC algorithm in a given environment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>env_fn</strong> – A function which creates a copy of the environment. The environment
must satisfy the gymnasium API.</p></li>
<li><p><strong>actor_critic</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.4)"><em>torch.nn.Module</em></a><em>, </em><em>optional</em>) – <p>The constructor method for a
Torch Module with an <code class="docutils literal notranslate"><span class="pre">act</span></code> method, a <code class="docutils literal notranslate"><span class="pre">pi</span></code> module and several
<code class="docutils literal notranslate"><span class="pre">Q</span></code> or <code class="docutils literal notranslate"><span class="pre">L</span></code> modules. The <code class="docutils literal notranslate"><span class="pre">act</span></code> method and <code class="docutils literal notranslate"><span class="pre">pi</span></code> module should
accept batches of observations as inputs, and the <code class="docutils literal notranslate"><span class="pre">Q*</span></code> and <code class="docutils literal notranslate"><span class="pre">L</span></code>
modules should accept a batch of observations and a batch of actions as
inputs. When called, these modules should return:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Call</p></th>
<th class="head"><p>Output Shape</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">act</span></code></p></td>
<td><p>(batch, act_dim)</p></td>
<td><div class="line-block">
<div class="line">Numpy array of actions for each</div>
<div class="line">observation.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">Q*/L</span></code></p></td>
<td><p>(batch,)</p></td>
<td><div class="line-block">
<div class="line">Tensor containing one current estimate</div>
<div class="line">of <code class="docutils literal notranslate"><span class="pre">Q*/L</span></code> for the provided</div>
<div class="line">observations and actions. (Critical:</div>
<div class="line">make sure to flatten this!)</div>
</div>
</td>
</tr>
</tbody>
</table>
<p>Calling <code class="docutils literal notranslate"><span class="pre">pi</span></code> should return:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>Shape</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">a</span></code></p></td>
<td><p>(batch, act_dim)</p></td>
<td><div class="line-block">
<div class="line">Tensor containing actions from policy</div>
<div class="line">given observations.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">logp_pi</span></code></p></td>
<td><p>(batch,)</p></td>
<td><div class="line-block">
<div class="line">Tensor containing log probabilities of</div>
<div class="line">actions in <code class="docutils literal notranslate"><span class="pre">a</span></code>. Importantly:</div>
<div class="line">gradients should be able to flow back</div>
<div class="line">into <code class="docutils literal notranslate"><span class="pre">a</span></code>.</div>
</div>
</td>
</tr>
</tbody>
</table>
<p>Defaults to
<a class="reference internal" href="../../autoapi/stable_learning_control/algos/pytorch/policies/soft_actor_critic/index.html#stable_learning_control.algos.pytorch.policies.soft_actor_critic.SoftActorCritic" title="stable_learning_control.algos.pytorch.policies.soft_actor_critic.SoftActorCritic"><code class="xref py py-class docutils literal notranslate"><span class="pre">SoftActorCritic</span></code></a></p>
</p></li>
<li><p><strong>ac_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><em>dict</em></a><em>, </em><em>optional</em>) – <p>Any kwargs appropriate for the ActorCritic
object you provided to SAC. Defaults to:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Kwarg</p></th>
<th class="head"><p>Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">hidden_sizes_actor</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">64</span> <span class="pre">x</span> <span class="pre">2</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">hidden_sizes_critic</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">128</span> <span class="pre">x</span> <span class="pre">2</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">activation</span></code></p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReLU</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">output_activation</span></code></p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReLU</span></code></a></p></td>
</tr>
</tbody>
</table>
</p></li>
<li><p><strong>opt_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The optimization type you want to use. Options
<code class="docutils literal notranslate"><span class="pre">maximize</span></code> and <code class="docutils literal notranslate"><span class="pre">minimize</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">maximize</span></code>.</p></li>
<li><p><strong>max_ep_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Maximum length of trajectory / episode /
rollout. Defaults to the environment maximum.</p></li>
<li><p><strong>epochs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of epochs to run and train agent. Defaults
to <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>steps_per_epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of steps of interaction
(state-action pairs) for the agent and the environment in each epoch.
Defaults to <code class="docutils literal notranslate"><span class="pre">2048</span></code>.</p></li>
<li><p><strong>start_steps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of steps for uniform-random action
selection, before running real policy. Helps exploration. Defaults to
<code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>update_every</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of env interactions that should elapse
between gradient descent updates. Defaults to <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>update_after</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of env interactions to collect before
starting to do gradient descent updates. Ensures replay buffer
is full enough for useful updates. Defaults to <code class="docutils literal notranslate"><span class="pre">1000</span></code>.</p></li>
<li><p><strong>steps_per_update</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of gradient descent steps that are
performed for each gradient descent update. This determines the ratio of
env steps to gradient steps (i.e. <code class="xref py py-obj docutils literal notranslate"><span class="pre">update_every</span></code>/
<code class="xref py py-obj docutils literal notranslate"><span class="pre">steps_per_update</span></code>). Defaults to <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>num_test_episodes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of episodes used to test the
deterministic policy at the end of each epoch. This is used for logging
the performance. Defaults to <code class="docutils literal notranslate"><span class="pre">10</span></code>.</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Entropy regularization coefficient (Equivalent to
inverse of reward scale in the original SAC paper). Defaults to
<code class="docutils literal notranslate"><span class="pre">0.99</span></code>.</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Discount factor. (Always between 0 and 1.).
Defaults to <code class="docutils literal notranslate"><span class="pre">0.99</span></code>.</p></li>
<li><p><strong>polyak</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – <p>Interpolation factor in polyak averaging for
target networks. Target networks are updated towards main networks
according to:</p>
<div class="math">
<p><img src="../../_images/math/a870a04782c115a9eb9eca78c12004e4b075c3d9.svg" alt="\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta"/></p>
</div><p>where <img class="math" src="../../_images/math/2fbecaad8bd4b240f53ad914202698a230a98713.svg" alt="\rho"/> is polyak (Always between 0 and 1, usually close to 1.).
In some papers <img class="math" src="../../_images/math/2fbecaad8bd4b240f53ad914202698a230a98713.svg" alt="\rho"/> is defined as (1 - <img class="math" src="../../_images/math/5a42b9d040dc41318429f68d53cd32fdbde54393.svg" alt="\tau"/>) where
<img class="math" src="../../_images/math/5a42b9d040dc41318429f68d53cd32fdbde54393.svg" alt="\tau"/> is the soft replacement factor. Defaults to <code class="docutils literal notranslate"><span class="pre">0.995</span></code>.</p>
</p></li>
<li><p><strong>target_entropy</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – <p>Initial target entropy used while learning
the entropy temperature (alpha). Defaults to the
maximum information (bits) contained in action space. This can be
calculated according to :</p>
<div class="math">
<p><img src="../../_images/math/2a0fb232d73f170ee01e8bc9cfa646b44e64488c.svg" alt="-{\prod }_{i=0}^{n}action\_di{m}_{i}\phantom{\rule{0ex}{0ex}}"/></p>
</div></p></li>
<li><p><strong>adaptive_temperature</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Enabled Automating Entropy Adjustment
for maximum Entropy RL_learning.</p></li>
<li><p><strong>lr_a</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Learning rate used for the actor. Defaults to
<code class="docutils literal notranslate"><span class="pre">1e-4</span></code>.</p></li>
<li><p><strong>lr_c</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Learning rate used for the (soft) critic. Defaults to
<code class="docutils literal notranslate"><span class="pre">1e-4</span></code>.</p></li>
<li><p><strong>lr_alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Learning rate used for the entropy temperature.
Defaults to <code class="docutils literal notranslate"><span class="pre">1e-4</span></code>.</p></li>
<li><p><strong>lr_a_final</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – The final actor learning rate that is achieved
at the end of the training. Defaults to <code class="docutils literal notranslate"><span class="pre">1e-10</span></code>.</p></li>
<li><p><strong>lr_c_final</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – The final critic learning rate that is achieved
at the end of the training. Defaults to <code class="docutils literal notranslate"><span class="pre">1e-10</span></code>.</p></li>
<li><p><strong>lr_alpha_final</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – The final alpha learning rate that is
achieved at the end of the training. Defaults to <code class="docutils literal notranslate"><span class="pre">1e-10</span></code>.</p></li>
<li><p><strong>lr_decay_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The learning rate decay type that is used (options
are: <code class="docutils literal notranslate"><span class="pre">linear</span></code> and <code class="docutils literal notranslate"><span class="pre">exponential</span></code> and <code class="docutils literal notranslate"><span class="pre">constant</span></code>). Defaults to
<code class="docutils literal notranslate"><span class="pre">linear</span></code>. Can be overridden by the specific learning rate decay types.</p></li>
<li><p><strong>lr_a_decay_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The learning rate decay type that is used for
the actor learning rate (options are: <code class="docutils literal notranslate"><span class="pre">linear</span></code> and <code class="docutils literal notranslate"><span class="pre">exponential</span></code> and
<code class="docutils literal notranslate"><span class="pre">constant</span></code>). If not specified, the general learning rate decay type is used.</p></li>
<li><p><strong>lr_c_decay_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The learning rate decay type that is used for
the critic learning rate (options are: <code class="docutils literal notranslate"><span class="pre">linear</span></code> and <code class="docutils literal notranslate"><span class="pre">exponential</span></code> and
<code class="docutils literal notranslate"><span class="pre">constant</span></code>). If not specified, the general learning rate decay type is used.</p></li>
<li><p><strong>lr_alpha_decay_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The learning rate decay type that is used
for the alpha learning rate (options are: <code class="docutils literal notranslate"><span class="pre">linear</span></code> and <code class="docutils literal notranslate"><span class="pre">exponential</span></code>
and <code class="docutils literal notranslate"><span class="pre">constant</span></code>). If not specified, the general learning rate decay type is used.</p></li>
<li><p><strong>lr_decay_ref</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The reference variable that is used for decaying
the learning rate (options: <code class="docutils literal notranslate"><span class="pre">epoch</span></code> and <code class="docutils literal notranslate"><span class="pre">step</span></code>). Defaults to <code class="docutils literal notranslate"><span class="pre">epoch</span></code>.</p></li>
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Minibatch size for SGD. Defaults to <code class="docutils literal notranslate"><span class="pre">256</span></code>.</p></li>
<li><p><strong>replay_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Maximum length of replay buffer. Defaults to
<code class="docutils literal notranslate"><span class="pre">1e6</span></code>.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a>) – Seed for random number generators. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The device the networks are placed on (options: <code class="docutils literal notranslate"><span class="pre">cpu</span></code>,
<code class="docutils literal notranslate"><span class="pre">gpu</span></code>, <code class="docutils literal notranslate"><span class="pre">gpu:0</span></code>, <code class="docutils literal notranslate"><span class="pre">gpu:1</span></code>, etc.). Defaults to <code class="docutils literal notranslate"><span class="pre">cpu</span></code>.</p></li>
<li><p><strong>logger_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><em>dict</em></a><em>, </em><em>optional</em>) – Keyword args for EpochLogger.</p></li>
<li><p><strong>save_freq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – How often (in terms of gap between epochs) to save
the current policy and value function.</p></li>
<li><p><strong>start_policy</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a>) – Path of a already trained policy to use as the starting
point for the training. By default a new policy is created.</p></li>
<li><p><strong>export</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a>) – Whether you want to export the model as a <code class="docutils literal notranslate"><span class="pre">TorchScript</span></code> such
that it can be deployed on hardware. By default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>tuple containing:</p>
<blockquote>
<div><ul class="simple">
<li><p>policy (<code class="xref py py-class docutils literal notranslate"><span class="pre">SAC</span></code>): The trained actor-critic policy.</p></li>
<li><p>replay_buffer (union[<a class="reference internal" href="../../autoapi/stable_learning_control/algos/common/buffers/index.html#stable_learning_control.algos.common.buffers.ReplayBuffer" title="stable_learning_control.algos.common.buffers.ReplayBuffer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReplayBuffer</span></code></a>, <a class="reference internal" href="../../autoapi/stable_learning_control/algos/common/buffers/index.html#stable_learning_control.algos.common.buffers.FiniteHorizonReplayBuffer" title="stable_learning_control.algos.common.buffers.FiniteHorizonReplayBuffer"><code class="xref py py-class docutils literal notranslate"><span class="pre">FiniteHorizonReplayBuffer</span></code></a>]):
The replay buffer used during training.</p></li>
</ul>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a>)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="saved-model-contents-pytorch-version">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Saved Model Contents: PyTorch Version</a><a class="headerlink" href="#saved-model-contents-pytorch-version" title="Permalink to this heading"></a></h3>
<p>The PyTorch version of the SAC algorithm is implemented by subclassing the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a> class. As a result,
the model weights are saved using the <code class="docutils literal notranslate"><span class="pre">model_state</span></code> dictionary (
<code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code>). These saved weights
can be found in the <code class="docutils literal notranslate"><span class="pre">torch_save/model_state.pt</span></code> file. For an example of how to load a model
using this file, see <a class="reference internal" href="../saving_and_loading.html#saving-and-loading"><span class="std std-ref">Experiment Outputs</span></a> or the
<a class="reference external" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch documentation</a>.</p>
</section>
<section id="algorithm-tensorflow-version">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Algorithm: TensorFlow Version</a><a class="headerlink" href="#algorithm-tensorflow-version" title="Permalink to this heading"></a></h3>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The TensorFlow version is still experimental. It is not guaranteed to work, and it is not
guaranteed to be up-to-date with the PyTorch version.</p>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="stable_learning_control.algos.tf2.sac.sac">
<span class="sig-prename descclassname"><span class="pre">stable_learning_control.algos.tf2.sac.</span></span><span class="sig-name descname"><span class="pre">sac</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">env_fn,</span> <span class="pre">actor_critic=None,</span> <span class="pre">ac_kwargs={'activation':</span> <span class="pre">{'actor':</span> <span class="pre">&lt;function</span> <span class="pre">relu&gt;,</span> <span class="pre">'critic':</span> <span class="pre">&lt;function</span> <span class="pre">relu&gt;},</span> <span class="pre">'hidden_sizes':</span> <span class="pre">{'actor':</span> <span class="pre">[256,</span> <span class="pre">256],</span> <span class="pre">'critic':</span> <span class="pre">[256,</span> <span class="pre">256]},</span> <span class="pre">'output_activation':</span> <span class="pre">{'actor':</span> <span class="pre">&lt;function</span> <span class="pre">relu&gt;,</span> <span class="pre">'critic':</span> <span class="pre">None}},</span> <span class="pre">opt_type='maximize',</span> <span class="pre">max_ep_len=None,</span> <span class="pre">epochs=100,</span> <span class="pre">steps_per_epoch=2048,</span> <span class="pre">start_steps=0,</span> <span class="pre">update_every=100,</span> <span class="pre">update_after=1000,</span> <span class="pre">steps_per_update=100,</span> <span class="pre">num_test_episodes=10,</span> <span class="pre">alpha=0.99,</span> <span class="pre">gamma=0.99,</span> <span class="pre">polyak=0.995,</span> <span class="pre">target_entropy=None,</span> <span class="pre">adaptive_temperature=True,</span> <span class="pre">lr_a=0.0001,</span> <span class="pre">lr_c=0.0003,</span> <span class="pre">lr_alpha=0.0001,</span> <span class="pre">lr_a_final=1e-10,</span> <span class="pre">lr_c_final=1e-10,</span> <span class="pre">lr_alpha_final=1e-10,</span> <span class="pre">lr_decay_type='linear',</span> <span class="pre">lr_a_decay_type=None,</span> <span class="pre">lr_c_decay_type=None,</span> <span class="pre">lr_alpha_decay_type=None,</span> <span class="pre">lr_decay_ref='epoch',</span> <span class="pre">batch_size=256,</span> <span class="pre">replay_size=1000000,</span> <span class="pre">seed=None,</span> <span class="pre">device='cpu',</span> <span class="pre">logger_kwargs={},</span> <span class="pre">save_freq=1,</span> <span class="pre">start_policy=None,</span> <span class="pre">export=False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/stable_learning_control/algos/tf2/sac/sac.html#sac"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_learning_control.algos.tf2.sac.sac" title="Permalink to this definition"></a></dt>
<dd><p>Trains the SAC algorithm in a given environment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>env_fn</strong> – A function which creates a copy of the environment. The environment
must satisfy the gymnasium API.</p></li>
<li><p><strong>actor_critic</strong> (<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Module" title="(in TensorFlow v2.8)"><em>tf.Module</em></a><em>, </em><em>optional</em>) – <p>The constructor method for a
TensorFlow Module with an <code class="docutils literal notranslate"><span class="pre">act</span></code> method, a <code class="docutils literal notranslate"><span class="pre">pi</span></code> module and several <code class="docutils literal notranslate"><span class="pre">Q</span></code>
or <code class="docutils literal notranslate"><span class="pre">L</span></code> modules. The <code class="docutils literal notranslate"><span class="pre">act</span></code> method and <code class="docutils literal notranslate"><span class="pre">pi</span></code> module should accept batches
of observations as inputs, and the <code class="docutils literal notranslate"><span class="pre">Q*</span></code> and
<code class="docutils literal notranslate"><span class="pre">L</span></code> modules should accept a batch of observations and a batch of actions
as inputs. When called, these modules should return:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Call</p></th>
<th class="head"><p>Output Shape</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">act</span></code></p></td>
<td><p>(batch, act_dim)</p></td>
<td><div class="line-block">
<div class="line">Numpy array of actions for each</div>
<div class="line">observation.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">Q*/L</span></code></p></td>
<td><p>(batch,)</p></td>
<td><div class="line-block">
<div class="line">Tensor containing one current estimate</div>
<div class="line">of <code class="docutils literal notranslate"><span class="pre">Q*/L</span></code> for the provided</div>
<div class="line">observations and actions. (Critical:</div>
<div class="line">make sure to flatten this!)</div>
</div>
</td>
</tr>
</tbody>
</table>
<p>Calling <code class="docutils literal notranslate"><span class="pre">pi</span></code> should return:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>Shape</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">a</span></code></p></td>
<td><p>(batch, act_dim)</p></td>
<td><div class="line-block">
<div class="line">Tensor containing actions from policy</div>
<div class="line">given observations.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">logp_pi</span></code></p></td>
<td><p>(batch,)</p></td>
<td><div class="line-block">
<div class="line">Tensor containing log probabilities of</div>
<div class="line">actions in <code class="docutils literal notranslate"><span class="pre">a</span></code>. Importantly:</div>
<div class="line">gradients should be able to flow back</div>
<div class="line">into <code class="docutils literal notranslate"><span class="pre">a</span></code>.</div>
</div>
</td>
</tr>
</tbody>
</table>
<p>Defaults to
<a class="reference internal" href="../../autoapi/stable_learning_control/algos/tf2/policies/soft_actor_critic/index.html#stable_learning_control.algos.tf2.policies.soft_actor_critic.SoftActorCritic" title="stable_learning_control.algos.tf2.policies.soft_actor_critic.SoftActorCritic"><code class="xref py py-class docutils literal notranslate"><span class="pre">SoftActorCritic</span></code></a></p>
</p></li>
<li><p><strong>ac_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><em>dict</em></a><em>, </em><em>optional</em>) – <p>Any kwargs appropriate for the ActorCritic
object you provided to SAC. Defaults to:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Kwarg</p></th>
<th class="head"><p>Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">hidden_sizes_actor</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">64</span> <span class="pre">x</span> <span class="pre">2</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">hidden_sizes_critic</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">128</span> <span class="pre">x</span> <span class="pre">2</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">activation</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tf.nn.relu</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">output_activation</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tf.nn.relu</span></code></p></td>
</tr>
</tbody>
</table>
</p></li>
<li><p><strong>opt_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The optimization type you want to use. Options
<code class="docutils literal notranslate"><span class="pre">maximize</span></code> and <code class="docutils literal notranslate"><span class="pre">minimize</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">maximize</span></code>.</p></li>
<li><p><strong>max_ep_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Maximum length of trajectory / episode /
rollout. Defaults to the environment maximum.</p></li>
<li><p><strong>epochs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of epochs to run and train agent. Defaults
to <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>steps_per_epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of steps of interaction
(state-action pairs) for the agent and the environment in each epoch.
Defaults to <code class="docutils literal notranslate"><span class="pre">2048</span></code>.</p></li>
<li><p><strong>start_steps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of steps for uniform-random action
selection, before running real policy. Helps exploration. Defaults to
<code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>update_every</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of env interactions that should elapse
between gradient descent updates. Defaults to <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>update_after</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of env interactions to collect before
starting to do gradient descent updates. Ensures replay buffer
is full enough for useful updates. Defaults to <code class="docutils literal notranslate"><span class="pre">1000</span></code>.</p></li>
<li><p><strong>steps_per_update</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of gradient descent steps that are
performed for each gradient descent update. This determines the ratio of
env steps to gradient steps (i.e. <code class="xref py py-obj docutils literal notranslate"><span class="pre">update_every</span></code>/
<code class="xref py py-obj docutils literal notranslate"><span class="pre">steps_per_update</span></code>). Defaults to <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>num_test_episodes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of episodes used to test the
deterministic policy at the end of each epoch. This is used for logging
the performance. Defaults to <code class="docutils literal notranslate"><span class="pre">10</span></code>.</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Entropy regularization coefficient (Equivalent to
inverse of reward scale in the original SAC paper). Defaults to
<code class="docutils literal notranslate"><span class="pre">0.99</span></code>.</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Discount factor. (Always between 0 and 1.).
Defaults to <code class="docutils literal notranslate"><span class="pre">0.99</span></code>.</p></li>
<li><p><strong>polyak</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – <p>Interpolation factor in polyak averaging for
target networks. Target networks are updated towards main networks
according to:</p>
<div class="math">
<p><img src="../../_images/math/a870a04782c115a9eb9eca78c12004e4b075c3d9.svg" alt="\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta"/></p>
</div><p>where <img class="math" src="../../_images/math/2fbecaad8bd4b240f53ad914202698a230a98713.svg" alt="\rho"/> is polyak (Always between 0 and 1, usually close to 1.).
In some papers <img class="math" src="../../_images/math/2fbecaad8bd4b240f53ad914202698a230a98713.svg" alt="\rho"/> is defined as (1 - <img class="math" src="../../_images/math/5a42b9d040dc41318429f68d53cd32fdbde54393.svg" alt="\tau"/>) where
<img class="math" src="../../_images/math/5a42b9d040dc41318429f68d53cd32fdbde54393.svg" alt="\tau"/> is the soft replacement factor. Defaults to <code class="docutils literal notranslate"><span class="pre">0.995</span></code>.</p>
</p></li>
<li><p><strong>target_entropy</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – <p>Initial target entropy used while learning
the entropy temperature (alpha). Defaults to the
maximum information (bits) contained in action space. This can be
calculated according to :</p>
<div class="math">
<p><img src="../../_images/math/2a0fb232d73f170ee01e8bc9cfa646b44e64488c.svg" alt="-{\prod }_{i=0}^{n}action\_di{m}_{i}\phantom{\rule{0ex}{0ex}}"/></p>
</div></p></li>
<li><p><strong>adaptive_temperature</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Enabled Automating Entropy Adjustment
for maximum Entropy RL_learning.</p></li>
<li><p><strong>lr_a</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Learning rate used for the actor. Defaults to
<code class="docutils literal notranslate"><span class="pre">1e-4</span></code>.</p></li>
<li><p><strong>lr_c</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Learning rate used for the (soft) critic. Defaults to
<code class="docutils literal notranslate"><span class="pre">1e-4</span></code>.</p></li>
<li><p><strong>lr_alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Learning rate used for the entropy temperature.
Defaults to <code class="docutils literal notranslate"><span class="pre">1e-4</span></code>.</p></li>
<li><p><strong>lr_a_final</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – The final actor learning rate that is achieved
at the end of the training. Defaults to <code class="docutils literal notranslate"><span class="pre">1e-10</span></code>.</p></li>
<li><p><strong>lr_c_final</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – The final critic learning rate that is achieved
at the end of the training. Defaults to <code class="docutils literal notranslate"><span class="pre">1e-10</span></code>.</p></li>
<li><p><strong>lr_decay_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The learning rate decay type that is used (
options are: <code class="docutils literal notranslate"><span class="pre">linear</span></code> and <code class="docutils literal notranslate"><span class="pre">exponential</span></code> and <code class="docutils literal notranslate"><span class="pre">constant</span></code>). Defaults to
<code class="docutils literal notranslate"><span class="pre">linear</span></code>.</p></li>
<li><p><strong>lr_alpha_final</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – The final alpha learning rate that is
achieved at the end of the training. Defaults to <code class="docutils literal notranslate"><span class="pre">1e-10</span></code>.</p></li>
<li><p><strong>lr_decay_type</strong> – The learning rate decay type that is used (options
are: <code class="docutils literal notranslate"><span class="pre">linear</span></code> and <code class="docutils literal notranslate"><span class="pre">exponential</span></code> and <code class="docutils literal notranslate"><span class="pre">constant</span></code>). Defaults to
<code class="docutils literal notranslate"><span class="pre">linear</span></code>.Can be overridden by the specific learning rate decay types.</p></li>
<li><p><strong>lr_a_decay_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The learning rate decay type that is used for
the actor learning rate (options are: <code class="docutils literal notranslate"><span class="pre">linear</span></code> and <code class="docutils literal notranslate"><span class="pre">exponential</span></code> and
<code class="docutils literal notranslate"><span class="pre">constant</span></code>). If not specified, the general learning rate decay type is used.</p></li>
<li><p><strong>lr_c_decay_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The learning rate decay type that is used for
the critic learning rate (options are: <code class="docutils literal notranslate"><span class="pre">linear</span></code> and <code class="docutils literal notranslate"><span class="pre">exponential</span></code> and
<code class="docutils literal notranslate"><span class="pre">constant</span></code>). If not specified, the general learning rate decay type is used.</p></li>
<li><p><strong>lr_alpha_decay_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The learning rate decay type that is used
for the alpha learning rate (options are: <code class="docutils literal notranslate"><span class="pre">linear</span></code> and <code class="docutils literal notranslate"><span class="pre">exponential</span></code>
and <code class="docutils literal notranslate"><span class="pre">constant</span></code>). If not specified, the general learning rate decay type is used.</p></li>
<li><p><strong>lr_decay_ref</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The reference variable that is used for decaying
the learning rate (options: <code class="docutils literal notranslate"><span class="pre">epoch</span></code> and <code class="docutils literal notranslate"><span class="pre">step</span></code>). Defaults to <code class="docutils literal notranslate"><span class="pre">epoch</span></code>.</p></li>
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Minibatch size for SGD. Defaults to <code class="docutils literal notranslate"><span class="pre">256</span></code>.</p></li>
<li><p><strong>replay_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Maximum length of replay buffer. Defaults to
<code class="docutils literal notranslate"><span class="pre">1e6</span></code>.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a>) – Seed for random number generators. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – The device the networks are placed on (options: <code class="docutils literal notranslate"><span class="pre">cpu</span></code>,
<code class="docutils literal notranslate"><span class="pre">gpu</span></code>, <code class="docutils literal notranslate"><span class="pre">gpu:0</span></code>, <code class="docutils literal notranslate"><span class="pre">gpu:1</span></code>, etc.). Defaults to <code class="docutils literal notranslate"><span class="pre">cpu</span></code>.</p></li>
<li><p><strong>logger_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><em>dict</em></a><em>, </em><em>optional</em>) – Keyword args for EpochLogger.</p></li>
<li><p><strong>save_freq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – How often (in terms of gap between epochs) to save
the current policy and value function.</p></li>
<li><p><strong>start_policy</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a>) – Path of a already trained policy to use as the starting
point for the training. By default a new policy is created.</p></li>
<li><p><strong>export</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a>) – Whether you want to export the model in the <code class="docutils literal notranslate"><span class="pre">SavedModel</span></code> format
such that it can be deployed to hardware. By default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>tuple containing:</p>
<blockquote>
<div><ul class="simple">
<li><p>policy (<a class="reference internal" href="../../autoapi/stable_learning_control/algos/tf2/sac/index.html#stable_learning_control.algos.tf2.sac.SAC" title="stable_learning_control.algos.tf2.sac.SAC"><code class="xref py py-class docutils literal notranslate"><span class="pre">SAC</span></code></a>): The trained actor-critic policy.</p></li>
<li><p>replay_buffer (union[<a class="reference internal" href="../../autoapi/stable_learning_control/algos/common/buffers/index.html#stable_learning_control.algos.common.buffers.ReplayBuffer" title="stable_learning_control.algos.common.buffers.ReplayBuffer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReplayBuffer</span></code></a>, <a class="reference internal" href="../../autoapi/stable_learning_control/algos/common/buffers/index.html#stable_learning_control.algos.common.buffers.FiniteHorizonReplayBuffer" title="stable_learning_control.algos.common.buffers.FiniteHorizonReplayBuffer"><code class="xref py py-class docutils literal notranslate"><span class="pre">FiniteHorizonReplayBuffer</span></code></a>]):
The replay buffer used during training.</p></li>
</ul>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a>)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="saved-model-contents-tensorflow-version">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Saved Model Contents: TensorFlow Version</a><a class="headerlink" href="#saved-model-contents-tensorflow-version" title="Permalink to this heading"></a></h3>
<p>The TensorFlow version of the SAC algorithm is implemented by subclassing the <code class="xref py py-class docutils literal notranslate"><span class="pre">tf.nn.Model</span></code>
class. As a result, both the full model and the current model weights are saved. The full model
can be found in the <code class="docutils literal notranslate"><span class="pre">saved_model.pb</span></code> file, while the current weights checkpoints are in the
<code class="docutils literal notranslate"><span class="pre">tf_safe/weights_checkpoint*</span></code> file. For an example of using these two methods, see
<a class="reference internal" href="../saving_and_loading.html#saving-and-loading"><span class="std std-ref">Experiment Outputs</span></a> or the <a class="reference external" href="https://www.tensorflow.org/tutorials/keras/save_and_load">TensorFlow documentation</a>.</p>
</section>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">References</a><a class="headerlink" href="#references" title="Permalink to this heading"></a></h2>
<section id="relevant-papers">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Relevant Papers</a><a class="headerlink" href="#relevant-papers" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>, Haarnoja et al, 2018</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1812.05905">Soft Actor-Critic: Algorithms and Applications</a>, Haarnoja et al, 2019</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1812.11103">Learning to Walk via Deep Reinforcement Learning</a>, Haarnoja et al, 2018</p></li>
</ul>
</section>
<section id="other-public-implementations">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Other Public Implementations</a><a class="headerlink" href="#other-public-implementations" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/haarnoja/sac">SAC release repo</a> (original “official” codebase).</p></li>
<li><p><a class="reference external" href="https://github.com/rail-berkeley/softlearning">Softlearning repo</a> (current “official” codebase).</p></li>
<li><p><a class="reference external" href="https://github.com/denisyarats/pytorch_sac">Yarats and Kostrikov repo</a></p></li>
<li><p><a class="reference external" href="https://github.com/openai/spinningup/tree/master/spinup">SpinningUp repo</a> (The version our version was based on).</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="latc.html" class="btn btn-neutral float-left" title="Lyapunov Actor-Twin Critic (LATC)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../running.html" class="btn btn-neutral float-right" title="Running Experiments" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Rick Staa.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>