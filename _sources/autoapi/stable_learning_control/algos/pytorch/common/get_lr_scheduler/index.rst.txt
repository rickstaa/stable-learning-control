stable_learning_control.algos.pytorch.common.get_lr_scheduler
=============================================================

.. py:module:: stable_learning_control.algos.pytorch.common.get_lr_scheduler

.. autoapi-nested-parse::

   Contains functions used for creating Pytorch learning rate schedulers.



Classes
-------

.. autoapisummary::

   stable_learning_control.algos.pytorch.common.get_lr_scheduler.ConstantLRScheduler


Functions
---------

.. autoapisummary::

   stable_learning_control.algos.pytorch.common.get_lr_scheduler.get_exponential_decay_rate
   stable_learning_control.algos.pytorch.common.get_lr_scheduler.get_linear_decay_rate
   stable_learning_control.algos.pytorch.common.get_lr_scheduler.get_lr_scheduler
   stable_learning_control.algos.pytorch.common.get_lr_scheduler.estimate_step_learning_rate


Module Contents
---------------

.. py:class:: ConstantLRScheduler(optimizer)

   Bases: :py:obj:`torch.optim.lr_scheduler.LambdaLR`


   A learning rate scheduler that keeps the learning rate constant.

   Initialize the constant learning rate scheduler.

   :param optimizer: The wrapped optimizer.
   :type optimizer: :class:`torch.optim.Optimizer`


.. py:function:: get_exponential_decay_rate(lr_start, lr_final, steps)

   Calculates the exponential decay rate needed to go from a initial learning rate
   to a final learning rate in N steps.

   :param lr_start: The starting learning rate.
   :type lr_start: float
   :param lr_final: The final learning rate.
   :type lr_final: float
   :param steps: The number of steps.
   :type steps: int

   :returns: The exponential decay rate (high precision).
   :rtype: decimal.Decimal


.. py:function:: get_linear_decay_rate(lr_init, lr_final, steps)

   Returns a linear decay factor (G) that enables a learning rate to transition
   from an initial value (`lr_init`) at step 0 to a final value (`lr_final`) at a
   specified step (N). This decay factor is compatible with the
   :class:`torch.optim.lr_scheduler.LambdaLR` scheduler. The decay factor is calculated
   using the following formula:

   .. math::
       lr_{terminal} = lr_{init} * (1.0 - G \cdot step)

   :param lr_init: The initial learning rate.
   :type lr_init: float
   :param lr_final: The final learning rate you want to achieve.
   :type lr_final: float
   :param steps: The number of steps/epochs over which the learning rate should
                 decay. This is equal to epochs -1.
   :type steps: int

   :returns: Linear learning rate decay factor (G).
   :rtype: decimal.Decimal


.. py:function:: get_lr_scheduler(optimizer, decaying_lr_type, lr_start, lr_final, steps)

   Creates a learning rate scheduler.

   :param optimizer: Wrapped optimizer.
   :type optimizer: torch.optim.Adam
   :param decaying_lr_type: The learning rate decay type that is used
                            (options are: ``linear`` and ``exponential`` and ``constant``).
   :type decaying_lr_type: str
   :param lr_start: Initial learning rate.
   :type lr_start: float
   :param lr_final: Final learning rate.
   :type lr_final: float
   :param steps: Number of steps/epochs used in the training. This
                 includes the starting step/epoch.
   :type steps: int, optional

   :returns: A learning rate scheduler object.
   :rtype: :obj:`torch.optim.lr_scheduler`

   .. seealso::
       See the :torch:`pytorch <docs/stable/optim.html>` documentation on how to
       implement other decay options.


.. py:function:: estimate_step_learning_rate(lr_scheduler, lr_start, lr_final, update_after, total_steps, step)

   Estimates the learning rate at a given step.

   This function estimates the learning rate for a specific training step. It differs
   from the `get_last_lr` method of the learning rate scheduler, which returns the
   learning rate at the last scheduler step, not necessarily the current training step.

   :param lr_scheduler: The learning rate scheduler.
   :type lr_scheduler: torch.optim.lr_scheduler
   :param lr_start: The initial learning rate.
   :type lr_start: float
   :param update_after: The step number after which the learning rate should start
                        decreasing.
   :type update_after: int
   :param lr_final: The final learning rate.
   :type lr_final: float
   :param total_steps: The total number of steps/epochs in the training process.
                       Excludes the initial step.
   :type total_steps: int
   :param step: The current step number. Excludes the initial step.
   :type step: int

   :returns: The learning rate at the given step.
   :rtype: float


