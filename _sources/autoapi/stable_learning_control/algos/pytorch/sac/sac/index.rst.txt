stable_learning_control.algos.pytorch.sac.sac
=============================================

.. py:module:: stable_learning_control.algos.pytorch.sac.sac

.. autoapi-nested-parse::

   Soft Actor-Critic (SAC) algorithm.

   This module contains the Pytorch implementation of the SAC algorithm of
   `Haarnoja et al. 2019 <https://arxiv.org/abs/1812.05905>`_.

   .. note::
       Code Conventions:
           - We use a `_` suffix to distinguish the next state from the current state.
           - We use a `targ` suffix to distinguish actions/values coming from the target
             network.



Attributes
----------

.. autoapisummary::

   stable_learning_control.algos.pytorch.sac.sac.SCALE_LAMBDA_MIN_MAX
   stable_learning_control.algos.pytorch.sac.sac.SCALE_ALPHA_MIN_MAX
   stable_learning_control.algos.pytorch.sac.sac.STD_OUT_LOG_VARS_DEFAULT
   stable_learning_control.algos.pytorch.sac.sac.VALID_DECAY_TYPES
   stable_learning_control.algos.pytorch.sac.sac.VALID_DECAY_REFERENCES
   stable_learning_control.algos.pytorch.sac.sac.DEFAULT_DECAY_TYPE
   stable_learning_control.algos.pytorch.sac.sac.DEFAULT_DECAY_REFERENCE
   stable_learning_control.algos.pytorch.sac.sac.parser


Classes
-------

.. autoapisummary::

   stable_learning_control.algos.pytorch.sac.sac.SAC


Functions
---------

.. autoapisummary::

   stable_learning_control.algos.pytorch.sac.sac.validate_args
   stable_learning_control.algos.pytorch.sac.sac.sac


Module Contents
---------------

.. py:data:: SCALE_LAMBDA_MIN_MAX
   :value: (0.0, 1.0)


.. py:data:: SCALE_ALPHA_MIN_MAX

.. py:data:: STD_OUT_LOG_VARS_DEFAULT
   :value: ['Epoch', 'TotalEnvInteracts', 'AverageEpRet', 'AverageTestEpRet', 'AverageTestEpLen',...


.. py:data:: VALID_DECAY_TYPES
   :value: ['linear', 'exponential', 'constant']


.. py:data:: VALID_DECAY_REFERENCES
   :value: ['step', 'epoch']


.. py:data:: DEFAULT_DECAY_TYPE
   :value: 'linear'


.. py:data:: DEFAULT_DECAY_REFERENCE
   :value: 'epoch'


.. py:class:: SAC(env, actor_critic=None, ac_kwargs=dict(hidden_sizes={'actor': [256] * 2, 'critic': [256] * 2}, activation={'actor': nn.ReLU, 'critic': nn.ReLU}, output_activation={'actor': nn.ReLU, 'critic': nn.Identity}), opt_type='maximize', alpha=0.99, gamma=0.99, polyak=0.995, target_entropy=None, adaptive_temperature=True, lr_a=0.0001, lr_c=0.0003, lr_alpha=0.0001, device='cpu')

   Bases: :py:obj:`torch.nn.Module`


   The Soft Actor Critic algorithm.

   .. attribute:: ac

      The soft actor critic module.

      :type: torch.nn.Module

   .. attribute:: ac_

      The target soft actor critic module.

      :type: torch.nn.Module

   .. attribute:: log_alpha

      The temperature Lagrance multiplier.

      :type: torch.Tensor

   Initialise the SAC algorithm.

   :param env: The gymnasium environment the SAC is training in. This is
               used to retrieve the activation and observation space dimensions. This
               is used while creating the network sizes. The environment must satisfy
               the gymnasium API.
   :type env: :obj:`gym.env`
   :param actor_critic: The constructor method for a
                        Torch Module with an ``act`` method, a ``pi`` module and several
                        ``Q`` or ``L`` modules. The ``act`` method and ``pi`` module should
                        accept batches of observations as inputs, and the ``Q*`` and ``L``
                        modules should accept a batch of observations and a batch of actions as
                        inputs. When called, these modules should return:

                        ===========  ================  ======================================
                        Call         Output Shape      Description
                        ===========  ================  ======================================
                        ``act``      (batch, act_dim)  | Numpy array of actions for each
                                                       | observation.
                        ``Q*/L``     (batch,)          | Tensor containing one current estimate
                                                       | of ``Q*/L`` for the provided
                                                       | observations and actions. (Critical:
                                                       | make sure to flatten this!)
                        ===========  ================  ======================================

                        Calling ``pi`` should return:

                        ===========  ================  ======================================
                        Symbol       Shape             Description
                        ===========  ================  ======================================
                        ``a``        (batch, act_dim)  | Tensor containing actions from policy
                                                       | given observations.
                        ``logp_pi``  (batch,)          | Tensor containing log probabilities of
                                                       | actions in ``a``. Importantly:
                                                       | gradients should be able to flow back
                                                       | into ``a``.
                        ===========  ================  ======================================

                        Defaults to
                        :class:`~stable_learning_control.algos.pytorch.policies.soft_actor_critic.SoftActorCritic`
   :type actor_critic: torch.nn.Module, optional
   :param ac_kwargs: Any kwargs appropriate for the ActorCritic
                     object you provided to SAC. Defaults to:

                     =======================  ============================================
                     Kwarg                    Value
                     =======================  ============================================
                     ``hidden_sizes_actor``    ``64 x 2``
                     ``hidden_sizes_critic``   ``128 x 2``
                     ``activation``            :class:`torch.nn.ReLU`
                     ``output_activation``     :class:`torch.nn.ReLU`
                     =======================  ============================================
   :type ac_kwargs: dict, optional
   :param opt_type: The optimization type you want to use. Options
                    ``maximize`` and ``minimize``. Defaults to ``maximize``.
   :type opt_type: str, optional
   :param alpha: Entropy regularization coefficient (Equivalent to
                 inverse of reward scale in the original SAC paper). Defaults to
                 ``0.99``.
   :type alpha: float, optional
   :param gamma: Discount factor. (Always between 0 and 1.).
                 Defaults to ``0.99``.
   :type gamma: float, optional
   :param polyak: Interpolation factor in polyak averaging for
                  target networks. Target networks are updated towards main networks
                  according to:

                  .. math:: \theta_{\text{targ}} \leftarrow
                      \rho \theta_{\text{targ}} + (1-\rho) \theta

                  where :math:`\rho` is polyak (Always between 0 and 1, usually close to
                  1.). In some papers :math:`\rho` is defined as (1 - :math:`\tau`)
                  where :math:`\tau` is the soft replacement factor. Defaults to
                  ``0.995``.
   :type polyak: float, optional
   :param target_entropy: Initial target entropy used while learning
                          the entropy temperature (alpha). Defaults to the
                          maximum information (bits) contained in action space. This can be
                          calculated according to :

                          .. math::
                              -{\prod }_{i=0}^{n}action\_di{m}_{i}\phantom{\rule{0ex}{0ex}}
   :type target_entropy: float, optional
   :param adaptive_temperature: Enabled Automating Entropy Adjustment
                                for maximum Entropy RL_learning.
   :type adaptive_temperature: bool, optional
   :param lr_a: Learning rate used for the actor. Defaults to
                ``1e-4``.
   :type lr_a: float, optional
   :param lr_c: Learning rate used for the (Soft) critic.
                Defaults to ``1e-4``.
   :type lr_c: float, optional
   :param lr_alpha: Learning rate used for the entropy temperature.
                    Defaults to ``1e-4``.
   :type lr_alpha: float, optional
   :param device: The device the networks are placed on (options:
                  ``cpu``, ``gpu``, ``gpu:0``, ``gpu:1``, etc.). Defaults to ``cpu``.
   :type device: str, optional


   .. py:attribute:: _setup_kwargs


   .. py:attribute:: _act_dim


   .. py:attribute:: _obs_dim


   .. py:attribute:: _device


   .. py:attribute:: _adaptive_temperature


   .. py:attribute:: _opt_type


   .. py:attribute:: _polyak


   .. py:attribute:: _gamma


   .. py:attribute:: _lr_a


   .. py:attribute:: _lr_c


   .. py:attribute:: log_alpha


   .. py:attribute:: actor_critic


   .. py:attribute:: ac


   .. py:attribute:: ac_targ


   .. py:attribute:: _pi_optimizer


   .. py:attribute:: _pi_params


   .. py:attribute:: _c_params


   .. py:attribute:: _c_optimizer


   .. py:method:: forward(s, deterministic=False)

      Wrapper around the :meth:`get_action` method that enables users to also
      receive actions directly by invoking ``SAC(observations)``.

      :param s: The current state.
      :type s: numpy.ndarray
      :param deterministic: Whether to return a deterministic action.
                            Defaults to ``False``.
      :type deterministic: bool, optional

      :returns: The current action.
      :rtype: numpy.ndarray



   .. py:method:: get_action(s, deterministic=False)

      Returns the current action of the policy.

      :param s: The current state.
      :type s: numpy.ndarray
      :param deterministic: Whether to return a deterministic action.
                            Defaults to ``False``.
      :type deterministic: bool, optional

      :returns: The current action.
      :rtype: numpy.ndarray



   .. py:method:: update(data)

      Update the actor critic network using stochastic gradient descent.

      :param data: Dictionary containing a batch of experiences.
      :type data: dict



   .. py:method:: save(path)

      Can be used to save the current model state.

      :param path: The path where you want to save the policy.
      :type path: str

      :raises Exception: Raises an exception if something goes wrong during saving.



   .. py:method:: restore(path, restore_lagrance_multipliers=False)

      Restores a already trained policy. Used for transfer learning.

      :param path: The path where the model :attr:`state_dict` of the policy is
                   found.
      :type path: str
      :param restore_lagrance_multipliers: Whether you want to restore
                                           the Lagrance multipliers. By fault ``False``.
      :type restore_lagrance_multipliers: bool, optional

      :raises Exception: Raises an exception if something goes wrong during loading.



   .. py:method:: export(path)
      :abstractmethod:


      Can be used to export the model as a ``TorchScript`` such that it can be
      deployed to hardware.

      :param path: The path where you want to export the policy too.
      :type path: str

      :raises NotImplementedError: Raised until the feature is fixed on the upstream.



   .. py:method:: load_state_dict(state_dict, restore_lagrance_multipliers=True)

      Copies parameters and buffers from :attr:`state_dict` into
      this module and its descendants.

      :param state_dict: a dict containing parameters and
                         persistent buffers.
      :type state_dict: dict
      :param restore_lagrance_multipliers: Whether you want to restore
                                           the Lagrance multipliers. By fault ``True``.
      :type restore_lagrance_multipliers: bool, optional



   .. py:method:: state_dict()

      Simple wrapper around the :meth:`torch.nn.Module.state_dict` method that
      saves the current class name. This is used to enable easy loading of the model.



   .. py:method:: bound_lr(lr_a_final=None, lr_c_final=None, lr_alpha_final=None)

      Function that can be used to make sure the learning rate doesn't go beyond
      a lower bound.

      :param lr_a_final: The lower bound for the actor learning rate.
                         Defaults to ``None``.
      :type lr_a_final: float, optional
      :param lr_c_final: The lower bound for the critic learning rate.
                         Defaults to ``None``.
      :type lr_c_final: float, optional
      :param lr_alpha_final: The lower bound for the alpha Lagrance
                             multiplier learning rate. Defaults to ``None``.
      :type lr_alpha_final: float, optional



   .. py:method:: _update_targets()

      Updates the target networks based on a Exponential moving average
      (Polyak averaging).



   .. py:method:: _set_learning_rates(lr_a=None, lr_c=None, lr_alpha=None)

      Can be used to manually adjusts the learning rates of the optimizers.

      :param lr_a: The learning rate of the actor optimizer. Defaults
                   to ``None``.
      :type lr_a: float, optional
      :param lr_c: The learning rate of the (soft) Critic. Defaults
                   to ``None``.
      :type lr_c: float, optional
      :param lr_alpha: The learning rate of the temperature optimizer.
                       Defaults to ``None``.
      :type lr_alpha: float, optional



   .. py:property:: alpha
      Property used to clip :attr:`alpha` to be equal or bigger than ``0.0`` to
      prevent it from becoming nan when :attr:`log_alpha` becomes ``-inf``. For
      :attr:`alpha` no upper bound is used.


   .. py:property:: target_entropy
      The target entropy used while learning the entropy temperature
      :attr:`alpha`.


   .. py:property:: device
      ``cpu``, ``gpu``, ``gpu:0``,
      ``gpu:1``, etc.).

      :type: The device the networks are placed on (options


.. py:function:: validate_args(**kwargs)

   Checks if the input arguments have valid values.

   :raises ValueError: If a value is invalid.


.. py:function:: sac(env_fn, actor_critic=None, ac_kwargs=dict(hidden_sizes={'actor': [256] * 2, 'critic': [256] * 2}, activation={'actor': nn.ReLU, 'critic': nn.ReLU}, output_activation={'actor': nn.ReLU, 'critic': nn.Identity}), opt_type='maximize', max_ep_len=None, epochs=100, steps_per_epoch=2048, start_steps=0, update_every=100, update_after=1000, steps_per_update=100, num_test_episodes=10, alpha=0.99, gamma=0.99, polyak=0.995, target_entropy=None, adaptive_temperature=True, lr_a=0.0001, lr_c=0.0003, lr_alpha=0.0001, lr_a_final=1e-10, lr_c_final=1e-10, lr_alpha_final=1e-10, lr_decay_type=DEFAULT_DECAY_TYPE, lr_a_decay_type=None, lr_c_decay_type=None, lr_alpha_decay_type=None, lr_decay_ref=DEFAULT_DECAY_REFERENCE, batch_size=256, replay_size=int(1000000.0), seed=None, device='cpu', logger_kwargs=dict(), save_freq=1, start_policy=None, export=False)

   Trains the SAC algorithm in a given environment.

   :param env_fn: A function which creates a copy of the environment. The environment
                  must satisfy the gymnasium API.
   :param actor_critic: The constructor method for a
                        Torch Module with an ``act`` method, a ``pi`` module and several
                        ``Q`` or ``L`` modules. The ``act`` method and ``pi`` module should
                        accept batches of observations as inputs, and the ``Q*`` and ``L``
                        modules should accept a batch of observations and a batch of actions as
                        inputs. When called, these modules should return:

                        ===========  ================  ======================================
                        Call         Output Shape      Description
                        ===========  ================  ======================================
                        ``act``      (batch, act_dim)   | Numpy array of actions for each
                                                        | observation.
                        ``Q*/L``     (batch,)           | Tensor containing one current estimate
                                                        | of ``Q*/L`` for the provided
                                                        | observations and actions. (Critical:
                                                        | make sure to flatten this!)
                        ===========  ================  ======================================

                        Calling ``pi`` should return:

                        ===========  ================  ======================================
                        Symbol       Shape             Description
                        ===========  ================  ======================================
                        ``a``        (batch, act_dim)   | Tensor containing actions from policy
                                                        | given observations.
                        ``logp_pi``  (batch,)           | Tensor containing log probabilities of
                                                        | actions in ``a``. Importantly:
                                                        | gradients should be able to flow back
                                                        | into ``a``.
                        ===========  ================  ======================================

                        Defaults to
                        :class:`~stable_learning_control.algos.pytorch.policies.soft_actor_critic.SoftActorCritic`
   :type actor_critic: torch.nn.Module, optional
   :param ac_kwargs: Any kwargs appropriate for the ActorCritic
                     object you provided to SAC. Defaults to:

                     =======================  ============================================
                     Kwarg                    Value
                     =======================  ============================================
                     ``hidden_sizes_actor``    ``64 x 2``
                     ``hidden_sizes_critic``   ``128 x 2``
                     ``activation``            :class:`torch.nn.ReLU`
                     ``output_activation``     :class:`torch.nn.ReLU`
                     =======================  ============================================
   :type ac_kwargs: dict, optional
   :param opt_type: The optimization type you want to use. Options
                    ``maximize`` and ``minimize``. Defaults to ``maximize``.
   :type opt_type: str, optional
   :param max_ep_len: Maximum length of trajectory / episode /
                      rollout. Defaults to the environment maximum.
   :type max_ep_len: int, optional
   :param epochs: Number of epochs to run and train agent. Defaults
                  to ``100``.
   :type epochs: int, optional
   :param steps_per_epoch: Number of steps of interaction
                           (state-action pairs) for the agent and the environment in each epoch.
                           Defaults to ``2048``.
   :type steps_per_epoch: int, optional
   :param start_steps: Number of steps for uniform-random action
                       selection, before running real policy. Helps exploration. Defaults to
                       ``0``.
   :type start_steps: int, optional
   :param update_every: Number of env interactions that should elapse
                        between gradient descent updates. Defaults to ``100``.
   :type update_every: int, optional
   :param update_after: Number of env interactions to collect before
                        starting to do gradient descent updates. Ensures replay buffer
                        is full enough for useful updates. Defaults to ``1000``.
   :type update_after: int, optional
   :param steps_per_update: Number of gradient descent steps that are
                            performed for each gradient descent update. This determines the ratio of
                            env steps to gradient steps (i.e. :obj:`update_every`/
                            :obj:`steps_per_update`). Defaults to ``100``.
   :type steps_per_update: int, optional
   :param num_test_episodes: Number of episodes used to test the
                             deterministic policy at the end of each epoch. This is used for logging
                             the performance. Defaults to ``10``.
   :type num_test_episodes: int, optional
   :param alpha: Entropy regularization coefficient (Equivalent to
                 inverse of reward scale in the original SAC paper). Defaults to
                 ``0.99``.
   :type alpha: float, optional
   :param gamma: Discount factor. (Always between 0 and 1.).
                 Defaults to ``0.99``.
   :type gamma: float, optional
   :param polyak: Interpolation factor in polyak averaging for
                  target networks. Target networks are updated towards main networks
                  according to:

                  .. math:: \theta_{\text{targ}} \leftarrow
                      \rho \theta_{\text{targ}} + (1-\rho) \theta

                  where :math:`\rho` is polyak (Always between 0 and 1, usually close to 1.).
                  In some papers :math:`\rho` is defined as (1 - :math:`\tau`) where
                  :math:`\tau` is the soft replacement factor. Defaults to ``0.995``.
   :type polyak: float, optional
   :param target_entropy: Initial target entropy used while learning
                          the entropy temperature (alpha). Defaults to the
                          maximum information (bits) contained in action space. This can be
                          calculated according to :

                          .. math::
                              -{\prod }_{i=0}^{n}action\_di{m}_{i}\phantom{\rule{0ex}{0ex}}
   :type target_entropy: float, optional
   :param adaptive_temperature: Enabled Automating Entropy Adjustment
                                for maximum Entropy RL_learning.
   :type adaptive_temperature: bool, optional
   :param lr_a: Learning rate used for the actor. Defaults to
                ``1e-4``.
   :type lr_a: float, optional
   :param lr_c: Learning rate used for the (soft) critic. Defaults to
                ``1e-4``.
   :type lr_c: float, optional
   :param lr_alpha: Learning rate used for the entropy temperature.
                    Defaults to ``1e-4``.
   :type lr_alpha: float, optional
   :param lr_a_final: The final actor learning rate that is achieved
                      at the end of the training. Defaults to ``1e-10``.
   :type lr_a_final: float, optional
   :param lr_c_final: The final critic learning rate that is achieved
                      at the end of the training. Defaults to ``1e-10``.
   :type lr_c_final: float, optional
   :param lr_alpha_final: The final alpha learning rate that is
                          achieved at the end of the training. Defaults to ``1e-10``.
   :type lr_alpha_final: float, optional
   :param lr_decay_type: The learning rate decay type that is used (options
                         are: ``linear`` and ``exponential`` and ``constant``). Defaults to
                         ``linear``. Can be overridden by the specific learning rate decay types.
   :type lr_decay_type: str, optional
   :param lr_a_decay_type: The learning rate decay type that is used for
                           the actor learning rate (options are: ``linear`` and ``exponential`` and
                           ``constant``). If not specified, the general learning rate decay type is used.
   :type lr_a_decay_type: str, optional
   :param lr_c_decay_type: The learning rate decay type that is used for
                           the critic learning rate (options are: ``linear`` and ``exponential`` and
                           ``constant``). If not specified, the general learning rate decay type is used.
   :type lr_c_decay_type: str, optional
   :param lr_alpha_decay_type: The learning rate decay type that is used
                               for the alpha learning rate (options are: ``linear`` and ``exponential``
                               and ``constant``). If not specified, the general learning rate decay type is used.
   :type lr_alpha_decay_type: str, optional
   :param lr_decay_ref: The reference variable that is used for decaying
                        the learning rate (options: ``epoch`` and ``step``). Defaults to ``epoch``.
   :type lr_decay_ref: str, optional
   :param batch_size: Minibatch size for SGD. Defaults to ``256``.
   :type batch_size: int, optional
   :param replay_size: Maximum length of replay buffer. Defaults to
                       ``1e6``.
   :type replay_size: int, optional
   :param seed: Seed for random number generators. Defaults to ``None``.
   :type seed: int
   :param device: The device the networks are placed on (options: ``cpu``,
                  ``gpu``, ``gpu:0``, ``gpu:1``, etc.). Defaults to ``cpu``.
   :type device: str, optional
   :param logger_kwargs: Keyword args for EpochLogger.
   :type logger_kwargs: dict, optional
   :param save_freq: How often (in terms of gap between epochs) to save
                     the current policy and value function.
   :type save_freq: int, optional
   :param start_policy: Path of a already trained policy to use as the starting
                        point for the training. By default a new policy is created.
   :type start_policy: str
   :param export: Whether you want to export the model as a ``TorchScript`` such
                  that it can be deployed on hardware. By default ``False``.
   :type export: bool

   :returns:

             tuple containing:

                 -   policy (:class:`SAC`): The trained actor-critic policy.
                 -   replay_buffer (union[:class:`~stable_learning_control.algos.common.buffers.ReplayBuffer`, :class:`~stable_learning_control.algos.common.buffers.FiniteHorizonReplayBuffer`]):
                     The replay buffer used during training.
   :rtype: (tuple)


.. py:data:: parser

