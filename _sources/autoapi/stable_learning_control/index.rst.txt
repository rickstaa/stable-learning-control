stable_learning_control
=======================

.. py:module:: stable_learning_control

.. autoapi-nested-parse::

   Module that Initialises the stable_learning_control package.



Subpackages
-----------

.. toctree::
   :maxdepth: 1

   /autoapi/stable_learning_control/algos/index
   /autoapi/stable_learning_control/common/index
   /autoapi/stable_learning_control/disturbers/index
   /autoapi/stable_learning_control/utils/index


Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/stable_learning_control/run/index
   /autoapi/stable_learning_control/user_config/index
   /autoapi/stable_learning_control/version/index


Attributes
----------

.. autoapisummary::

   stable_learning_control.__version__
   stable_learning_control.__version_tuple__


Functions
---------

.. autoapisummary::

   stable_learning_control.lac_pytorch
   stable_learning_control.latc_pytorch
   stable_learning_control.sac_pytorch
   stable_learning_control.tf_installed


Package Contents
----------------

.. py:function:: lac_pytorch(env_fn, actor_critic=None, ac_kwargs=dict(hidden_sizes={'actor': [256] * 2, 'critic': [256] * 2}, activation=nn.ReLU, output_activation=nn.ReLU), opt_type='minimize', max_ep_len=None, epochs=100, steps_per_epoch=2048, start_steps=0, update_every=100, update_after=1000, steps_per_update=100, num_test_episodes=10, alpha=0.99, alpha3=0.2, labda=0.99, gamma=0.99, polyak=0.995, target_entropy=None, adaptive_temperature=True, lr_a=0.0001, lr_c=0.0003, lr_alpha=0.0001, lr_labda=0.0003, lr_a_final=1e-10, lr_c_final=1e-10, lr_alpha_final=1e-10, lr_labda_final=1e-10, lr_decay_type=DEFAULT_DECAY_TYPE, lr_a_decay_type=None, lr_c_decay_type=None, lr_alpha_decay_type=None, lr_labda_decay_type=None, lr_decay_ref=DEFAULT_DECAY_REFERENCE, batch_size=256, replay_size=int(1000000.0), horizon_length=0, seed=None, device='cpu', logger_kwargs=dict(), save_freq=1, start_policy=None, export=False)

   Trains the LAC algorithm in a given environment.

   :param env_fn: A function which creates a copy of the environment. The environment
                  must satisfy the gymnasium API.
   :param actor_critic: The constructor method for a
                        Torch Module with an ``act`` method, a ``pi`` module and several
                        ``Q`` or ``L`` modules. The ``act`` method and ``pi`` module should
                        accept batches of observations as inputs, and the ``Q*`` and ``L``
                        modules should accept a batch of observations and a batch of actions as
                        inputs. When called, these modules should return:

                        ===========  ================  ======================================
                        Call         Output Shape      Description
                        ===========  ================  ======================================
                        ``act``      (batch, act_dim)   | Numpy array of actions for each
                                                        | observation.
                        ``Q*/L``     (batch,)           | Tensor containing one current estimate
                                                        | of ``Q*/L`` for the provided
                                                        | observations and actions. (Critical:
                                                        | make sure to flatten this!)
                        ===========  ================  ======================================

                        Calling ``pi`` should return:

                        ===========  ================  ======================================
                        Symbol       Shape             Description
                        ===========  ================  ======================================
                        ``a``        (batch, act_dim)   | Tensor containing actions from policy
                                                        | given observations.
                        ``logp_pi``  (batch,)           | Tensor containing log probabilities of
                                                        | actions in ``a``. Importantly:
                                                        | gradients should be able to flow back
                                                        | into ``a``.
                        ===========  ================  ======================================

                        Defaults to
                        :class:`~stable_learning_control.algos.pytorch.policies.lyapunov_actor_critic.LyapunovActorCritic`
   :type actor_critic: torch.nn.Module, optional
   :param ac_kwargs: Any kwargs appropriate for the ActorCritic
                     object you provided to LAC. Defaults to:

                     =======================  ============================================
                     Kwarg                    Value
                     =======================  ============================================
                     ``hidden_sizes_actor``    ``256 x 2``
                     ``hidden_sizes_critic``   ``256 x 2``
                     ``activation``            :class:`torch.nn.ReLU`
                     ``output_activation``     :class:`torch.nn.ReLU`
                     =======================  ============================================
   :type ac_kwargs: dict, optional
   :param opt_type: The optimization type you want to use. Options
                    ``maximize`` and ``minimize``. Defaults to ``maximize``.
   :type opt_type: str, optional
   :param max_ep_len: Maximum length of trajectory / episode /
                      rollout. Defaults to the environment maximum.
   :type max_ep_len: int, optional
   :param epochs: Number of epochs to run and train agent. Defaults
                  to ``100``.
   :type epochs: int, optional
   :param steps_per_epoch: Number of steps of interaction
                           (state-action pairs) for the agent and the environment in each epoch.
                           Defaults to ``2048``.
   :type steps_per_epoch: int, optional
   :param start_steps: Number of steps for uniform-random action
                       selection, before running real policy. Helps exploration. Defaults to
                       ``0``.
   :type start_steps: int, optional
   :param update_every: Number of env interactions that should elapse
                        between gradient descent updates. Defaults to ``100``.
   :type update_every: int, optional
   :param update_after: Number of env interactions to collect before
                        starting to do gradient descent updates. Ensures replay buffer
                        is full enough for useful updates. Defaults to ``1000``.
   :type update_after: int, optional
   :param steps_per_update: Number of gradient descent steps that are
                            performed for each gradient descent update. This determines the ratio of
                            env steps to gradient steps (i.e. :obj:`update_every`/
                            :obj:`steps_per_update`). Defaults to ``100``.
   :type steps_per_update: int, optional
   :param num_test_episodes: Number of episodes used to test the
                             deterministic policy at the end of each epoch. This is used for logging
                             the performance. Defaults to ``10``.
   :type num_test_episodes: int, optional
   :param alpha: Entropy regularization coefficient (Equivalent to
                 inverse of reward scale in the original SAC paper). Defaults to
                 ``0.99``.
   :type alpha: float, optional
   :param alpha3: The Lyapunov constraint error boundary. Defaults
                  to ``0.2``.
   :type alpha3: float, optional
   :param labda: The Lyapunov Lagrance multiplier. Defaults to
                 ``0.99``.
   :type labda: float, optional
   :param gamma: Discount factor. (Always between 0 and 1.).
                 Defaults to ``0.99``.
   :type gamma: float, optional
   :param polyak: Interpolation factor in polyak averaging for
                  target networks. Target networks are updated towards main networks
                  according to:

                  .. math:: \theta_{\text{targ}} \leftarrow
                      \rho \theta_{\text{targ}} + (1-\rho) \theta

                  where :math:`\rho` is polyak (Always between 0 and 1, usually close to 1.).
                  In some papers :math:`\rho` is defined as (1 - :math:`\tau`) where
                  :math:`\tau` is the soft replacement factor. Defaults to ``0.995``.
   :type polyak: float, optional
   :param target_entropy: Initial target entropy used while learning
                          the entropy temperature (alpha). Defaults to the
                          maximum information (bits) contained in action space. This can be
                          calculated according to :

                          .. math::
                              -{\prod }_{i=0}^{n}action\_di{m}_{i}\phantom{\rule{0ex}{0ex}}
   :type target_entropy: float, optional
   :param adaptive_temperature: Enabled Automating Entropy Adjustment
                                for maximum Entropy RL_learning.
   :type adaptive_temperature: bool, optional
   :param lr_a: Learning rate used for the actor. Defaults to
                ``1e-4``.
   :type lr_a: float, optional
   :param lr_c: Learning rate used for the (lyapunov) critic.
                Defaults to ``1e-4``.
   :type lr_c: float, optional
   :param lr_alpha: Learning rate used for the entropy temperature.
                    Defaults to ``1e-4``.
   :type lr_alpha: float, optional
   :param lr_labda: Learning rate used for the Lyapunov Lagrance
                    multiplier. Defaults to ``3e-4``.
   :type lr_labda: float, optional
   :param lr_a_final: The final actor learning rate that is achieved
                      at the end of the training. Defaults to ``1e-10``.
   :type lr_a_final: float, optional
   :param lr_c_final: The final critic learning rate that is achieved
                      at the end of the training. Defaults to ``1e-10``.
   :type lr_c_final: float, optional
   :param lr_alpha_final: The final alpha learning rate that is
                          achieved at the end of the training. Defaults to ``1e-10``.
   :type lr_alpha_final: float, optional
   :param lr_labda_final: The final labda learning rate that is
                          achieved at the end of the training. Defaults to ``1e-10``.
   :type lr_labda_final: float, optional
   :param lr_decay_type: The learning rate decay type that is used (options
                         are: ``linear`` and ``exponential`` and ``constant``). Defaults to
                         ``linear``.Can be overridden by the specific learning rate decay types.
   :type lr_decay_type: str, optional
   :param lr_a_decay_type: The learning rate decay type that is used for
                           the actor learning rate (options are: ``linear`` and ``exponential`` and
                           ``constant``). If not specified, the general learning rate decay type is used.
   :type lr_a_decay_type: str, optional
   :param lr_c_decay_type: The learning rate decay type that is used for
                           the critic learning rate (options are: ``linear`` and ``exponential`` and
                           ``constant``). If not specified, the general learning rate decay type is used.
   :type lr_c_decay_type: str, optional
   :param lr_alpha_decay_type: The learning rate decay type that is used
                               for the alpha learning rate (options are: ``linear`` and ``exponential``
                               and ``constant``). If not specified, the general learning rate decay type is used.
   :type lr_alpha_decay_type: str, optional
   :param lr_labda_decay_type: The learning rate decay type that is used
                               for the labda learning rate (options are: ``linear`` and ``exponential``
                               and ``constant``). If not specified, the general learning rate decay type is used.
   :type lr_labda_decay_type: str, optional
   :param lr_decay_ref: The reference variable that is used for decaying
                        the learning rate (options: ``epoch`` and ``step``). Defaults to ``epoch``.
   :type lr_decay_ref: str, optional
   :param batch_size: Minibatch size for SGD. Defaults to ``256``.
   :type batch_size: int, optional
   :param replay_size: Maximum length of replay buffer. Defaults to
                       ``1e6``.
   :type replay_size: int, optional
   :param horizon_length: The length of the finite-horizon used for the
                          Lyapunov Critic target. Defaults to ``0`` meaning the infinite-horizon
                          bellman backup is used.
   :type horizon_length: int, optional
   :param seed: Seed for random number generators. Defaults to ``None``.
   :type seed: int
   :param device: The device the networks are placed on (options: ``cpu``,
                  ``gpu``, ``gpu:0``, ``gpu:1``, etc.). Defaults to ``cpu``.
   :type device: str, optional
   :param logger_kwargs: Keyword args for EpochLogger.
   :type logger_kwargs: dict, optional
   :param save_freq: How often (in terms of gap between epochs) to save
                     the current policy and value function.
   :type save_freq: int, optional
   :param start_policy: Path of a already trained policy to use as the starting
                        point for the training. By default a new policy is created.
   :type start_policy: str
   :param export: Whether you want to export the model as a ``TorchScript`` such
                  that it can be deployed on hardware. By default ``False``.
   :type export: bool

   :returns:

             tuple containing:

                 -   policy (:class:`LAC`): The trained actor-critic policy.
                 -   replay_buffer (union[:class:`~stable_learning_control.algos.pytorch.common.buffers.ReplayBuffer`, :class:`~stable_learning_control.algos.pytorch.common.buffers.FiniteHorizonReplayBuffer`]):
                     The replay buffer used during training.
   :rtype: (tuple)


.. py:function:: latc_pytorch(env_fn, actor_critic=None, *args, **kwargs)

   Trains the LATC algorithm in a given environment.

   :param env_fn: A function which creates a copy of the environment. The environment
                  must satisfy the gymnasium API.
   :param actor_critic: The constructor method for a
                        Torch Module with an ``act`` method, a ``pi`` module and several
                        ``Q`` or ``L`` modules. The ``act`` method and ``pi`` module should
                        accept batches of observations as inputs, and the ``Q*`` and ``L``
                        modules should accept a batch of observations and a batch of actions as
                        inputs. When called, these modules should return:

                        ===========  ================  ======================================
                        Call         Output Shape      Description
                        ===========  ================  ======================================
                        ``act``      (batch, act_dim)   | Numpy array of actions for each
                                                        | observation.
                        ``Q*/L``     (batch,)           | Tensor containing one current estimate
                                                        | of ``Q*/L`` for the provided
                                                        | observations and actions. (Critical:
                                                        | make sure to flatten this!)
                        ===========  ================  ======================================

                        Calling ``pi`` should return:

                        ===========  ================  ======================================
                        Symbol       Shape             Description
                        ===========  ================  ======================================
                        ``a``        (batch, act_dim)   | Tensor containing actions from policy
                                                        | given observations.
                        ``logp_pi``  (batch,)           | Tensor containing log probabilities of
                                                        | actions in ``a``. Importantly:
                                                        | gradients should be able to flow back
                                                        | into ``a``.
                        ===========  ================  ======================================

                        Defaults to
                        :class:`~stable_learning_control.algos.pytorch.policies.lyapunov_actor_twin_critic.LyapunovActorTwinCritic`
   :type actor_critic: torch.nn.Module, optional
   :param \*args: The positional arguments to pass to the :meth:`~stable_learning_control.algos.pytorch.lac.lac.lac` method.
   :param \*\*kwargs: The keyword arguments to pass to the :meth:`~stable_learning_control.algos.pytorch.lac.lac.lac` method.

   .. note::
       Wraps the :func:`~stable_learning_control.algos.pytorch.lac.lac.lac` function so
       that the :class:`~stable_learning_control.algos.pytorch.policies.lyapunov_actor_twin_critic.LyapunovActorTwinCritic`
       architecture is used as the actor critic.


.. py:function:: sac_pytorch(env_fn, actor_critic=None, ac_kwargs=dict(hidden_sizes={'actor': [256] * 2, 'critic': [256] * 2}, activation={'actor': nn.ReLU, 'critic': nn.ReLU}, output_activation={'actor': nn.ReLU, 'critic': nn.Identity}), opt_type='maximize', max_ep_len=None, epochs=100, steps_per_epoch=2048, start_steps=0, update_every=100, update_after=1000, steps_per_update=100, num_test_episodes=10, alpha=0.99, gamma=0.99, polyak=0.995, target_entropy=None, adaptive_temperature=True, lr_a=0.0001, lr_c=0.0003, lr_alpha=0.0001, lr_a_final=1e-10, lr_c_final=1e-10, lr_alpha_final=1e-10, lr_decay_type=DEFAULT_DECAY_TYPE, lr_a_decay_type=None, lr_c_decay_type=None, lr_alpha_decay_type=None, lr_decay_ref=DEFAULT_DECAY_REFERENCE, batch_size=256, replay_size=int(1000000.0), seed=None, device='cpu', logger_kwargs=dict(), save_freq=1, start_policy=None, export=False)

   Trains the SAC algorithm in a given environment.

   :param env_fn: A function which creates a copy of the environment. The environment
                  must satisfy the gymnasium API.
   :param actor_critic: The constructor method for a
                        Torch Module with an ``act`` method, a ``pi`` module and several
                        ``Q`` or ``L`` modules. The ``act`` method and ``pi`` module should
                        accept batches of observations as inputs, and the ``Q*`` and ``L``
                        modules should accept a batch of observations and a batch of actions as
                        inputs. When called, these modules should return:

                        ===========  ================  ======================================
                        Call         Output Shape      Description
                        ===========  ================  ======================================
                        ``act``      (batch, act_dim)   | Numpy array of actions for each
                                                        | observation.
                        ``Q*/L``     (batch,)           | Tensor containing one current estimate
                                                        | of ``Q*/L`` for the provided
                                                        | observations and actions. (Critical:
                                                        | make sure to flatten this!)
                        ===========  ================  ======================================

                        Calling ``pi`` should return:

                        ===========  ================  ======================================
                        Symbol       Shape             Description
                        ===========  ================  ======================================
                        ``a``        (batch, act_dim)   | Tensor containing actions from policy
                                                        | given observations.
                        ``logp_pi``  (batch,)           | Tensor containing log probabilities of
                                                        | actions in ``a``. Importantly:
                                                        | gradients should be able to flow back
                                                        | into ``a``.
                        ===========  ================  ======================================

                        Defaults to
                        :class:`~stable_learning_control.algos.pytorch.policies.soft_actor_critic.SoftActorCritic`
   :type actor_critic: torch.nn.Module, optional
   :param ac_kwargs: Any kwargs appropriate for the ActorCritic
                     object you provided to SAC. Defaults to:

                     =======================  ============================================
                     Kwarg                    Value
                     =======================  ============================================
                     ``hidden_sizes_actor``    ``64 x 2``
                     ``hidden_sizes_critic``   ``128 x 2``
                     ``activation``            :class:`torch.nn.ReLU`
                     ``output_activation``     :class:`torch.nn.ReLU`
                     =======================  ============================================
   :type ac_kwargs: dict, optional
   :param opt_type: The optimization type you want to use. Options
                    ``maximize`` and ``minimize``. Defaults to ``maximize``.
   :type opt_type: str, optional
   :param max_ep_len: Maximum length of trajectory / episode /
                      rollout. Defaults to the environment maximum.
   :type max_ep_len: int, optional
   :param epochs: Number of epochs to run and train agent. Defaults
                  to ``100``.
   :type epochs: int, optional
   :param steps_per_epoch: Number of steps of interaction
                           (state-action pairs) for the agent and the environment in each epoch.
                           Defaults to ``2048``.
   :type steps_per_epoch: int, optional
   :param start_steps: Number of steps for uniform-random action
                       selection, before running real policy. Helps exploration. Defaults to
                       ``0``.
   :type start_steps: int, optional
   :param update_every: Number of env interactions that should elapse
                        between gradient descent updates. Defaults to ``100``.
   :type update_every: int, optional
   :param update_after: Number of env interactions to collect before
                        starting to do gradient descent updates. Ensures replay buffer
                        is full enough for useful updates. Defaults to ``1000``.
   :type update_after: int, optional
   :param steps_per_update: Number of gradient descent steps that are
                            performed for each gradient descent update. This determines the ratio of
                            env steps to gradient steps (i.e. :obj:`update_every`/
                            :obj:`steps_per_update`). Defaults to ``100``.
   :type steps_per_update: int, optional
   :param num_test_episodes: Number of episodes used to test the
                             deterministic policy at the end of each epoch. This is used for logging
                             the performance. Defaults to ``10``.
   :type num_test_episodes: int, optional
   :param alpha: Entropy regularization coefficient (Equivalent to
                 inverse of reward scale in the original SAC paper). Defaults to
                 ``0.99``.
   :type alpha: float, optional
   :param gamma: Discount factor. (Always between 0 and 1.).
                 Defaults to ``0.99``.
   :type gamma: float, optional
   :param polyak: Interpolation factor in polyak averaging for
                  target networks. Target networks are updated towards main networks
                  according to:

                  .. math:: \theta_{\text{targ}} \leftarrow
                      \rho \theta_{\text{targ}} + (1-\rho) \theta

                  where :math:`\rho` is polyak (Always between 0 and 1, usually close to 1.).
                  In some papers :math:`\rho` is defined as (1 - :math:`\tau`) where
                  :math:`\tau` is the soft replacement factor. Defaults to ``0.995``.
   :type polyak: float, optional
   :param target_entropy: Initial target entropy used while learning
                          the entropy temperature (alpha). Defaults to the
                          maximum information (bits) contained in action space. This can be
                          calculated according to :

                          .. math::
                              -{\prod }_{i=0}^{n}action\_di{m}_{i}\phantom{\rule{0ex}{0ex}}
   :type target_entropy: float, optional
   :param adaptive_temperature: Enabled Automating Entropy Adjustment
                                for maximum Entropy RL_learning.
   :type adaptive_temperature: bool, optional
   :param lr_a: Learning rate used for the actor. Defaults to
                ``1e-4``.
   :type lr_a: float, optional
   :param lr_c: Learning rate used for the (soft) critic. Defaults to
                ``1e-4``.
   :type lr_c: float, optional
   :param lr_alpha: Learning rate used for the entropy temperature.
                    Defaults to ``1e-4``.
   :type lr_alpha: float, optional
   :param lr_a_final: The final actor learning rate that is achieved
                      at the end of the training. Defaults to ``1e-10``.
   :type lr_a_final: float, optional
   :param lr_c_final: The final critic learning rate that is achieved
                      at the end of the training. Defaults to ``1e-10``.
   :type lr_c_final: float, optional
   :param lr_alpha_final: The final alpha learning rate that is
                          achieved at the end of the training. Defaults to ``1e-10``.
   :type lr_alpha_final: float, optional
   :param lr_decay_type: The learning rate decay type that is used (options
                         are: ``linear`` and ``exponential`` and ``constant``). Defaults to
                         ``linear``. Can be overridden by the specific learning rate decay types.
   :type lr_decay_type: str, optional
   :param lr_a_decay_type: The learning rate decay type that is used for
                           the actor learning rate (options are: ``linear`` and ``exponential`` and
                           ``constant``). If not specified, the general learning rate decay type is used.
   :type lr_a_decay_type: str, optional
   :param lr_c_decay_type: The learning rate decay type that is used for
                           the critic learning rate (options are: ``linear`` and ``exponential`` and
                           ``constant``). If not specified, the general learning rate decay type is used.
   :type lr_c_decay_type: str, optional
   :param lr_alpha_decay_type: The learning rate decay type that is used
                               for the alpha learning rate (options are: ``linear`` and ``exponential``
                               and ``constant``). If not specified, the general learning rate decay type is used.
   :type lr_alpha_decay_type: str, optional
   :param lr_decay_ref: The reference variable that is used for decaying
                        the learning rate (options: ``epoch`` and ``step``). Defaults to ``epoch``.
   :type lr_decay_ref: str, optional
   :param batch_size: Minibatch size for SGD. Defaults to ``256``.
   :type batch_size: int, optional
   :param replay_size: Maximum length of replay buffer. Defaults to
                       ``1e6``.
   :type replay_size: int, optional
   :param seed: Seed for random number generators. Defaults to ``None``.
   :type seed: int
   :param device: The device the networks are placed on (options: ``cpu``,
                  ``gpu``, ``gpu:0``, ``gpu:1``, etc.). Defaults to ``cpu``.
   :type device: str, optional
   :param logger_kwargs: Keyword args for EpochLogger.
   :type logger_kwargs: dict, optional
   :param save_freq: How often (in terms of gap between epochs) to save
                     the current policy and value function.
   :type save_freq: int, optional
   :param start_policy: Path of a already trained policy to use as the starting
                        point for the training. By default a new policy is created.
   :type start_policy: str
   :param export: Whether you want to export the model as a ``TorchScript`` such
                  that it can be deployed on hardware. By default ``False``.
   :type export: bool

   :returns:

             tuple containing:

                 -   policy (:class:`SAC`): The trained actor-critic policy.
                 -   replay_buffer (union[:class:`~stable_learning_control.algos.common.buffers.ReplayBuffer`, :class:`~stable_learning_control.algos.common.buffers.FiniteHorizonReplayBuffer`]):
                     The replay buffer used during training.
   :rtype: (tuple)


.. py:function:: tf_installed()

   Checks if TensorFlow is installed.

   :returns: Returns ``True`` if TensorFlow is installed.
   :rtype: bool


.. py:data:: __version__
   :value: '6.0.0'


.. py:data:: __version_tuple__

